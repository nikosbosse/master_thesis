\tableofcontents

# Introduction {#intro}

Policy makers have recently started to rely on forecasts to make decisions. Accurate knowledge of the future is immensely valuable in all sorts of areas from farming to economics to public health. With the rise of the novel coronavirus SARS-CoV-2, statistical forecasting has gathered renewed attention. As the virus has spread over the globe, more and more research teams began forecasting the trajectory of the pandemic to help inform public policy. Several countries like the United States, Germany and the United Kingdom have therefore started to aggregate forecasts from different teams. Among these efforts, the US Forecast Hub [@umass-amherstinfluenzaforecastingcenterofexcellenceCovid19forecasthubOrg2020] is the largest and most visible. Its goal is to collect forecasts, to aggregate them, and to make them available to policy makers and the general public in the best possible way. Two questions have been at the centre of these efforts: The first is "how can we best evaluate the performance of a model?" The second is "how can we combine and aggregate different models to get the best possible prediction?". These two questions will be our guiding questions as well throughout this work. 

#### Objectives {-}

This thesis has three main objectives: To obtain a deeper understanding of model evaluation, to explore ways to aggregate models to ensembles, and to facilitate model evaluation and model aggregation by creating appropriate tools. As a case study we analysed the predictions of eight models submitted to the US Forecast Hub between the 22th of June 2020 and the 3rd of August 2020, as well as three different ways of aggregating these models to ensembles. One of the models, 'epiforecasts-ensemble1', was the model submitted by the working group at the London School of Hygiene and Tropical Medicine (LSHTM) which co-supervised this thesis. While the comparison of different models is interesting in and of itself, the main goal of this analysis is to obtain a deeper understanding of the evaluation metrics and model aggregation techniques studied throughout this thesis. To that end we elucidate the concepts behind model evaluation and discuss a variety of possible evaluation metrics in detail. This theoretical discussion allows us then to thoroughly asses the performance of the Forecast Hub models as well as gain a better understanding of the metrics in an applied setting. In addition to this, we explore ways of aggregating models to ensembles. These two things, evaluation and model aggregation are closely connected, because the metrics used to score forecasts can also be used as a target to guide the formation of an optimal ensemble. Apart from a simple mean ensemble, we look into two approaches to combine individual model predictions. The first, Quantile Regression Averaging [@nowotarskiComputingElectricitySpot2015], forms an ensemble by minimising the so called weighted interval score [@gneitingStrictlyProperScoring2007]. The second method is a novel stacking approach [@yaoUsingStackingAverage2018] that optimises the continuous ranked probability score [@mathesonScoringRulesContinuous1976; @gneitingStrictlyProperScoring2007]. We discuss these model aggregation techniques, apply them to the eight original forecast models and evaluate the ensemble performance alongside the other models. 

#### Contributions {-}

A number of novel contributions come out of this thesis. The first one is the structured model evaluation approach described and proposed in Chapter \@ref(evaluation). Model evaluation has already been discussed extensively in the literature (see e.g. @gneitingStrictlyProperScoring2007, @gneitingMakingEvaluatingPoint2010, @bracherEvaluatingEpidemicForecasts2020, and @funkAssessingPerformanceRealtime2019). For most parts, however, this discussion is quite technical in nature. Chapter \@ref(evaluation) summarises important aspects from the literature and proposes a structured evaluation approach that can easily be applied even by non-experts. Most of the metrics discussed there have been previously published [@bracherEvaluatingEpidemicForecasts2020; @funkAssessingPerformanceRealtime2019; @gneitingStrictlyProperScoring2007], but some of them have been adapted or newly developed within the working group at LSHTM and are described here for the first time. The second contribution is the `scoringutils` package [@R-scoringutils] that was developed in the context of this thesis and used to evaluate the predictions from the Forecast Hub. The package implements the metrics discussed in Chapter \@ref(evaluation) and greatly facilitates structured model evaluation in a simple and unified workflow. The third contribution is the `stackr` package [@R-stackr] that was developed in collaboration with Yuling Yao from the Columbia University in New York. It implements a novel stacking procedure based on the continuous ranked probability score. This technique is described in Chapter \@ref(model-aggregation) and explored in practice in Chapter \@ref(results). The fourth contribution is the comprehensive evaluation of eight Forecast Hub models and three different ensembles. This evaluation allows us to obtain a better understanding of the models, as well as of the evaluation metrics themselves. 

#### Structure {-}

The remainder of this thesis is structued as follows: Chapter \@ref(evaluation) gives a detailed introduction to forecast evaluation. It introduces and discusses different metrics and proper scoring rules in detail that form the basis for the coming chapters. Chapter \@ref(model-aggregation) is dedicated to model ensembles. It provides an intuition for how different predictive distributions can be combined and describes two different model aggregation techniques that build upon the scoring rules described in Chapter \@ref(evaluation). Chapter \@ref(background-data) provides some context for the later analysis in Chapter \@ref(results). It describes the Forecast Hub in greater detail, takes a first look at the observed data and gives an overview of the different forecasting models. Chapter \@ref(results) applies the tools described in Chapters \@ref(evaluation) and \@ref(model-aggregation) to the forecasts from eight different models submitted to the US Forecast Hub. It assesses the individual models and analyses the performance of the different model aggregation techniques. Chapter \@ref(results) also examines the sensitivity of the results to different choices of model aggregation and evaluation parameters. Chapter \@ref(discussion) discusses the results and concludes this thesis. 

#### Code {-}

The analysis for this thesis was conducted in R, version 4.0.2 [@R-base]. All code is publicly available. This includes the code for this thesis^[[github.com/nikosbosse/master_thesis](https://github.com/nikosbosse/master_thesis)], for the `scoringutils`^[[github.com/epiforecasts/scoringutils](https://github.com/epiforecasts/scoringutils)] and the `stackr`^[[github.com/epiforecasts/stackr](https://github.com/epiforecasts/stackr)] package as well as the code used to create the epiforecasts-ensemble predictions^[[github.com/epiforecasts/covid-us-forecasts](https://github.com/epiforecasts/covid-us-forecasts)] submitted to the Forecast Hub. 






