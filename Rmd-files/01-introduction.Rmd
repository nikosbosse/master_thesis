# Introduction

Policy makers have always relied on some kind of forecast of what the future will be like. Accurate knowledge of the future is immensely valuable in all sorts of areas from farming to economics to public health. The last of these areas, public health, has gathered renewed attention with the rise of the novel coronavirus SARS-CoV-2. As the virus spread throughout the world, more and more research teams around the world have started forecasting the trajectory of the epidemic. Several countries like the United States, the United Kingdom and Germany have started aggregating forecasts from different teams [CITATIONS]. Two questions have been at the centre of these efforts. The first is "how can we best evaluate the predictive performance of a model?" The second is "how can we combine and aggregate different models to get the best possible prediction?". These two questions will be the focus of this thesis as well. 

## Aim and motivation

The goal and motivation of this thesis is therefore twofold: It's first aim is to present the application of a consistent evaluation framework to weekly death forecasts submitted to the Reich Forecast Hub from XXX to XXX. It's second aim is to use this evaluation framework to evaluate and compare different ensemble techniques. 

## Scope of this work, background, and personal contribution

This thesis is in part built on work that comes out of a research collaboration with the group led by Sebstian Funk at the London School of Hygiene and Tropical Medicine that started in January 2020. The original goal back then was to work on forecasting for Ebola cases in Kongo. The idea was to use $R_t$, the average number of people each infected person at time $t$ will infect themselves, to improve case forecasts. At that time I had worked on an implementation of Bayesian Structural Time Series models (BSTS) in Stan CITATION. In late January priorities quickly changed with the emergence of Covid-19. 

In February, I contributed to a branching process model CITATION that the group worked on to simulate potential trajectories the epidemic could take given different levels of contact tracing and asymptomatic spreaders. This work is largely unrelated to this master thesis. 

In March, the focus on the working group shifted towards estimating $R_t$ and making these estimates available online. My priority then was again $R_t$ based forecasting - now for Covid-19 cases. Much of that work is now merged into an R package called `EpiSoon`. $R_t$-based forecasting was also part of the forecasts that we submitted to the US forecast Hub later on. In order to be able to evaluate the forecasts I started working on an R package called `scoringuitls` that bundles together different metrics to evaluate forecasts. Most of the metrics in the literature have previously been published CITATION and were used e.g. in previous work on Ebola CITATION. Some of them were already implemented in other R packages, namely the `scoringRules` package. My main contribution was to collect these rules, document and bundle them in an easy-to-use package and build them into a consistent evaluation framework. A substantial part of building and coding up this consistent evaluation framework has happened as part of this thesis in May, June and July. 

In late April I began officially working on this master thesis. As stated above, this work focuses on two main topics: evaluating models and ensembling models. To tackle the second topic I started development of a package called `stackr` in late April. This package uses the Continuous Ranked Probability Score (CRPS), a proper scoring rule to create an ensemble that minimises discrepancy between the predictive distribution and the true observed data. This package was developed in collaboration with Yuling Yao at Columbia University in New York. Most of the methodological contributions to model are his. I revised the Stan code and built the framework and the R package around it. 

In June, the working group started contributing to the Reich Lab Forecast Hub CITATION. The model we submitted, epiforecasts-ensemble1 will be discussed later. My contribution to that was work on setting up the entire forecasting pipeline. I especially focused on evaluating and ensembling the forecasts mode. 

## Outline of this thesis

This master thesis is going to 

- give a theoretical introduction into different types of forecasts and how to evaluate them

- give a theoretical introduction to different ways to ensemble models

- Give a quick introduction to the US Forecast Hub and the models discussed later

- Use the evaluation and ensembling techniques introduced on the models submitted to the Forecast Hub. 


## Code

The code used for this master thesis is available on github.
- scoringutils package
- covid-us-forecasts
- stackr package
- master-thesis repo


