# Data and forecasting models {#background-data}

After Chapters \@ref(evaluation) and \@ref(model-aggregation) have described the theoretical foundations of model evaluation and model aggregation, this chapter gives a first overview of the data and the different forecasting models analysed in Chapter \@ref(results) to provide some context for the ensuing evaluation. It first describes the Forecasting Hub in more detail and offers a first look at the data. It then gives a brief overview of the different forecasting models. 

## Introduction to the COVID-19 Forecast Hub and overview of the data

The COVID-19 Forecast Hub [@umass-amherstinfluenzaforecastingcenterofexcellenceCovid19forecasthubOrg2020] is a collaboration between the U.S. Centers for Disease Control and Prevention (CDC), academic research groups led by Professor Nicholas Reich at the University of Massachussets Amherst, and different industry partners. Starting on April 13, the consortium elicited weekly forecasts for all US states and territories and the US as a whole from research teams around the world. Forecasts were submitted every Monday in a probabilistic format. The predictive distribution was represented by the median and eleven prediction intervals ranging from a 10% prediction interval to a 98% prediction interval (i.e. the following 23 quantiles were recorded: `c(0.01, 0.025, seq(0.05, 0.95, by = 0.05), 0.975, 0.99)`). Forecasts were made for one to (at least) four week ahead horizons of targets like daily or weekly deaths and case numbers. 

Out of these targets, this we focus only on weekly death incidences. The forecasts we analysed were made between June 22nd 2020 and August 3rd 2020 in twelve different US states and the US as a whole. As the ensemble models need at least one week of past data to estimate weights, only forecasts between June 29th and August 3rd. were evaluated. Eight models were chosen from all Forecast Hub models in a way that attempts to reflect the variety of models submitted to the Hub. Among these models also was the 'epiforecasts-ensemble1' model that was submitted by the working group at the London School of Hygiene and Tropical Medicine that co-supervised this work. Dates and locations were chosen to obtain a complete set of predictions with no missing forecasts for any location or time point. While this isn't strictly necessary, it avoids downstream complications with model aggregation and evaluation that are beyond the scope of this thesis. 

The ground truth data is provided by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University [@dongInteractiveWebbasedDashboard2020]. Table \@ref(tab:overview) gives an overview of the dates, locations and models included. Figure \@ref(fig:us-data) shows observed deaths in all thirteen locations. The time frame that is actually analysed is highlighted in green. We can see that the evolution of deaths exhibits quite different dynamics across different locations. In states like Illinois, Maryland or Massachusetts, deaths were mostly constant or falling. In others, like Arizona, Florida or Texas, deaths showed a strong upwards trend. Chapter \@ref(results) will analyse in detail how models fare in these different scenarios. 

``` {r us-data, echo = FALSE, out.width = "95%", fig.cap = "Observed deaths in different regions. Weeks for which predictions are analysed are highlighted in green."}

knitr::include_graphics("../visualisation/chapter-2-background-data/plot-observations.png")

```

## An overview the different forecast models

The models analysed in-depth in the next chapter vary substantially in the way they generate predictions. This section therefore gives a quick overview of the different model types. This overview should not be thought of as an exhaustive discussion, but is merely intended as a short primer that allows us to mentally place the COVID-19 Forecast Hub models in broad categories. The information used to inform this overview is taken from the descriptions provided by the research teams themselves^[Model descriptions were uploaded by the teams on [github.com/reichlab/covid19-forecast-hub/tree/master/data-processed](https://github.com/reichlab/covid19-forecast-hub/tree/master/data-processed). The descriptions were accessed on 07.08.2020. As descriptions on github are updated whenever the model changes, this overview may therefore not be entirely accurate for all models over the entire history of past submissions included in the analysis.]. 

Among the most widely used models in epidemiology are so-called compartmental models (see e.g. @brauerCompartmentalModelsEpidemiology2008 for an extensive overview). These split the general population in compartments and model the flow between these compartments. The basic compartments are *Susceptible* (S), *Infectious* (I) and *Recovered* (R), giving these models the name SIR models. The flow from one compartment to the other is usually modeled using a set of differential equations. Compartmental models help to model specific characterisitcs of people in different compartments. For example, People in the *Susceptible* compartment can be infected, while those who are in the *Recovered* compartment are assumed to be immune against further infection. Compartmental models are therefore able to model the depletion of susceptibles as the epidemic progresses. Other compartments, like *Exposed but not infectious* (E) can be introduced ad libitum to model e.g. incubation periods. Most models submitted to the Forecast Hub are compartmental models. Three models analysed here belong to that category: UMass-MechBayes, YYG-ParamSearch and CU-select.

UMass-MechBayes^[[github.com/dsheldon/covid](https://github.com/dsheldon/covid)] is a Bayesian model build from a classical SEIR compartmental model. The model is fit independently to each state and allows its parameters to vary over time. 

The YYGParamSearch model^[[covid19-projections.com](https://covid19-projections.com) and   [github.com/youyanggu/covid19_projections](https://github.com/youyanggu/covid19_projections)] adds an additional machine learning layer to a classical SEIR model to learn optimal hyperparameters for the model. Some of the parameters like the infectious period are shared across locations, but most parameters like the mortality from Covid-19 or the effetive reproduction number $R_t$ (the average number of people each infected person will infect in turn, see e.g. @nishiuraEffectiveReproductionNumber2009 or @coriNewFrameworkSoftware2013) etc. are determined per state. 
The CU-select model is a SEIR model that is augmented by human insight. The Columbia University regularly submits a range of projections under different scenarios. For the CU-select model they always hand-pick the one they believe is most plausible. 

Another common approach is to model the evolution of the pandemic using a time-varying growth rate. Based on the assumption that the spread of a disease is exponential in nature it is intuitive to estimate future case numbers by modeling the growth rate. The LANL-GrowthRate model and to a certain extent the epiforecasts-ensemble1 take this approach. 

The LANL-GrowthRate model is a two component model. The first component models the number of infections using a time-varying growth parameter that connects present (or future) infections to earlier infections and the number of susceptibles. In a second step, this infections get mapped to reported deaths by assuming a fraction of infections likely to die. 

The epiforecasts-ensemble combines the growth rate approach with classical time series modeling. It consists of three submodels that were aggregated using first an equally weighted quantile average and later a quantile regression average. The three submodels were two time series models and an $R_t$-based prediction model. $R_t$ denotes the effective reproduction number, i.e. the average number of people each infected person will infect. The two timeseries models were generated using the forecastHybrid package [@R-forecastHybrid]. The package automatically selects an Autoregressive Integrated Moving Average (ARIMA) model or a State Space model with appropriate error, trend and seasonality (ETS model). One of the epiforecasts-ensemble1 timeseries models included current cases as a lagged predictor, while the other was based on deaths only. The third model was generated using the R packages `EpiSoon` [@R-EpiSoon], `EpiNow` [@R-EpiNow], and later `EpiNow2` [@R-EpiNow2]. `EpiSoon` takes in reported cases to estimate the trajectory of $R_t$. This trajectory is then forecasted into the future using `forecastHybrid` and transformed back to incidences using the so-called renewal equation that models futures cases as a weighted sum of past cases times $R_t$. These three models are fit independently to each location. 

A third possibility is to model deaths more directly in terms of a regression framework. This the approach chosen by the UT-Mobliity model [@woodyProjectionsFirstwaveCOVID192020]) that employs a Bayesian negative binomial regression model. One of the major predictors used is GPS mobility data provided by a company called SafeGraph. This data is used to compute measures of social distancing that are ultimately fed into the regression model.

In order to provide a sensible baseline to compare models against, the Forecast Hub created the COVIDhub-baseline model. This model basically assumes that incidences will be the same as in the past and models the uncertainty around this estimate according to the distribution of past changes in incidences. More precisely, forecasts for incidences are generated in the following way: The timeseries of all past incidences is taken and first differences are formed. These first differences, as well as the negative of these incidences are then used further along. From this vector of past changes, samples are drawn to get a predictive distribution of future changes. A predictive distribution for future incidences is then obtained by adding these samples to the last observed incidence. The predictive distribution is then shifted to enforce that the mean of the distribution equals the last observed value. Incidences below zero are truncated, and quantiles are obtained from the samples. 
In addition to the models described above, four model ensembles were analysed. One of these ensembles, the COVIDhub-ensemble was an ensemble created by the Forecast Hub itself using a quantile average approach. The model was formed by taking the arithmetic mean of the corresponding quantiles from all eligible models. Models are deemed eligible if they pass some general sense checks (e.g. cumulative forecasts must in general not decrease over time). The number of models included varies from state to state as not all teams submit forecasts for all locations. The other three ensembles were formed from the above described eight original models. 

The mean-ensemble is a simple equally weighted quantile average of all original models. All models included in the mean-ensemble are therefore also included in the COVIDhub-ensemble. We can therefore think of the mean-ensemble as a variotion of the COVIDhub-ensemble that puts higher weight on the models included in this analysis and zero weight to the models excluded. The mean-ensemble serves as an important control. Any performance difference between the two ensembles can be attributed to the effect of the selection of models included for the analysis. 

The qra-ensemble is formed based on the two last weeks of data using the methodology outlined in Chapter \@ref(model-aggregation). The qra-ensemble takes all past forecasts for which we have observations into account. It is therefore created using one and two-week-ahead forecasts. For the first evaluation date, June 29th, only one week of past data was used to inform the ensemble weights. Two weeks of past data were chosen as a sensible default, instead of optimised for, in order to avoid overfitting. Chapter \@ref(results), however, also provides a sensitivity analysis that explores other choices. 

The crps-ensemble was also formed using two weeks of past data. But in order to make model aggregation possible, forecasts had to be transformed first. While the CRPS ensemble approach described in Chapter \@ref(model-aggregation) is based on predictive samples, the Forecast Hub only stores predictive distributions in a quantile forecast. We therefore fit a separate gamma distribution to every set of predictive quantiles. The gamma distribution was chosen for its simplicity and reasonably good fit. The fitted distributions were subsequently used to obtain 1 000 predictive samples per forecast. We then estimated weights based on the predictive samples and created a mixture distribution by random sampling from the individual model samples with probability equal to the estimated weights. The samples drawn for the mixture model were then again used to create quantiles. This approach is not ideal as it is bound to lose information. Overall, however, it seemed to have worked not too badly. Figure \@ref(fig:difference-true-sampled) shows the difference between quantiles from the actual predictive distributions submitted to the Forecast Hub and sample quantiles obtained by random sampling from a gamma distribution fitted to the same forecasts. This figure includes all forecasts for all locations, forecast dates and quantiles at once, so it may not show the full picture. It broadly suggests however, that the approach works reasonably for most quantiles of the predictive distribution. For higher quantiles we can see substantial deviations. As `stackr` currently does not support multiple forecast horizons, a choice also had to be made for a forecast horizon to optimise for. We chose to optimise for two-week-ahead forecasts. This effectively means that the crps-ensemble could use less data than the qra-ensemble, as forecasts from the previous week did not yet have matching observations and therefore only the two-week-ahead forecasts from two weeks ago could be used. Again, other possible choices for these parameters are explored in Chapter \@ref(results). 

``` {r difference-true-sampled, echo = FALSE, out.width = "95%", fig.cap = "Observed deaths in different regions. Weeks for which predictions are analysed are highlighted in green."}

knitr::include_graphics("../visualisation/chapter-4-ensemble/difference-true-sampled.png")

```

Table \@ref(tab:overview) provides a convenient summary of the different locations, forecast dates and models included in the analysis. 

```{r overview, echo=FALSE, results='asis'}
df <- readRDS("../visualisation/chapter-2-background-data/table-overview.RDS")

options(knitr.kable.NA = '')
knitr::kable(
  df, 
  caption = 'An overview of the locations, dates and models included in the analysis.',
  booktabs = TRUE) 
```










<!-- Metrics that directly capture the trajectory of the epidemic include the effective reproduction number and the doubling or halving time. The effective reproduction number, $R_t$ is the expected number of people that each infected person at time $t$ will infect themselves. In contrast to the basic reproduction number, $R_0$, the effective reproduction number varies over time, e.g. with changes in behaviour or different interventions implemented. See for example  for an in-depth discussion and  -->

<!-- for one possible way to estimate $R_t$. @gosticPracticalConsiderationsMeasuring2020 (with input from our working group at LSHTM) discuss some important practical considerations for estimating $R_t$.  -->

<!-- RENEWAL EQUATION? OR JUST OMIT THAT PART?  -->

<!-- The doubling or halving time (not further discussed throughout this thesis) is the time expected until the number of cases doubles or halves, depending on the state of the epidemic. Both quantities can be a useful basis for policy decision, e.g. about whether or not to implement a lockdown, but they are not trivial to estimate. Also, there exists no ground truth against which different models could be compared.  -->

<!-- The most obvious prediction target, perhaps, is the number of infected cases, but there are a variety of problems associated with case numbers. First, it is not clear what the best definition of a case is. This was an issue especially in early 2020, when PCR tests were not readily available. Case numbers varied substantially depending on whether one would count only cases confirmed by PCR testing or also those who clearly showed Covid-19 symptoms, but had not received a test. China, for example, reported 15 152 new cases for February 12 [@nationalhealthcommissionoftheprcFeb13Daily], a 600% surge from previous days, when they changed the reporting regime and decided to also report clinically confirmed cases.  -->
<!-- Secondly, changes in reported case counts cannot always directly be attributed to changes in the underlying epidemic. Even today, the number of confirmed cases depends on and varies a lot with testing efforts. @timothywrusselljoelhellewellsamabbottnickgoldinghamishgibbschristopherijarviskevinvanzandvoortcmmidncovworkinggroupstefanflascherosalindmeggowjohnedmundsadamjkucharskiUsingDelayadjustedCase2020 have even used this to estimate under-reporting by comparing the number of deaths to the number of reported cases. Overall cases are in many locations estimated to be a multiple of what reported cases are. Thirdly, reported cases are subject to various delays and weekday effects. Patients may not be able to see a doctor over the weekend or hospitals do not forward cases to the authorities until the next Monday. This thesis will therefore only look at weekly aggregated data. Lastly, for many practical purposes such as estimating $R_t$, we need to figure out the date of symptom onset or even the date of infection of a patient, not only the date of report. A few countries publish very detailed line lists that include precise information about the estimated date of onset or date of infection of a particular person. But usually, delay distributions have to be estimated from very sparse data and have a lot of uncertainty around them.  -->

<!-- The COVID-19 Forecast Hub has decided to focus most of their efforts on deaths for a variety of reasons. Death counts are a very reliable source of data. While there has been some debate about people who are dying 'with Covid-19' or 'from Covid-19', a death can be defined and counted relatively easily compared to other metrics. Death counts are also largely independent from testing rates. They are therefore able to capture the pandemic as a whole, whereas reported case counts would often miss a substantial fraction of overall cases. The major problem with death counts in terms of their usefulness is that they are subject to long delays. Today's death counts therefore inform more about the state of affairs a month ago, then about the future trajectory of the disease.  -->

<!-- To a certain extent, the quantities described above can be used interchangeably, if we accept some loss of precision. We can model the sequence from infection to symptom onset, to report, to hospitalisation and finally to death by a sequence of delays that are characterised by different delay distributions. By convoluting the appropriate delay distributions we can e.g. shift cases back from their date of report to their probable date of symptom onset or date of infection.  -->

<!-- MAYBE FIGURE WITH ESTIMATED CASES ON DATE OF INFECTION. OR MAYBE JUST DROP THAT PARAGRAPH ALL TOGETHER... -->



