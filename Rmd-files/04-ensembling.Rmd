# Ensembling

The following chapter gives an introduction to the idea of model ensembling and presents several approaches to ensemble models. 

## Theoretical idea

Usually, a single forecasting model is not able to capture the nuances and full complexity of the true data-generating process. Forecasting models usually get some things right and others wrong. The important thing to note is that different forecasting models usually get different things wrong. Let us consider $K$ different models that make predictions $\hat{y}_1, \dots, \hat{y}_k, \dots \hat{y}_K$ for an unknown true value $y$. In a very simplified manner, we can assume that these all models make a random error, $\epsilon_k$ such that predictions can be expressed as $\hat{y}_k = y + \epsilon_k$. We can minimise the prediction error by taking the average prediction made by different models, instead of a single model. If prediction errors are independent and have expactation zero, then $\hat{y}_{\text{ensemble}} = \frac{1}{K} \sum_{k = 1}^{K} \hat{y}_k = \frac{1}{K} \sum_{k = 1}^{K} (y + \epsilon_k)$ will converge to the true value $y$ for an increasing number of models. This is the idea behind a simple mean ensemble that has proven very effective. [CITATIONS]

In practice, prediction errors of different models are seldom completely independent, but are instead often correlated. In practice, this reduces the effectiveness of ensembling. Note also that ensembling also only makes sense if we believe our models to be correct, i.e. if we expect $\mathbb{E}(\epsilon_k) = 0$. It can therefore make sense to give less weight to models that have performed poorly in the past and more weight towards those that performed well. This is the idea behind weighted ensembles. 

The last two paragraphs implicitly assumed that we create the ensembling by taking a weighted average of the model predictions. While this is one option, it is not the only one. As we are dealing with probabilistic forecasts and predictive distributions, we can also work with a mixture distribution instead of an average distribution. In principle, the average is more appropriate if we believe that all models predict the same thing and we are looking for the optimal candidat. The mixture may be more appropriate if we believe that our models reflect different possible scenarios and are somewhat uncertain, which of these scenarios will occur. 

PLOT DIFFERERENCE MIXTURE AND AVERAGE

In the following, two different ensembling strategies beyond the mean ensemble will be presented. The Quantile Regression Average is an ensemble strategy suited for quantile forecasts that determines optimal weights for a weighted average. The CRPS ensemble is based on predictive samples and determines optimal weights for a mixture distribution generated by bootstrapping from the individual ensemble members. 

## QRA ensembling


## CRPS ensembling
