# Epidemiological background, data and forecasting models {#background-data}

Tracking a pandemic is not easy. Trying to predict its trajectory is even harder. While some of the challenges are inherent to forecasting in general, others are very specific to epidemiological modelling. The following chapter aims to highlight some of these challenges and to provide background and context for all later chapters. One difficulty in epidemiological modelling is that it is not immediately clear what quantity to look at. This chapter will therefore first give a brief overview of different possible targets of interest and the rationale behind looking at deaths in particular. The purpose is not to give a complete introduction, but rather to point out some of the complexities surrounding the choice of a prediction target. It then offers a very short introduction to different classes of modelling approaches common in epidemiology. It lastly introduces the COVID-19 Forecast Hub and gives an overview of the different forecasting models that will be analysed and aggregated in the later chapters. 

## Overview of epidemiological prediction targets

Many different quantities may be of interest depending on the specific decisions to be made. These include e.g. current and future incidences or cumulative numbers of cases and deaths, the number of people who need a ventilator or treatment in an intensive care unit (ICU), or indicators about the likely trajectory of the epidemic. 

Metrics that directly capture the trajectory of the epidemic include the effective reproduction number and the doubling or halving time. The effective reproduction number, $R_t$ is the expected number of people that each infected person at time $t$ will infect themselves. In contrast to the basic reproduction number, $R_0$, the effective reproduction number varies over time, e.g. with changes in behaviour or different interventions implemented. See for example @nishiuraEffectiveReproductionNumber2009 for an in-depth discussion and @coriNewFrameworkSoftware2013 for one possible way to estimate $R_t$. @gosticPracticalConsiderationsMeasuring2020 (with input from our working group at LSHTM) discuss some important practical considerations for estimating $R_t$. 

RENEWAL EQUATION? OR JUST OMIT THAT PART? 

The doubling or halving time (not further discussed throughout this thesis) is the time expected until the number of cases doubles or halves, depending on the state of the epidemic. Both quantities can be a useful basis for policy decision, e.g. about whether or not to implement a lockdown, but they are not trivial to estimate. Also, there exists no ground truth against which different models could be compared. 

The most obvious prediction target, perhaps, is the number of infected cases, but there are a variety of problems associated with case numbers. First, it is not clear what the best definition of a case is. This was an issue especially in early 2020, when PCR tests were not readily available. Case numbers varied substantially depending on whether one would count only cases confirmed by PCR testing or also those who clearly showed Covid-19 symptoms, but had not received a test. China, for example, reported 15 152 new cases for February 12 [@nationalhealthcommissionoftheprcFeb13Daily], a 600% surge from previous days, when they changed the reporting regime and decided to also report clinically confirmed cases. 
Secondly, changes in reported case counts cannot always directly be attributed to changes in the underlying epidemic. Even today, the number of confirmed cases depends on and varies a lot with testing efforts. @timothywrusselljoelhellewellsamabbottnickgoldinghamishgibbschristopherijarviskevinvanzandvoortcmmidncovworkinggroupstefanflascherosalindmeggowjohnedmundsadamjkucharskiUsingDelayadjustedCase2020 have even used this to estimate under-reporting by comparing the number of deaths to the number of reported cases. Overall cases are in many locations estimated to be a multiple of what reported cases are. Thirdly, reported cases are subject to various delays and weekday effects. Patients may not be able to see a doctor over the weekend or hospitals do not forward cases to the authorities until the next Monday. This thesis will therefore only look at weekly aggregated data. Lastly, for many practical purposes such as estimating $R_t$, we need to figure out the date of symptom onset or even the date of infection of a patient, not only the date of report. A few countries publish very detailed line lists that include precise information about the estimated date of onset or date of infection of a particular person. But usually, delay distributions have to be estimated from very sparse data and have a lot of uncertainty around them. 

The COVID-19 Forecast Hub has decided to focus most of their efforts on deaths for a variety of reasons. Death counts are a very reliable source of data. While there has been some debate about people who are dying 'with Covid-19' or 'from Covid-19', a death can be defined and counted relatively easily compared to other metrics. Death counts are also largely independent from testing rates. They are therefore able to capture the pandemic as a whole, whereas reported case counts would often miss a substantial fraction of overall cases. The major problem with death counts in terms of their usefulness is that they are subject to long delays. Today's death counts therefore inform more about the state of affairs a month ago, then about the future trajectory of the disease. 

To a certain extent, the quantities described above can be used interchangeably, if we accept some loss of precision. We can model the sequence from infection to symptom onset, to report, to hospitalisation and finally to death by a sequence of delays that are characterised by different delay distributions. By convoluting the appropriate delay distributions we can e.g. shift cases back from their date of report to their probable date of symptom onset or date of infection. 

MAYBE FIGURE WITH ESTIMATED CASES ON DATE OF INFECTION. OR MAYBE JUST DROP THAT PARAGRAPH ALL TOGETHER...


## An overview of common epidemiological models

A lot of different models are used in epidemiology to track and forecast epidemics. The following section will give a very brief overview of some general model types. This shall not be thought of as an exhaustive discussion, but is merely intended as a short primer that shall allow us to mentally place the COVID-19 Forecast Hub models in broad categories. 

### Compartmental models

Among the most widely used models in epidemiology are compartmental models (see e.g. @brauerCompartmentalModelsEpidemiology2008 for an extensive overview). These split the general population in compartments. The basic compartments are *Susceptible* (S), *Infectious* (I) and *Recovered* (R), giving these models the name SIR models. The flow from one compartment to the other is characterised by a set of differential equations. Compartmental models help to model specific characterisitcs of people in different compartments. For example, People in the *Susceptible* compartment can be infected, while those who are in the *Recovered* compartment are assumed to be immune against further infection. Compartmental models are therefore able to model the depletion of susceptibles as the epidemic progresses. Other compartments, like *Exposed but not infectious* (E) can be introduced ad libitum to model e.g. incubation periods. 


### Time series models

Instead of explicitly modeling the disease dynamics, we can also use an agnostic approach and simply treat e.g. case numbers as an ordinary time series. A plethora of time series models has been developed in the past (for an extensive discussion, see e.g. @hyndmanrobjForecastingPrinciplesPractice2019). The most notable classes are Auto-regressive Integrated Moving Average (ARIMA) models and State Space models that separate Error, Trend and Seasonality components (ETS) of a time series. ARIMA and ETS models are very good at picking up past trends in the data. Without exogeneous predictors, they are however limited to the information available in the past time series. 

### Semi-mechanistic models

Semi-mechanistic models are a very broad category of models that combine agnostic forecasting tools with epidemiological insights. For example, time series models can be extended by including some knowledge about the disease dynamics into the forecasts. One possibility that was employed in our own ensemble forecasts is $R_t$-based forecasting. The idea behind this is to estimate and forecast $R_t$ values and to model future cases based on the projected time series of $R_t$. The specific details of this approach shall be discussed later. 


## The COVID-19 Forecast Hub

The COVID-19 Forecast Hub [@umass-amherstinfluenzaforecastingcenterofexcellenceCovid19forecasthubOrg2020] is a collaboration between the U.S. Centers for Disease Control and Prevention (CDC), academic research groups led by Professor Nicholas Reich at the University of Massachussets Amherst, and different industry partners. Starting on April 13, the consortium elicited weekly forecasts for all US states and territories and the US as a whole from research teams around the world. Forecasts were submitted every Monday in a probabilistic format. The predictive distribution was represented by the median and eleven prediction intervals ranging from a 10% prediction interval to a 98% prediction interval (i.e. the following 23 quantiles were recorded: `c(0.01, 0.025, seq(0.05, 0.95, by = 0.05), 0.975, 0.99)`). Forecasts were made for one to (at least) four week ahead horizons of targets like daily or weekly deaths and case numbers. 

Out of these targets, this thesis will focus only on weekly death incidences. The forecasts analysed here were made between XXX and XXXX in twelve different US states and the US as a whole. Eight models were chosen from all Forecast Hub models in a way that attempts to reflect the variety of models submitted to the Hub. Including our own model, 'epiforecasts-ensemble1' made it necessary to restrict the dates and locations included in order to deal with a complete set of forecasts from all models for all dates and locations. While this isn't strictly necessary, it avoids downstream complications with model aggregation and evaluation beyond the scope of this thesis. The ground truth data is provided by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University [@dongInteractiveWebbasedDashboard2020]. Table \@ref(tab:overview) gives an overview of the dates, locations and models included. A plot of the data in the analysed locations can be seen in Figure \@ref(fig:us-data). The time frame for which predictions are analysed is highlighted in green. 


```{r overview, echo=FALSE, results='asis'}
df <- readRDS("../visualisation/chapter-2-background-data/table-overview.RDS")

options(knitr.kable.NA = '')
knitr::kable(
  df, 
  caption = 'An overview of the locations, dates and models included in the analysis.',
  booktabs = TRUE) 
```

``` {r us-data, echo = FALSE, out.width = "95%", fig.cap = "Observed deaths in different regions. Weeks for which predictions are analysed are highlighted in green."}

knitr::include_graphics("../visualisation/chapter-2-background-data/plot-observations.png")

```



## Overview of the forecasting models analysed

Eight individual models are included in this analysis, among them a baseline model and an ensemble model provided by the Forecast Hub team. These models were again aggregated using three different ensemble techniques for the purpose of this analysis. In the following, an overview over the different models submitted is given. The information is taken from the descriptions provided by the research teams on [github.com/reichlab/covid19-forecast-hub/tree/master/data-processed]((https://github.com/reichlab/covid19-forecast-hub/tree/master/data-processed)) as of 07.08.2020. Descriptions on github are updated whenever the model changes. This overview may therefore not accurately reflect the model components over the entire history of past submissions included in the analysis. 

### COVIDhub-baseline

The baseline model basically assumes that incidences will be the same as in the past and models the uncertainty around this estimate according to the distribution of past changes in incidences. More precisely, forecasts for incidences are generated in the following way: The timeseries of all past incidences is taken and first differences are formed. These first differences, as well as the negative of these incidences are then used further along. From this vector of past changes, samples are drawn to get a predictive distribution of future changes. A predictive distribution for future incidences is then obtained by adding these samples to the last observed incidence. The predictive distribution is then shifted to enforce that the mean of the distribution equals the last observed value. Incidences below zero are truncated, and quantiles are obtained from the samples. 

### COVIDhub-ensemble

The COVID-19 ensemble model is formed by taking the arithmetic mean of respective quantiles from all eligible models. Models are deemed eligible if they pass some general sense checks (e.g. cumulative forecasts must in general not decrease over time). The number of models included varies from state to state as not all teams submit forecasts for all locations. 

### epiforecasts-ensemble1

The epiforecasts ensemble is the model submitted by our working group at the London School of Hygiene and Tropical Medicine. It consists of three submodels that were aggregated using first an equally weighted quantile average and later a quantile regression average. The three submodels are two timeseries models and an $R_t$-based prediction model. The two timeseries models were generated using the forecastHybrid package [@R-forecastHybrid]. The package automatically selects an Autoregressive Integrated Moving Average (ARMIMA) model or a State Space model with appropriate error, trend and seasonality (ETS model). One of the timeseries models includes current cases as a lagged predictor, while the other is based on deaths only. The third model was generated using first our own R packages EpiSoon and EpiNow [@R-EpiNow], and late EpiNow2 [@R-EpiNow2]. EpiSoon takes in reported cases to estimate the trajectory of $R_t$, the average number of people infected by each infected person. This trajectory is then forecasted into the future using forecastHybrid and transformed back to incidences. These three models are fit independently to each location. 
MAYBE MORE DETAILS ON THE RENEWAL EQUATION HERE OR ABOVE AS IT'S OUR MODEL
Future cases were model using an exponential model, i.e. cases tomorrow were modeled as past cases times some factor. 

<!-- ## IHME-CurveFit -->

<!-- The Institute for Health Metrics and Evaluation (IHME) seems to be using an SEIR model for their model predictions. While the metadata submitted to the Forecast Hub mention an R package called `CurveFit` (https://github.com/ihmeuw-msca/CurveFit), the `CurveFit` github page point to two other repositories with code for SEIR models and their website IHME (https://ihmeuw-msca.github.io/CurveFit/) confirms this.  -->
  
  
### UMass-MechBayes

UMass-MechBayes is a Bayesian model build from a classical SEIR compartmental model. All parameters have prior distributions and time-varying dynamics as well as noise from observations are modeled explicitly. In addition, it includes two compartments 'dead' and 'hospitalised and will die'. The model is fit independently to each state. More information is available at [github.com/dsheldon/covid](https://github.com/dsheldon/covid). 


### YYG-ParamSearch

The ParamSearch model is also based on a SEIR model, but an additional layer is added to learn optimal hyperparameters for the model. Some of the parameters like the infectious period are shared across locations, but most parameters like the mortality, $R_t$ etc. are determined per state. More information is available at [covid19-projections.com](https://covid19-projections.com) and [github.com/youyanggu/covid19_projections](https://github.com/youyanggu/covid19_projections). 


### CU-select

The CU-select model is another SEIR model that is augmented by human insight. The Columbia University regularly submits a range of projections under different scenarios. For the CU-select model they always hand-pick the one they believe is most plausible. 

### UT-Mobility

The UT-Mobility model [@woodyProjectionsFirstwaveCOVID192020]) uses a Bayesian negative binomial regression model. One of the major predictors used is GPS mobility data provided by a company called SafeGraph. This data is used to compute measures of social distancing that are ultimately fed into the model. 

### LANL-GrowthRate

The GrowthRate model form the Los Alamos National Labs is a two component model. The first component models the number of infections using a time-varying growth parameter that connects present (or future) infections to earlier infections and the number of susceptibles. In a second step, this infections get mapped to reported deaths by assuming a fraction of infections likely to die. 




