# Forecasting and evaluation

The following chapter will give a quick overview of different types of forecasts and will detail strategies to score and evaluate these forecasts. 

## Types of forecasts

Suppose we were interested in the number of Covid-19 cases in December 2020. The number of infections is generated by some data-generating process unfortunately unknown to us. We could approach this forecasting problem in a variety of different ways. The simplest approach is a point forecast. For example, we could predict that the number of cases will be 50,000 cases. This, in essence, is an estimate of the mean of the unknown true data-generating distribution. We coould go one step further and also try and quantify our uncertainty by stating a variance or standard deviation. This could be understood as a point forecast for the variance of the distribution - or it could be understood as a first step towards a probabilistic forecast. 

Ideally, we would like state our predictions in terms of predictive distributions. This allows us to state our beliefs about all aspects of the underlying data-generating distribution (including e.g. skewness or the width of its tails) and helps us to accurately quantify our uncertainty. We could for example make a forecast that the number of cases in December 2020 follows a discretised normal distribution with a mean of 50,000 and a variance of 2,000. If we wanted to model uncertainty differently, maybe a different distribution like poisson or negative binomial could be appropriate. Such a forecast is called a probabilistic forecast. This master thesis will from now on almost exclusively deal with probabilistic forecasts. 

Instead of explicitly specifying a predictive distribution, we can make our forecasts in the form of predictive samples. This is especially handy as we can use MCMC algorithms to generate predictions if no analytical solution is available. The downside is that predictive samples take a lot of storage space. They also come with a loss of precision that is especially pronounced in the tails of the predictive distribution, where we need quite a lot of samples to characterise the distribution accurately. 

To circumvent these problems, we may decide to store the quantiles of the predictive distribution. Quantile forecasts can easily be obtained from explicit distributional forecasts as well as from predictive samples. Setting aside rather philosophical debates, we shall describe quantile forecasts as probabilistic forecasts as well. 

Note that we could also in principle state our forecasts in a binary way. We could for example ask: "Will the number of cases in December 2020 exceed 50,000?" and give a probability that this will happen. This type of forecasting is common in many Machine Learning and classification problems, but is beyond the scope of this thesis. 



## What is a good forecast? - The forecasting paradigm
We want our forecasts be as precises and as close to the true value, as possible. But it is not immediately clear how deviations should be taken into account. I.e. be very correct most of the time and very wrong sometimes? Or better to be a bit wrong most of the time? --> depends on the goal of the forecaster. Different metrics focus on different aspects. 

The general forecastig paradmigm as formulated by Gneiting: We want to maximise sharpness of the predictive distribution subject to calibration. EXPLANATION SHARPNESS AND CALIBRATION. 

Ideally, the predictive distribution == data generating distribution. 

## Assessing forecasts

In order to be able to improve our forecasts, we need to be able to assess them. There are a few ways to approach assessing forecasts. We can either look assess calibration and sharpness independently in different ways, or make use of proper scoring rules that allow us to represent the quality of a forecast in one numeric value. The following chapter will first detail some approaches that help get a sense of calibration and forecasts. Afterwards, different proper scoring rules will be introduced and discussed. 


### Assesssing calibration 
Calibration is a property that broadly captures consistency between the predictive distribution and the true data-generating distribution. Miscalibration is characterised by systematic deviations of the predictive distribution from the true distribution. These deviations can be quite obviuous, such as a consistent overestimation or a consistent underestimation of uncertainty. But they can also be more subtle and harder to detect, for example if the tails of the predictive distribution are a bit too wide. To detect miscalibration, we will look at the so called probability integral transformation (PIT) as well as a closely related bias metric to assess systematic over- or underprediction. Let us first look at bias. 

#### Assessing systematic bias

For continuous forecasts, bias can be measured as
$$B_t (P_t, x_t) = 1 - 2 \cdot (P_t (x_t))$$

where $P_t$ is the empirical cumulative distribution function of the prediction for the true value $x_t$. Computationally, $P_t (x_t)$ is just calculated as the fraction of predictive samples for $x_t$ that are smaller than $x_t$. 

For integer valued forecasts, Bias can be measured as

$$B_t (P_t, x_t) = 1 - (P_t (x_t) + P_t (x_t + 1))$$

to adjust for the integer nature of the forecasts. In both cases, Bias can assume values between -1 and 1 and is 0 ideally.

CITATION Seb, CITATION DERIVATION



#### Assessing calibration

THINKING: Is there a better way than PIT? 

#### Calibration for integer forecasts

#### Calibration for quantile forecasts



### Assessing sharpness


### Scoring point forecasts
Numerous different metrics are available to help evaluate the quality of point forecasts. The package `metrics` lists XXX. Most important: Mean Squared Error (MSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE). 

--> bottom line: 

### Assessing probabilistic forecasts
Scoring probabilistic forecasts is more difficult, as the entire predictive distribution has to be taken into account. This can be achieved using so called proper scoring rules CITATION. 

### CRPS

### Log Score

### DSS

### Interval Score

### Assessing Calibration

Other metrics

### Scoring quantile forecasts
Interval Score

## Interval Score
The Interval Score is a Proper Scoring Rule to score quantile predictions,
following Gneiting and Raftery (2007). Smaller values are better.

The score is computed as


$$ \text{score} = (\text{upper} - \text{lower}) + \\
\frac{2}{\alpha} \cdot (\text{lower} - \text{true_value}) \cdot 1(\text{true_values} < \text{lower}) + \\
\frac{2}{\alpha} \cdot (\text{true_value} - \text{upper}) \cdot
1(\text{true_value} > \text{upper})$$

where $1()$ is the indicator function and $\alpha$ is the decimal value that indicates how much is outside the prediction interval. No specific distribution is assumed. One can weigh the score by $\frac{\alpha}{2}$ such that 
the Interval Score converges to the CRPS for increasing number of quantiles. CITATION. 


## The scoringutils package
In order to score the predictions made for the purpose of this thesis, a collection of metrics and proper scoring rules were bundled in an `R` package called `scoringutils`. CITATION. In the package included are the above described metrics as well as functionality to automatically score a set of predictions with the appropriate metrics.  
