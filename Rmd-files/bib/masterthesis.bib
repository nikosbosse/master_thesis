
@article{held_probabilistic_2017,
	title = {Probabilistic forecasting in infectious disease epidemiology: the 13th {Armitage} lecture},
	volume = {36},
	copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	shorttitle = {Probabilistic forecasting in infectious disease epidemiology},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7363},
	doi = {10.1002/sim.7363},
	abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {22},
	urldate = {2019-09-16},
	journal = {Statistics in Medicine},
	author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
	year = {2017},
	keywords = {age-structured contact matrix, endemic–epidemic modelling, multivariate probabilistic forecasting, proper scoring rules, spatio-temporal surveillance data},
	pages = {3443--3460},
	file = {Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:/mnt/data/Google Drive/Zotero/storage/DIJH7TNP/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:application/pdf}
}

@article{funk_assessing_2019,
	title = {Assessing the performance of real-time epidemic forecasts: {A} case study of {Ebola} in the {Western} {Area} region of {Sierra} {Leone}, 2014-15},
	volume = {15},
	issn = {1553-7358},
	shorttitle = {Assessing the performance of real-time epidemic forecasts},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006785},
	doi = {10.1371/journal.pcbi.1006785},
	abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013–16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
	language = {en},
	number = {2},
	urldate = {2019-09-16},
	journal = {PLOS Computational Biology},
	author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Lowe, Rachel and Eggo, Rosalind M. and Edmunds, W. John},
	month = feb,
	year = {2019},
	keywords = {Epidemiology, Infectious disease epidemiology, Probability distribution, Mathematical models, Forecasting, Infectious diseases, Public and occupational health, Sierra Leone},
	pages = {e1006785},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/JN28VVKF/article.html:text/html}
}

@article{morgan_how_2019,
	title = {How decision makers can use quantitative approaches to guide outbreak responses},
	volume = {374},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0365},
	doi = {10.1098/rstb.2018.0365},
	abstract = {Decision makers are responsible for directing staffing, logistics, selecting public health interventions, communicating to professionals and the public, planning future response needs, and establishing strategic and tactical priorities along with their funding requirements. Decision makers need to rapidly synthesize data from different experts across multiple disciplines, bridge data gaps and translate epidemiological analysis into an operational set of decisions for disease control. Analytic approaches can be defined for specific response phases: investigation, scale-up and control. These approaches include: improved applications of quantitative methods to generate insightful epidemiological descriptions of outbreaks; robust investigations of causal agents and risk factors; tools to assess response needs; identifying and monitoring optimal interventions or combinations of interventions; and forecasting for response planning. Data science and quantitative approaches can improve decision-making in outbreak response. To realize these benefits, we need to develop a structured approach that will improve the quality and timeliness of data collected during outbreaks, establish analytic teams within the response structure and define a research agenda for data analytics in outbreak response.This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control’. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.},
	number = {1776},
	urldate = {2019-09-16},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Morgan, Oliver},
	month = jul,
	year = {2019},
	pages = {20180365},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/L6K9JPT7/Morgan - 2019 - How decision makers can use quantitative approache.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/CXQI7XFI/rstb.2018.html:text/html}
}

@article{kraemer_utilizing_2019,
	title = {Utilizing general human movement models to predict the spread of emerging infectious diseases in resource poor settings},
	volume = {9},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-019-41192-3},
	doi = {10.1038/s41598-019-41192-3},
	language = {en},
	number = {1},
	urldate = {2019-09-16},
	journal = {Scientific Reports},
	author = {Kraemer, M. U. G. and Golding, N. and Bisanzio, D. and Bhatt, S. and Pigott, D. M. and Ray, S. E. and Brady, O. J. and Brownstein, J. S. and Faria, N. R. and Cummings, D. A. T. and Pybus, O. G. and Smith, D. L. and Tatem, A. J. and Hay, S. I. and Reiner, R. C.},
	month = dec,
	year = {2019},
	pages = {5151},
	file = {Kraemer et al. - 2019 - Utilizing general human movement models to predict.pdf:/mnt/data/Google Drive/Zotero/storage/Y98P356R/Kraemer et al. - 2019 - Utilizing general human movement models to predict.pdf:application/pdf}
}

@article{wang_deep_2019,
	title = {Deep {Learning} for {Spatio}-{Temporal} {Data} {Mining}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Spatio}-{Temporal} {Data} {Mining}},
	url = {https://arxiv.org/abs/1906.04928v2},
	abstract = {With the fast development of various positioning techniques such as Global
Position System (GPS), mobile devices and remote sensing, spatio-temporal data
has become increasingly available nowadays. Mining valuable knowledge from
spatio-temporal data is critically important to many real world applications
including human mobility understanding, smart transportation, urban planning,
public safety, health care and environmental management. As the number, volume
and resolution of spatio-temporal datasets increase rapidly, traditional data
mining methods, especially statistics based methods for dealing with such data
are becoming overwhelmed. Recently, with the advances of deep learning
techniques, deep leaning models such as convolutional neural network (CNN) and
recurrent neural network (RNN) have enjoyed considerable success in various
machine learning tasks due to their powerful hierarchical feature learning
ability in both spatial and temporal domains, and have been widely applied in
various spatio-temporal data mining (STDM) tasks such as predictive learning,
representation learning, anomaly detection and classification. In this paper,
we provide a comprehensive survey on recent progress in applying deep learning
techniques for STDM. We first categorize the types of spatio-temporal data and
briefly introduce the popular deep learning models that are used in STDM. Then
a framework is introduced to show a general pipeline of the utilization of deep
learning models for STDM. Next we classify existing literatures based on the
types of ST data, the data mining tasks, and the deep learning models, followed
by the applications of deep learning for STDM in different domains including
transportation, climate science, human mobility, location based social network,
crime analysis, and neuroscience. Finally, we conclude the limitations of
current research and point out future research directions.},
	language = {en},
	urldate = {2019-09-19},
	author = {Wang, Senzhang and Cao, Jiannong and Yu, Philip S.},
	month = jun,
	year = {2019},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/D7VKGB9B/Wang et al. - 2019 - Deep Learning for Spatio-Temporal Data Mining A S.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/KQZYU92I/1906.html:text/html}
}

@inproceedings{jain_structural-rnn:_2016,
	address = {Las Vegas, NV, USA},
	title = {Structural-{RNN}: {Deep} {Learning} on {Spatio}-{Temporal} {Graphs}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Structural-{RNN}},
	url = {http://ieeexplore.ieee.org/document/7780942/},
	doi = {10.1109/CVPR.2016.573},
	abstract = {Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can beneﬁt from it. Spatiotemporal graphs are a popular tool for imposing such highlevel intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatiotemporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well deﬁned steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.},
	language = {en},
	urldate = {2019-09-19},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jain, Ashesh and Zamir, Amir R. and Savarese, Silvio and Saxena, Ashutosh},
	month = jun,
	year = {2016},
	pages = {5308--5317},
	file = {2019.12.27.889212v1.full.pdf:/mnt/data/Google Drive/Zotero/storage/PGL5S7MA/2019.12.27.889212v1.full.pdf:application/pdf;Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf:/mnt/data/Google Drive/Zotero/storage/4B2UEDPF/Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf:application/pdf}
}

@misc{tang_st-lstm:_2019,
	type = {Research article},
	title = {{ST}-{LSTM}: {A} {Deep} {Learning} {Approach} {Combined} {Spatio}-{Temporal} {Features} for {Short}-{Term} {Forecast} in {Rail} {Transit}},
	shorttitle = {{ST}-{LSTM}},
	url = {https://www.hindawi.com/journals/jat/2019/8392592/},
	abstract = {The short-term forecast of rail transit is one of the most essential issues in urban intelligent transportation system (ITS). Accurate forecast result can provide support for the forewarning of flow outburst and enables passengers to make an appropriate travel plan. Therefore, it is significant to develop a more accurate forecast model. Long short-term memory (LSTM) network has been proved to be effective on data with temporal features. However, it cannot process the correlation between time and space in rail transit. As a result, a novel forecast model combining spatio-temporal features based on LSTM network (ST-LSTM) is proposed. Different from other forecast methods, ST-LSTM network uses a new method to extract spatio-temporal features from the data and combines them together as the input. Compared with other conventional models, ST-LSTM network can achieve a better performance in experiments.},
	language = {en},
	urldate = {2019-09-19},
	journal = {Journal of Advanced Transportation},
	author = {Tang, Qicheng and Yang, Mengning and Yang, Ying},
	year = {2019},
	doi = {10.1155/2019/8392592},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/B28NWTUQ/Tang et al. - 2019 - ST-LSTM A Deep Learning Approach Combined Spatio-.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/PFQZ6RKG/8392592.html:application/xhtml+xml}
}

@misc{lee_geoslegend/deep-learning-for-spatio-temporal-prediction_2019,
	title = {geoslegend/{Deep}-{Learning}-for-{Spatio}-temporal-{Prediction}},
	url = {https://github.com/geoslegend/Deep-Learning-for-Spatio-temporal-Prediction},
	abstract = {Contribute to geoslegend/Deep-Learning-for-Spatio-temporal-Prediction development by creating an account on GitHub.},
	urldate = {2019-09-19},
	author = {Lee, Seongkyu},
	month = sep,
	year = {2019},
	note = {original-date: 2018-11-01T02:02:00Z}
}

@article{held_probabilistic_2017-1,
	title = {Probabilistic forecasting in infectious disease epidemiology: the 13th {Armitage} lecture: {L}. {HELD}, {S}. {MEYER} {AND} {J}. {BRACHER}},
	volume = {36},
	issn = {02776715},
	shorttitle = {Probabilistic forecasting in infectious disease epidemiology},
	url = {http://doi.wiley.com/10.1002/sim.7363},
	doi = {10.1002/sim.7363},
	language = {en},
	number = {22},
	urldate = {2019-09-21},
	journal = {Statistics in Medicine},
	author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
	month = sep,
	year = {2017},
	pages = {3443--3460},
	file = {Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:/mnt/data/Google Drive/Zotero/storage/4GBAAE6J/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:application/pdf}
}

@article{schobel_statistical_nodate,
	title = {Statistical {Inferencefor} {Propagation} {Processeson} {Complex} {Networks}},
	language = {en},
	author = {Schöbel, Dr Anita and Göttingen, Georg-August-Universität and Kneib, Dr Thomas},
	pages = {200},
	file = {Statistical Inferencefor Propagation Processeson Complex Networks.pdf:/mnt/data/Google Drive/Zotero/storage/L2YVK83S/Schöbel et al. - Mitglieder der Prüfungskommision.pdf:application/pdf}
}

@article{cooper_method_2015,
	title = {A method for detecting and characterizing outbreaks of infectious disease from clinical reports},
	volume = {53},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046414001920},
	doi = {10.1016/j.jbi.2014.08.011},
	abstract = {Outbreaks of infectious disease can pose a signiﬁcant threat to human health. Thus, detecting and characterizing outbreaks quickly and accurately remains an important problem. This paper describes a Bayesian framework that links clinical diagnosis of individuals in a population to epidemiological modeling of disease outbreaks in the population. Computer-based diagnosis of individuals who seek healthcare is used to guide the search for epidemiological models of population disease that explain the pattern of diagnoses well. We applied this framework to develop a system that detects inﬂuenza outbreaks from emergency department (ED) reports. The system diagnoses inﬂuenza in individuals probabilistically from evidence in ED reports that are extracted using natural language processing. These diagnoses guide the search for epidemiological models of inﬂuenza that explain the pattern of diagnoses well. Those epidemiological models with a high posterior probability determine the most likely outbreaks of speciﬁc diseases; the models are also used to characterize properties of an outbreak, such as its expected peak day and estimated size. We evaluated the method using both simulated data and data from a real inﬂuenza outbreak. The results provide support that the approach can detect and characterize outbreaks early and well enough to be valuable. We describe several extensions to the approach that appear promising.},
	language = {en},
	urldate = {2019-10-08},
	journal = {Journal of Biomedical Informatics},
	author = {Cooper, Gregory F. and Villamarin, Ricardo and (Rich) Tsui, Fu-Chiang and Millett, Nicholas and Espino, Jeremy U. and Wagner, Michael M.},
	month = feb,
	year = {2015},
	pages = {15--26},
	file = {Cooper et al. - 2015 - A method for detecting and characterizing outbreak.pdf:/mnt/data/Google Drive/Zotero/storage/TPYMSQ39/Cooper et al. - 2015 - A method for detecting and characterizing outbreak.pdf:application/pdf}
}

@article{meyer_hhh4:_nodate,
	title = {hhh4: {Endemic}-epidemic modeling of areal count time series},
	abstract = {The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The R package surveillance can handle various levels of aggregation at which infective events have been recorded. This vignette illustrates the analysis of area-level time series of counts using the endemic-epidemic multivariate time-series model “hhh4” described in, e.g., Meyer and Held (2014, Section 3). See vignette("hhh4") for a more general introduction to hhh4 models, including the univariate and non-spatial bivariate case. We ﬁrst describe the general modeling approach and then exemplify data handling, model ﬁtting, visualization, and simulation methods for weekly counts of measles infections by district in the Weser-Ems region of Lower Saxony, Germany, 2001–2002.},
	language = {en},
	author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
	pages = {23},
	file = {Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:/mnt/data/Google Drive/Zotero/storage/UJLPF42G/Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:application/pdf}
}

@article{meyer_spatio-temporal_2017,
	title = {Spatio-{Temporal} {Analysis} of {Epidemic} {Phenomena} {Using} the {R} {Package} surveillance},
	volume = {77},
	copyright = {Copyright (c) 2017 Sebastian Meyer, Leonhard Held, Michael Höhle},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v077i11},
	doi = {10.18637/jss.v077.i11},
	language = {en},
	number = {1},
	urldate = {2019-10-29},
	journal = {Journal of Statistical Software},
	author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
	month = may,
	year = {2017},
	keywords = {spatio-temporal surveillance data, branching process with immigration, endemic-epidemic modeling, infectious disease epidemiology, multivariate time series of counts, self-exciting point process},
	pages = {1--55},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/AJL3KXFX/Meyer et al. - 2017 - Spatio-Temporal Analysis of Epidemic Phenomena Usi.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/K3RTGPW8/v077i11.html:text/html}
}

@article{lessler_mapping_2018,
	title = {Mapping the burden of cholera in sub-{Saharan} {Africa} and implications for control: an analysis of data across geographical scales},
	volume = {391},
	issn = {0140-6736, 1474-547X},
	shorttitle = {Mapping the burden of cholera in sub-{Saharan} {Africa} and implications for control},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(17)33050-7/abstract},
	doi = {10.1016/S0140-6736(17)33050-7},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Cholera remains a persistent health problem in sub-Saharan Africa and worldwide. Cholera can be controlled through appropriate water and sanitation, or by oral cholera vaccination, which provides transient (∼3 years) protection, although vaccine supplies remain scarce. We aimed to map cholera burden in sub-Saharan Africa and assess how geographical targeting could lead to more efficient interventions.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}We combined information on cholera incidence in sub-Saharan Africa (excluding Djibouti and Eritrea) from 2010 to 2016 from datasets from WHO, Médecins Sans Frontières, ProMED, ReliefWeb, ministries of health, and the scientific literature. We divided the study region into 20 km × 20 km grid cells and modelled annual cholera incidence in each grid cell assuming a Poisson process adjusted for covariates and spatially correlated random effects. We combined these findings with data on population distribution to estimate the number of people living in areas of high cholera incidence ({\textgreater}1 case per 1000 people per year). We further estimated the reduction in cholera incidence that could be achieved by targeting cholera prevention and control interventions at areas of high cholera incidence.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}We included 279 datasets covering 2283 locations in our analyses. In sub-Saharan Africa (excluding Djibouti and Eritrea), a mean of 141 918 cholera cases (95\% credible interval [CrI] 141 538–146 505) were reported per year. 4·0\% (95\% CrI 1·7–16·8) of districts, home to 87·2 million people (95\% CrI 60·3 million to 118·9 million), have high cholera incidence. By focusing on the highest incidence districts first, effective targeted interventions could eliminate 50\% of the region's cholera by covering 35·3 million people (95\% CrI 26·3 million to 62·0 million), which is less than 4\% of the total population.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}Although cholera occurs throughout sub-Saharan Africa, its highest incidence is concentrated in a small proportion of the continent. Prioritising high-risk areas could substantially increase the efficiency of cholera control programmes.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}The Bill \& Melinda Gates Foundation.{\textless}/p{\textgreater}},
	language = {English},
	number = {10133},
	urldate = {2019-10-29},
	journal = {The Lancet},
	author = {Lessler, Justin and Moore, Sean M. and Luquero, Francisco J. and McKay, Heather S. and Grais, Rebecca and Henkens, Myriam and Mengel, Martin and Dunoyer, Jessica and M'bangombe, Maurice and Lee, Elizabeth C. and Djingarey, Mamoudou Harouna and Sudre, Bertrand and Bompangue, Didier and Fraser, Robert S. M. and Abubakar, Abdinasir and Perea, William and Legros, Dominique and Azman, Andrew S.},
	month = may,
	year = {2018},
	pmid = {29502905},
	pages = {1908--1915},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/R34ESHXT/Lessler et al. - 2018 - Mapping the burden of cholera in sub-Saharan Afric.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/G9WXKBP6/fulltext.html:text/html}
}

@article{badkundri_forecasting_2019,
	title = {Forecasting the 2017-2018 {Yemen} {Cholera} {Outbreak} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1902.06739},
	abstract = {The ongoing Yemen cholera outbreak has been deemed one of the worst cholera outbreaks in history, with over a million people impacted and thousands dead. Triggered by a civil war, the outbreak has been shaped by various political, environmental, and epidemiological factors and continues to worsen. While cholera has several effective treatments, the untimely and inefficient distribution of existing medicines has been the primary cause of cholera mortality. With the hope of facilitating resource allocation, various mathematical models have been created to track the Yemeni outbreak and identify at-risk administrative divisions, called governorates. Existing models are not powerful enough to accurately and consistently forecast cholera cases per governorate over multiple timeframes. To address the need for a complex, reliable model, we offer the Cholera Artificial Learning Model (CALM); a system of 4 extreme-gradient-boosting (XGBoost) machine learning models that forecast the number of new cholera cases a Yemeni governorate will experience from a time range of 2 weeks to 2 months. CALM provides a novel machine learning approach that makes use of rainfall data, past cholera cases and deaths data, civil war fatalities, and inter-governorate interactions represented across multiple time frames. Additionally, the use of machine learning, along with extensive feature engineering, allows CALM to easily learn complex non-linear relations apparent in an epidemiological phenomenon. CALM is able to forecast cholera incidence 2 weeks to 2 months in advance within a margin of just 5 cholera cases per 10,000 people in real-world simulation.},
	urldate = {2019-10-29},
	journal = {arXiv:1902.06739 [cs, q-bio]},
	author = {Badkundri, Rohil and Valbuena, Victor and Pinnamareddy, Srikusmanjali and Cantrell, Brittney and Standeven, Janet},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06739},
	keywords = {Computer Science - Machine Learning, 68T01, Computer Science - Computers and Society, Quantitative Biology - Quantitative Methods},
	annote = {Comment: Originally completed as part of the iGEM competition (see http://2018.igem.org/Team:Lambert\_GA/Software); 3431 words, 1 table, 2 manuscript figures, 2 supplementary figures},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/CIJMPID3/Badkundri et al. - 2019 - Forecasting the 2017-2018 Yemen Cholera Outbreak w.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/I8V88CEC/1902.html:text/html}
}

@article{camacho_cholera_2018,
	title = {Cholera epidemic in {Yemen}, 2016–18: an analysis of surveillance data},
	volume = {6},
	issn = {2214-109X},
	shorttitle = {Cholera epidemic in {Yemen}, 2016–18},
	url = {https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30230-4/abstract},
	doi = {10.1016/S2214-109X(18)30230-4},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}In war-torn Yemen, reports of confirmed cholera started in late September, 2016. The disease continues to plague Yemen today in what has become the largest documented cholera epidemic of modern times. We aimed to describe the key epidemiological features of this epidemic, including the drivers of cholera transmission during the outbreak.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}The Yemen Health Authorities set up a national cholera surveillance system to collect information on suspected cholera cases presenting at health facilities. Individual variables included symptom onset date, age, severity of dehydration, and rapid diagnostic test result. Suspected cholera cases were confirmed by culture, and a subset of samples had additional phenotypic and genotypic analysis. We first conducted descriptive analyses at national and governorate levels. We divided the epidemic into three time periods: the first wave (Sept 28, 2016, to April 23, 2017), the increasing phase of the second wave (April 24, 2017, to July 2, 2017), and the decreasing phase of the second wave (July 3, 2017, to March 12, 2018). We reconstructed the changes in cholera transmission over time by estimating the instantaneous reproduction number, \textit{R}$_{\textrm{t}}$. Finally, we estimated the association between rainfall and the daily cholera incidence during the increasing phase of the second epidemic wave by fitting a spatiotemporal regression model.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}From Sept 28, 2016, to March 12, 2018, 1 103 683 suspected cholera cases (attack rate 3·69\%) and 2385 deaths (case fatality risk 0·22\%) were reported countrywide. The epidemic consisted of two distinct waves with a surge in transmission in May, 2017, corresponding to a median \textit{R}$_{\textrm{t}}$ of more than 2 in 13 of 23 governorates. Microbiological analyses suggested that the same \textit{Vibrio cholerae} O1 Ogawa strain circulated in both waves. We found a positive, non-linear, association between weekly rainfall and suspected cholera incidence in the following 10 days; the relative risk of cholera after a weekly rainfall of 25 mm was 1·42 (95\% CI 1·31–1·55) compared with a week without rain.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}Our analysis suggests that the small first cholera epidemic wave seeded cholera across Yemen during the dry season. When the rains returned in April, 2017, they triggered widespread cholera transmission that led to the large second wave. These results suggest that cholera could resurge during the ongoing 2018 rainy season if transmission remains active. Therefore, health authorities and partners should immediately enhance current control efforts to mitigate the risk of a new cholera epidemic wave in Yemen.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}Health Authorities of Yemen, WHO, and Médecins Sans Frontières.{\textless}/p{\textgreater}},
	language = {English},
	number = {6},
	urldate = {2019-10-29},
	journal = {The Lancet Global Health},
	author = {Camacho, Anton and Bouhenia, Malika and Alyusfi, Reema and Alkohlani, Abdulhakeem and Naji, Munna Abdulla Mohammed and Radiguès, Xavier de and Abubakar, Abdinasir M. and Almoalmi, Abdulkareem and Seguin, Caroline and Sagrado, Maria Jose and Poncin, Marc and McRae, Melissa and Musoke, Mohammed and Rakesh, Ankur and Porten, Klaudia and Haskew, Christopher and Atkins, Katherine E. and Eggo, Rosalind M. and Azman, Andrew S. and Broekhuijsen, Marije and Saatcioglu, Mehmet Akif and Pezzoli, Lorenzo and Quilici, Marie-Laure and Al-Mesbahy, Abdul Rahman and Zagaria, Nevio and Luquero, Francisco J.},
	month = jun,
	year = {2018},
	pmid = {29731398},
	pages = {e680--e690},
	file = {1-s2.0-S2214109X18302304-mmc1.pdf:/mnt/data/Google Drive/Zotero/storage/3LTL4IF4/1-s2.0-S2214109X18302304-mmc1.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/5X397UB8/Camacho et al. - 2018 - Cholera epidemic in Yemen, 2016–18 an analysis of.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/7F6Q9SUT/fulltext.html:text/html}
}

@article{wells_exacerbation_2019,
	title = {The exacerbation of {Ebola} outbreaks by conflict in the {Democratic} {Republic} of the {Congo}},
	copyright = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/early/2019/10/15/1913980116},
	doi = {10.1073/pnas.1913980116},
	abstract = {The interplay between civil unrest and disease transmission is not well understood. Violence targeting healthcare workers and Ebola treatment centers in the Democratic Republic of the Congo (DRC) has been thwarting the case isolation, treatment, and vaccination efforts. The extent to which conflict impedes public health response and contributes to incidence has not previously been evaluated. We construct a timeline of conflict events throughout the course of the epidemic and provide an ethnographic appraisal of the local conditions that preceded and followed conflict events. Informed by temporal incidence and conflict data as well as the ethnographic evidence, we developed a model of Ebola transmission and control to assess the impact of conflict on the epidemic in the eastern DRC from April 30, 2018, to June 23, 2019. We found that both the rapidity of case isolation and the population-level effectiveness of vaccination varied notably as a result of preceding unrest and subsequent impact of conflict events. Furthermore, conflict events were found to reverse an otherwise declining phase of the epidemic trajectory. Our model framework can be extended to other infectious diseases in the same and other regions of the world experiencing conflict and violence.},
	language = {en},
	urldate = {2019-10-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Wells, Chad R. and Pandey, Abhishek and Mbah, Martial L. Ndeffo and Gaüzère, Bernard-A. and Malvy, Denis and Singer, Burton H. and Galvani, Alison P.},
	month = oct,
	year = {2019},
	pmid = {31636188},
	keywords = {epidemiology, healthcare workers, humanitarian crisis, insecurity},
	pages = {201913980},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/ULSEZQQ3/Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/EH3MGU3D/1913980116.html:text/html}
}

@article{cori_new_2013,
	title = {A {New} {Framework} and {Software} to {Estimate} {Time}-{Varying} {Reproduction} {Numbers} {During} {Epidemics}},
	volume = {178},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/178/9/1505/89262},
	doi = {10.1093/aje/kwt133},
	abstract = {Abstract.  The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be meas},
	language = {en},
	number = {9},
	urldate = {2019-10-30},
	journal = {American Journal of Epidemiology},
	author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
	month = nov,
	year = {2013},
	pages = {1505--1512},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/827JXSHL/Cori et al. - 2013 - A New Framework and Software to Estimate Time-Vary.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/UY7M5Z9A/89262.html:text/html}
}

@article{thompson_improved_2019,
	title = {Improved inference of time-varying reproduction numbers during infectious disease outbreaks},
	issn = {1755-4365},
	url = {http://www.sciencedirect.com/science/article/pii/S1755436519300350},
	doi = {10.1016/j.epidem.2019.100356},
	abstract = {Accurate estimation of the parameters characterising infectious disease transmission is vital for optimising control interventions during epidemics. A valuable metric for assessing the current threat posed by an outbreak is the time-dependent reproduction number, i.e. the expected number of secondary cases caused by each infected individual. This quantity can be estimated using data on the numbers of observed new cases at successive times during an epidemic and the distribution of the serial interval (the time between symptomatic cases in a transmission chain). Some methods for estimating the reproduction number rely on pre-existing estimates of the serial interval distribution and assume that the entire outbreak is driven by local transmission. Here we show that accurate inference of current transmissibility, and the uncertainty associated with this estimate, requires: (i) up-to-date observations of the serial interval to be included, and; (ii) cases arising from local transmission to be distinguished from those imported from elsewhere. We demonstrate how pathogen transmissibility can be inferred appropriately using datasets from outbreaks of H1N1 influenza, Ebola virus disease and Middle-East Respiratory Syndrome. We present a tool for estimating the reproduction number in real-time during infectious disease outbreaks accurately, which is available as an R software package (EpiEstim 2.2). It is also accessible as an interactive, user-friendly online interface (EpiEstim App), permitting its use by non-specialists. Our tool is easy to apply for assessing the transmission potential, and hence informing control, during future outbreaks of a wide range of invading pathogens.},
	language = {en},
	urldate = {2019-10-30},
	journal = {Epidemics},
	author = {Thompson, R. N. and Stockwin, J. E. and van Gaalen, R. D. and Polonsky, J. A. and Kamvar, Z. N. and Demarsh, P. A. and Dahlqwist, E. and Li, S. and Miguel, E. and Jombart, T. and Lessler, J. and Cauchemez, S. and Cori, A.},
	month = aug,
	year = {2019},
	keywords = {Infectious disease epidemiology, Disease control, Mathematical modelling, Parameter inference, Reproduction number, Serial interval},
	pages = {100356},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/QLQPSF74/Thompson et al. - 2019 - Improved inference of time-varying reproduction nu.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/Y8GML2T9/S1755436519300350.html:text/html}
}

@article{gasparrini_penalized_2017,
	title = {A penalized framework for distributed lag non-linear models},
	volume = {73},
	issn = {1541-0420},
	doi = {10.1111/biom.12645},
	abstract = {Distributed lag non-linear models (DLNMs) are a modelling tool for describing potentially non-linear and delayed dependencies. Here, we illustrate an extension of the DLNM framework through the use of penalized splines within generalized additive models (GAM). This extension offers built-in model selection procedures and the possibility of accommodating assumptions on the shape of the lag structure through specific penalties. In addition, this framework includes, as special cases, simpler models previously proposed for linear relationships (DLMs). Alternative versions of penalized DLNMs are compared with each other and with the standard unpenalized version in a simulation study. Results show that this penalized extension to the DLNM class provides greater flexibility and improved inferential properties. The framework exploits recent theoretical developments of GAMs and is implemented using efficient routines within freely available software. Real-data applications are illustrated through two reproducible examples in time series and survival analysis.},
	language = {eng},
	number = {3},
	journal = {Biometrics},
	author = {Gasparrini, Antonio and Scheipl, Fabian and Armstrong, Ben and Kenward, Michael G.},
	year = {2017},
	pmid = {28134978},
	keywords = {Software, Distributed lag, Exposure-lag-response, Generalized additive models, Latency, Nonlinear Dynamics, Penalized splines},
	pages = {938--948},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/2MRP9PLR/Gasparrini et al. - 2017 - A penalized framework for distributed lag non-line.pdf:application/pdf}
}

@misc{noauthor_2016_gasparrini_epidem_nodate,
	title = {2016\_gasparrini\_Epidem},
	url = {http://www.ag-myresearch.com/2016_gasparrini_epidem.html},
	abstract = {Gasparrini A Epidemiology . 2016; 27 (6):835-842},
	language = {en},
	urldate = {2019-10-31},
	journal = {Antonio Gasparrini: my research},
	annote = {Added R code
github.com/gasparrini/2016\_gasparrini\_Epidem\_Rcode},
	file = {2016_gasparrini_Epidem.pdf:/mnt/data/Google Drive/Zotero/storage/GFY3ANQU/2016_gasparrini_Epidem.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/YRVFWKQT/2016_gasparrini_epidem.html:text/html}
}

@article{gasparrini_distributed_2011,
	title = {Distributed {Lag} {Linear} and {Non}-{Linear} {Models} in \textit{{R}} : {The} {Package} \textbf{dlnm}},
	volume = {43},
	issn = {1548-7660},
	shorttitle = {Distributed {Lag} {Linear} and {Non}-{Linear} {Models} in \textit{{R}}},
	url = {http://www.jstatsoft.org/v43/i08/},
	doi = {10.18637/jss.v043.i08},
	abstract = {Distributed lag non-linear models (DLNMs) represent a modeling framework to ﬂexibly describe associations showing potentially non-linear and delayed eﬀects in time series data. This methodology rests on the deﬁnition of a crossbasis, a bi-dimensional functional space expressed by the combination of two sets of basis functions, which specify the relationships in the dimensions of predictor and lags, respectively. This framework is implemented in the R package dlnm, which provides functions to perform the broad range of models within the DLNM family and then to help interpret the results, with an emphasis on graphical representation. This paper oﬀers an overview of the capabilities of the package, describing the conceptual and practical steps to specify and interpret DLNMs with an example of application to real data.},
	language = {en},
	number = {8},
	urldate = {2019-10-31},
	journal = {Journal of Statistical Software},
	author = {Gasparrini, Antonio},
	year = {2011},
	file = {Gasparrini - 2011 - Distributed Lag Linear and Non-Linear Models in i.pdf:/mnt/data/Google Drive/Zotero/storage/J4FFTYLL/Gasparrini - 2011 - Distributed Lag Linear and Non-Linear Models in i.pdf:application/pdf}
}

@article{funk_real-time_2018,
	title = {Real-time forecasting of infectious disease dynamics with a stochastic semi-mechanistic model},
	volume = {22},
	issn = {1755-4365},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5871642/},
	doi = {10.1016/j.epidem.2016.11.003},
	abstract = {•
              A Bayesian semi-mechanistic model was applied to the Ebola Forecasting Challenge.
            
            
              •
              Model fits to simulated data were obtained from particle Markov-chain Monte Carlo.
            
            
              •
              Posterior samples of model parameters were used to generate forecast trajectories.
            
            
              •
              The forecasts were assessed using subsequently released simulation points.
            
          
        , Real-time forecasts of infectious diseases can help public health planning, especially during outbreaks. If forecasts are generated from mechanistic models, they can be further used to target resources or to compare the impact of possible interventions. However, paremeterising such models is often difficult in real time, when information on behavioural changes, interventions and routes of transmission are not readily available. Here, we present a semi-mechanistic model of infectious disease dynamics that was used in real time during the 2013–2016 West African Ebola epidemic, and show fits to a Ebola Forecasting Challenge conducted in late 2015 with simulated data mimicking the true epidemic. We assess the performance of the model in different situations and identify strengths and shortcomings of our approach. Models such as the one presented here which combine the power of mechanistic models with the flexibility to include uncertainty about the precise outbreak dynamics may be an important tool in combating future outbreaks.},
	urldate = {2019-11-01},
	journal = {Epidemics},
	author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Eggo, Rosalind M. and Edmunds, W. John},
	month = mar,
	year = {2018},
	pmid = {28038870},
	pmcid = {PMC5871642},
	pages = {56--61},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/E3U9WS5C/Funk et al. - 2018 - Real-time forecasting of infectious disease dynami.pdf:application/pdf}
}

@misc{link_fitting_nodate,
	title = {Fitting {Bayesian} structural time series with the bsts {R} package},
	url = {http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html},
	abstract = {by STEVEN L. SCOTT   Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data scien...},
	urldate = {2019-11-04},
	author = {link, Get and Facebook and Twitter and Pinterest and Email and Apps, Other},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/6I3IP6MX/fitting-bayesian-structural-time-series.html:text/html}
}

@book{arnold_state_nodate,
	title = {State {Space} {Models} in {Stan}},
	url = {https://jrnold.github.io/ssmodels-in-stan/index.html},
	abstract = {Documentation for State Space Models in Stan.},
	urldate = {2019-11-05},
	author = {Arnold, Jeffrey B.},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/5VIFYEF5/index.html:text/html}
}

@article{chatzilena_contemporary_2019,
	title = {Contemporary statistical inference for infectious disease models using {Stan}},
	issn = {1755-4365},
	url = {http://www.sciencedirect.com/science/article/pii/S1755436519300325},
	doi = {10.1016/j.epidem.2019.100367},
	abstract = {This paper is concerned with the application of recent statistical advances to inference of infectious disease dynamics. We describe the fitting of a class of epidemic models using Hamiltonian Monte Carlo and variational inference as implemented in the freely available Stan software. We apply the two methods to real data from outbreaks as well as routinely collected observations. Our results suggest that both inference methods are computationally feasible in this context, and show a trade-off between statistical efficiency versus computational speed. The latter appears particularly relevant for real-time applications.},
	language = {en},
	urldate = {2019-11-06},
	journal = {Epidemics},
	author = {Chatzilena, Anastasia and van Leeuwen, Edwin and Ratmann, Oliver and Baguelin, Marc and Demiris, Nikolaos},
	month = oct,
	year = {2019},
	keywords = {Automatic differentiation variational inference, Epidemic models, Hamiltonian Monte Carlo, No-U-turn sampler, Stan},
	pages = {100367},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/VUJUVH3M/Chatzilena et al. - 2019 - Contemporary statistical inference for infectious .pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/VPECFYK6/S1755436519300325.html:text/html}
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	issn = {09641998},
	url = {http://arxiv.org/abs/1709.01449},
	doi = {10.1111/rssa.12378},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workﬂow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
	language = {en},
	number = {2},
	urldate = {2020-01-13},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	month = feb,
	year = {2019},
	note = {arXiv: 1709.01449},
	keywords = {Statistics - Methodology, Statistics - Applications},
	pages = {389--402},
	annote = {Comment: 17 pages, 11 Figures. Includes supplementary material},
	file = {Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf:/mnt/data/Google Drive/Zotero/storage/B6BPIY9I/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf:application/pdf}
}

@misc{link_fitting_nodate-1,
	title = {Fitting {Bayesian} structural time series with the bsts {R} package},
	url = {http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html},
	abstract = {by STEVEN L. SCOTT   Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data scien...},
	urldate = {2020-01-13},
	author = {link, Get and Facebook and Twitter and Pinterest and Email and Apps, Other},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/RI92BKFZ/fitting-bayesian-structural-time-series.html:text/html}
}

@misc{noauthor_ebola_nodate,
	title = {Ebola {Cases} and {Deaths} in the {North} {Kivu} {Ebola} {Outbreak} in the {Democratic} {Republic} of the {Congo} ({DRC}) - {Humanitarian} {Data} {Exchange}},
	url = {https://data.humdata.org/dataset/ebola-cases-and-deaths-drc-north-kivu},
	urldate = {2020-01-13},
	file = {Ebola Cases and Deaths in the North Kivu Ebola Outbreak in the Democratic Republic of the Congo (DRC) - Humanitarian Data Exchange:/mnt/data/Google Drive/Zotero/storage/TBIT49SJ/ebola-cases-and-deaths-drc-north-kivu.html:text/html}
}

@misc{ro_gulfamsc_ebola_2019,
	title = {Gulfa/msc\_ebola},
	copyright = {GPL-3.0},
	url = {https://github.com/Gulfa/msc_ebola},
	abstract = {Ebola modelling MSC project. Contribute to Gulfa/msc\_ebola development by creating an account on GitHub.},
	urldate = {2020-01-13},
	author = {Ro, Gunnar},
	month = oct,
	year = {2019},
	note = {original-date: 2019-04-14T11:20:17Z}
}

@article{meyer_hhh4_nodate,
	title = {hhh4: {Endemic}-epidemic modeling of areal count time series},
	abstract = {The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The R package surveillance can handle various levels of aggregation at which infective events have been recorded. This vignette illustrates the analysis of area-level time series of counts using the endemic-epidemic multivariate time-series model “hhh4” described in, e.g., Meyer and Held (2014, Section 3). See vignette("hhh4") for a more general introduction to hhh4 models, including the univariate and non-spatial bivariate case. We ﬁrst describe the general modeling approach and then exemplify data handling, model ﬁtting, visualization, and simulation methods for weekly counts of measles infections by district in the Weser-Ems region of Lower Saxony, Germany, 2001–2002.},
	language = {en},
	author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
	pages = {23},
	file = {Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:/mnt/data/Google Drive/Zotero/storage/2BBGK8KT/Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:application/pdf}
}

@article{wells_exacerbation_2019-1,
	title = {The exacerbation of {Ebola} outbreaks by conflict in the {Democratic} {Republic} of the {Congo}},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1913980116},
	doi = {10.1073/pnas.1913980116},
	abstract = {The interplay between civil unrest and disease transmission is not well understood. Violence targeting healthcare workers and Ebola treatment centers in the Democratic Republic of the Congo (DRC) has been thwarting the case isolation, treatment, and vaccination efforts. The extent to which conflict impedes public health response and contributes to incidence has not previously been evaluated. We construct a timeline of conflict events throughout the course of the epidemic and provide an ethnographic appraisal of the local conditions that preceded and followed conflict events. Informed by temporal incidence and conflict data as well as the ethnographic evidence, we developed a model of Ebola transmission and control to assess the impact of conflict on the epidemic in the eastern DRC from April 30, 2018, to June 23, 2019. We found that both the rapidity of case isolation and the population-level effectiveness of vaccination varied notably as a result of preceding unrest and subsequent impact of conflict events. Furthermore, conflict events were found to reverse an otherwise declining phase of the epidemic trajectory. Our model framework can be extended to other infectious diseases in the same and other regions of the world experiencing conflict and violence.},
	language = {en},
	number = {48},
	urldate = {2020-01-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Wells, Chad R. and Pandey, Abhishek and Ndeffo Mbah, Martial L. and Gaüzère, Bernard-A. and Malvy, Denis and Singer, Burton H. and Galvani, Alison P.},
	month = nov,
	year = {2019},
	pages = {24366--24372},
	file = {Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf:/mnt/data/Google Drive/Zotero/storage/4GPMKJMW/Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf:application/pdf}
}

@article{demetrescu_bias_nodate,
	title = {Bias {Corrections} for {Exponentially} {Transformed} {Forecasts}: is it worth the eﬀort?},
	abstract = {In many economic applications the log transformation of the process of interest allows to model and to forecast log values as linear time series. However, a reverse transformation of the log forecasts introduces a bias which should accounted for. In this paper we compare diﬀerent bias correction methods for the reverse transformation of log series following a linear autoregressive process. We ﬁnd that the correction method to choose in ﬁnite samples depends much on the empirical error distribution whereby for some cases no bias correction is advantageous. Our results are illustrated both in Monte Carlo simulations and in an empirical study.},
	language = {en},
	author = {Demetrescu, Matei and Kiel, CAU and Golosnoy, Vasyl and Bochum, Ruhr-University},
	pages = {22},
	file = {Demetrescu et al. - Bias Corrections for Exponentially Transformed For.pdf:/mnt/data/Google Drive/Zotero/storage/Q7WTUXP3/Demetrescu et al. - Bias Corrections for Exponentially Transformed For.pdf:application/pdf}
}

@article{kucharski_effectiveness_nodate,
	title = {Effectiveness of {Ring} {Vaccination} as {Control} {Strategy} for {Ebola} {Virus} {Disease} - {Volume} 22, {Number} 1—{January} 2016 - {Emerging} {Infectious} {Diseases} journal - {CDC}},
	url = {https://wwwnc.cdc.gov/eid/article/22/1/15-1410_article},
	doi = {10.3201/eid2201.151410},
	abstract = {Using an Ebola virus disease transmission model, we found that addition of ring vaccination at the outset of the West Africa epidemic might not have l...},
	language = {en-us},
	urldate = {2020-01-27},
	author = {Kucharski, Adam J. and Eggo, Rosalind M. and Watson, Conall and Camacho, Anton and Funk, Sebastian and Edmunds, W. John},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/GABQRTK5/Kucharski et al. - Effectiveness of Ring Vaccination as Control Strat.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/GKN58IHT/15-1410_article.html:text/html}
}

@article{yang_duration_2014,
	title = {Duration of urination does not change with body size},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/111/33/11932},
	doi = {10.1073/pnas.1402289111},
	abstract = {Many urological studies rely on models of animals, such as rats and pigs, but their relation to the human urinary system is poorly understood. Here, we elucidate the hydrodynamics of urination across five orders of magnitude in body mass. Using high-speed videography and flow-rate measurement obtained at Zoo Atlanta, we discover that all mammals above 3 kg in weight empty their bladders over nearly constant duration of 21 ± 13 s. This feat is possible, because larger animals have longer urethras and thus, higher gravitational force and higher flow speed. Smaller mammals are challenged during urination by high viscous and capillary forces that limit their urine to single drops. Our findings reveal that the urethra is a flow-enhancing device, enabling the urinary system to be scaled up by a factor of 3,600 in volume without compromising its function. This study may help to diagnose urinary problems in animals as well as inspire the design of scalable hydrodynamic systems based on those in nature.},
	language = {en},
	number = {33},
	urldate = {2020-01-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Patricia J. and Pham, Jonathan and Choo, Jerome and Hu, David L.},
	month = aug,
	year = {2014},
	pmid = {24969420},
	keywords = {allometry, Bernoulli's principle, scaling, urology},
	pages = {11932--11937},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/XSC7XIRG/Yang et al. - 2014 - Duration of urination does not change with body si.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/R58JV3WZ/11932.html:text/html}
}

@article{jombart_cost_2020,
	title = {The cost of insecurity: from flare-up to control of a major {Ebola} virus disease hotspot during the outbreak in the {Democratic} {Republic} of the {Congo}, 2019},
	volume = {25},
	issn = {1560-7917},
	shorttitle = {The cost of insecurity},
	url = {https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2020.25.2.1900735},
	doi = {10.2807/1560-7917.ES.2020.25.2.1900735},
	abstract = {The ongoing Ebola outbreak in the eastern Democratic Republic of the Congo is facing unprecedented levels of insecurity and violence. We evaluate the likely impact in terms of added transmissibility and cases of major security incidents in the Butembo coordination hub. We also show that despite this additional burden, an adapted response strategy involving enlarged ring vaccination around clusters of cases and enhanced community engagement managed to bring this main hotspot under control.},
	language = {en},
	number = {2},
	urldate = {2020-01-28},
	journal = {Eurosurveillance},
	author = {Jombart, Thibaut and Jarvis, Christopher I. and Mesfin, Samuel and Tabal, Nabil and Mossoko, Mathias and Mpia, Luigino Minikulu and Abedi, Aaron Aruna and Chene, Sonia and Forbin, Ekokobe Elias and Belizaire, Marie Roseline D. and Radiguès, Xavier de and Ngombo, Richy and Tutu, Yannick and Finger, Flavio and Crowe, Madeleine and Edmunds, W. John and Nsio, Justus and Yam, Abdoulaye and Diallo, Boubacar and Gueye, Abdou Salam and Ahuka-Mundeke, Steve and Yao, Michel and Fall, Ibrahima Socé},
	month = jan,
	year = {2020},
	pages = {1900735},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/PZWFGHLK/Jombart et al. - 2020 - The cost of insecurity from flare-up to control o.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/U2AUSPSS/1560-7917.ES.2020.25.2.html:text/html}
}

@article{gasparrini_distributed_nodate,
	title = {Distributed lag linear and non-linear models for time series data},
	language = {en},
	author = {Gasparrini, Antonio},
	pages = {12},
	file = {Gasparrini - Distributed lag linear and non-linear models for t.pdf:/mnt/data/Google Drive/Zotero/storage/HAUGWLZV/Gasparrini - Distributed lag linear and non-linear models for t.pdf:application/pdf}
}

@article{betancourt_diagnosing_2016,
	title = {Diagnosing {Suboptimal} {Cotangent} {Disintegrations} in {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1604.00695},
	abstract = {When properly tuned, Hamiltonian Monte Carlo scales to some of the most challenging high-dimensional problems at the frontiers of applied statistics, but when that tuning is suboptimal the performance leaves much to be desired. In this paper I show how suboptimal choices of one critical degree of freedom, the cotangent disintegration, manifest in readily observed diagnostics that facilitate the robust application of the algorithm.},
	urldate = {2020-01-28},
	journal = {arXiv:1604.00695 [stat]},
	author = {Betancourt, Michael},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00695},
	keywords = {Statistics - Methodology},
	annote = {Comment: 17 pages, 9 figures},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/QC7GG6QV/Betancourt - 2016 - Diagnosing Suboptimal Cotangent Disintegrations in.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/IHVMNDXE/1604.html:text/html}
}

@misc{noauthor_brief_nodate,
	title = {Brief {Guide} to {Stan}’s {Warnings}},
	url = {https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded},
	urldate = {2020-01-28},
	file = {Brief Guide to Stan’s Warnings:/mnt/data/Google Drive/Zotero/storage/YDGRAKPL/warnings.html:text/html}
}

@article{linden_using_2011,
	title = {Using the negative binomial distribution to model overdispersion in ecological count data},
	volume = {92},
	issn = {0012-9658},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1890/10-1831.1},
	doi = {10.1890/10-1831.1},
	abstract = {A Poisson process is a commonly used starting point for modeling stochastic variation of ecological count data around a theoretical expectation. However, data typically show more variation than implied by the Poisson distribution. Such overdispersion is often accounted for by using models with different assumptions about how the variance changes with the expectation. The choice of these assumptions can naturally have apparent consequences for statistical inference. We propose a parameterization of the negative binomial distribution, where two overdispersion parameters are introduced to allow for various quadratic mean?variance relationships, including the ones assumed in the most commonly used approaches. Using bird migration as an example, we present hypothetical scenarios on how overdispersion can arise due to sampling, flocking behavior or aggregation, environmental variability, or combinations of these factors. For all considered scenarios, mean?variance relationships can be appropriately described by the negative binomial distribution with two overdispersion parameters. To illustrate, we apply the model to empirical migration data with a high level of overdispersion, gaining clearly different model fits with different assumptions about mean?variance relationships. The proposed framework can be a useful approximation for modeling marginal distributions of independent count data in likelihood-based analyses.},
	number = {7},
	urldate = {2020-01-29},
	journal = {Ecology},
	author = {Lindén, Andreas and Mäntyniemi, Samu},
	month = jul,
	year = {2011},
	keywords = {bird migration, count data, environmental stochasticity, flocking, generalized linear models, mean–variance relationship, negative binomial distribution, overdispersion, Poisson process, sampling error},
	pages = {1414--1421},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/8ITCFURB/10-1831.html:text/html}
}

@article{marz_xgboostlss_2019,
	title = {{XGBoostLSS} -- {An} extension of {XGBoost} to probabilistic forecasting},
	url = {http://arxiv.org/abs/1907.03178},
	abstract = {We propose a new framework of XGBoost that predicts the entire conditional distribution of a univariate response variable. In particular, XGBoostLSS models all moments of a parametric distribution (i.e., mean, location, scale and shape [LSS]) instead of the conditional mean only. Choosing from a wide range of continuous, discrete and mixed discrete-continuous distribution, modelling and predicting the entire conditional distribution greatly enhances the ﬂexibility of XGBoost, as it allows to gain additional insight into the data generating process, as well as to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. We present both a simulation study and real world examples that demonstrate the virtues of our approach.},
	language = {en},
	urldate = {2020-02-06},
	journal = {arXiv:1907.03178 [cs, stat]},
	author = {März, Alexander},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.03178},
	keywords = {Statistics - Methodology, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Bayesian Optimization; Distributional Modeling; Expectile Regression; GAMLSS; Probabilistic Forecast; Uncertainty Quantification; XGBoost},
	file = {März - 2019 - XGBoostLSS -- An extension of XGBoost to probabili.pdf:/mnt/data/Google Drive/Zotero/storage/NYKHUUHG/März - 2019 - XGBoostLSS -- An extension of XGBoost to probabili.pdf:application/pdf}
}

@article{czado_predictive_2009,
	title = {Predictive model assessment for count data},
	volume = {65},
	issn = {1541-0420},
	doi = {10.1111/j.1541-0420.2009.01191.x},
	abstract = {We discuss tools for the evaluation of probabilistic forecasts and the critique of statistical models for count data. Our proposals include a nonrandomized version of the probability integral transform, marginal calibration diagrams, and proper scoring rules, such as the predictive deviance. In case studies, we critique count regression models for patent data, and assess the predictive performance of Bayesian age-period-cohort models for larynx cancer counts in Germany. The toolbox applies in Bayesian or classical and parametric or nonparametric settings and to any type of ordered discrete outcomes.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Czado, Claudia and Gneiting, Tilmann and Held, Leonhard},
	month = dec,
	year = {2009},
	pmid = {19432783},
	keywords = {Biometry, Humans, Regression Analysis, Bayes Theorem, Models, Statistical, Statistics, Nonparametric, Cohort Studies, Germany, Laryngeal Neoplasms},
	pages = {1254--1261}
}

@article{jordan_evaluating_2019,
	title = {Evaluating {Probabilistic} {Forecasts} with \textbf{{scoringRules}}},
	volume = {90},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v90/i12/},
	doi = {10.18637/jss.v090.i12},
	abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several ﬁelds including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
	language = {en},
	number = {12},
	urldate = {2020-02-13},
	journal = {Journal of Statistical Software},
	author = {Jordan, Alexander and Krüger, Fabian and Lerch, Sebastian},
	year = {2019},
	file = {Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:/mnt/data/Google Drive/Zotero/storage/DSYW6QUF/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:application/pdf}
}

@article{gneiting_probabilistic_2007,
	title = {Probabilistic forecasts, calibration and sharpness},
	volume = {69},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
	doi = {10.1111/j.1467-9868.2007.00587.x},
	abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
	language = {en},
	number = {2},
	urldate = {2020-02-17},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
	year = {2007},
	keywords = {Cross-validation, Density forecast, Ensemble prediction system, Ex post evaluation, Forecast verification, Model diagnostics, Posterior predictive assessment, Predictive distribution, Prequential principle, Probability integral transform, Proper scoring rule},
	pages = {243--268},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/EUCMSBKN/j.1467-9868.2007.00587.html:text/html}
}

@article{assel_brier_2017,
	title = {The {Brier} score does not evaluate the clinical utility of diagnostic tests or prediction models},
	volume = {1},
	issn = {2397-7523},
	url = {https://doi.org/10.1186/s41512-017-0020-3},
	doi = {10.1186/s41512-017-0020-3},
	abstract = {A variety of statistics have been proposed as tools to help investigators assess the value of diagnostic tests or prediction models. The Brier score has been recommended on the grounds that it is a proper scoring rule that is affected by both discrimination and calibration. However, the Brier score is prevalence dependent in such a way that the rank ordering of tests or models may inappropriately vary by prevalence.},
	number = {1},
	urldate = {2020-02-25},
	journal = {Diagnostic and Prognostic Research},
	author = {Assel, Melissa and Sjoberg, Daniel D. and Vickers, Andrew J.},
	month = dec,
	year = {2017},
	pages = {19},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/M3IGKA6P/Assel et al. - 2017 - The Brier score does not evaluate the clinical uti.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/LL74JDGX/s41512-017-0020-3.html:text/html}
}

@article{jordan_evaluating_2019-1,
	title = {Evaluating {Probabilistic} {Forecasts} with \textbf{{scoringRules}}},
	volume = {90},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v90/i12/},
	doi = {10.18637/jss.v090.i12},
	abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several ﬁelds including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
	language = {en},
	number = {12},
	urldate = {2020-03-07},
	journal = {Journal of Statistical Software},
	author = {Jordan, Alexander and Krüger, Fabian and Lerch, Sebastian},
	year = {2019},
	file = {Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:/mnt/data/Google Drive/Zotero/storage/K5UPEP6Y/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:application/pdf}
}

@article{yao_using_2018,
	title = {Using stacking to average {Bayesian} predictive distributions},
	volume = {13},
	issn = {1936-0975},
	url = {http://arxiv.org/abs/1704.02030},
	doi = {10.1214/17-BA1091},
	abstract = {The widely recommended procedure of Bayesian model averaging is ﬂawed in the M-open setting in which the true data-generating process is not one of the candidate models being ﬁt. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions, extending the utility function to any proper scoring rule, using Pareto smoothed importance sampling to eﬃciently compute the required leave-one-out posterior distributions and regularization to get more stability. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with BB-pseudo-BMA as an approximate alternative when computation cost is an issue.},
	language = {en},
	number = {3},
	urldate = {2020-03-10},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	month = sep,
	year = {2018},
	note = {arXiv: 1704.02030},
	keywords = {Statistics - Methodology, Statistics - Computation},
	pages = {917--1007},
	file = {Yao et al. - 2018 - Using stacking to average Bayesian predictive dist.pdf:/mnt/data/Google Drive/Zotero/storage/4J6RFHS6/Yao et al. - 2018 - Using stacking to average Bayesian predictive dist.pdf:application/pdf}
}

@article{machete_early_2013,
	title = {Early {Warning} with {Calibrated} and {Sharper} {Probabilistic} {Forecasts}},
	volume = {32},
	issn = {1099-131X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2242},
	doi = {10.1002/for.2242},
	abstract = {ABSTRACTGiven a nonlinear model, a probabilistic forecast may be obtained by Monte Carlo simulations. At a given forecast horizon, Monte Carlo simulations yield sets of discrete forecasts, which can be converted to density forecasts. The resulting density forecasts will inevitably be downgraded by model misspecification. In order to enhance the quality of the density forecasts, one can mix them with the unconditional density. This paper examines the value of combining conditional density forecasts with the unconditional density. The findings have positive implications for issuing early warnings in different disciplines including economics and meteorology, but UK inflation forecasts are considered as an example. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2020-03-20},
	journal = {Journal of Forecasting},
	author = {Machete, Reason L.},
	year = {2013},
	keywords = {calibration, combining forecasts, density forecasts, scoring rule},
	pages = {452--468},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/FHU93V4Z/Machete - 2013 - Early Warning with Calibrated and Sharper Probabil.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/S554FQ63/for.html:text/html}
}

@article{machete_contrasting_2012,
	title = {Contrasting {Probabilistic} {Scoring} {Rules}},
	url = {http://arxiv.org/abs/1112.4530},
	abstract = {There are several scoring rules that one can choose from in order to score probabilistic forecasting models or estimate model parameters. Whilst it is generally agreed that proper scoring rules are preferable, there is no clear criterion for preferring one proper scoring rule above another. This manuscript compares and contrasts some commonly used proper scoring rules and provides guidance on scoring rule selection. In particular, it is shown that the logarithmic scoring rule prefers erring with more uncertainty, the spherical scoring rule prefers erring with lower uncertainty, whereas the other scoring rules are indiﬀerent to either option.},
	language = {en},
	urldate = {2020-03-21},
	journal = {arXiv:1112.4530 [math, stat]},
	author = {Machete, Reason Lesego},
	month = jul,
	year = {2012},
	note = {arXiv: 1112.4530},
	keywords = {Mathematics - Statistics Theory, 62B10, 62C05, 62G05, 62G07, 62F99, 62P05, 62P12, 62P20},
	annote = {Comment: 17 pages, 0 figures},
	file = {Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf:/mnt/data/Google Drive/Zotero/storage/8FYPC3Y4/Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf:application/pdf}
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2020-03-22},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	pages = {359--378},
	file = {Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:/mnt/data/Google Drive/Zotero/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf}
}

@article{nouvellet_simple_2018,
	series = {The {RAPIDD} {Ebola} {Forecasting} {Challenge}},
	title = {A simple approach to measure transmissibility and forecast incidence},
	volume = {22},
	issn = {1755-4365},
	url = {http://www.sciencedirect.com/science/article/pii/S1755436517300245},
	doi = {10.1016/j.epidem.2017.02.012},
	abstract = {Outbreaks of novel pathogens such as SARS, pandemic influenza and Ebola require substantial investments in reactive interventions, with consequent implementation plans sometimes revised on a weekly basis. Therefore, short-term forecasts of incidence are often of high priority. In light of the recent Ebola epidemic in West Africa, a forecasting exercise was convened by a network of infectious disease modellers. The challenge was to forecast unseen “future” simulated data for four different scenarios at five different time points. In a similar method to that used during the recent Ebola epidemic, we estimated current levels of transmissibility, over variable time-windows chosen in an ad hoc way. Current estimated transmissibility was then used to forecast near-future incidence. We performed well within the challenge and often produced accurate forecasts. A retrospective analysis showed that our subjective method for deciding on the window of time with which to estimate transmissibility often resulted in the optimal choice. However, when near-future trends deviated substantially from exponential patterns, the accuracy of our forecasts was reduced. This exercise highlights the urgent need for infectious disease modellers to develop more robust descriptions of processes – other than the widespread depletion of susceptible individuals – that produce non-exponential patterns of incidence.},
	language = {en},
	urldate = {2020-03-29},
	journal = {Epidemics},
	author = {Nouvellet, Pierre and Cori, Anne and Garske, Tini and Blake, Isobel M. and Dorigatti, Ilaria and Hinsley, Wes and Jombart, Thibaut and Mills, Harriet L. and Nedjati-Gilani, Gemma and Van Kerkhove, Maria D. and Fraser, Christophe and Donnelly, Christl A. and Ferguson, Neil M. and Riley, Steven},
	month = mar,
	year = {2018},
	keywords = {Forecasting, Branching process, MCMC, Rapid response, Renewal equation},
	pages = {29--35},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6NPQQ8AP/Nouvellet et al. - 2018 - A simple approach to measure transmissibility and .pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/AY6K9FY5/S1755436517300245.html:text/html}
}

@article{brooks_nonmechanistic_2018,
	title = {Nonmechanistic forecasts of seasonal influenza with iterative one-week-ahead distributions},
	volume = {14},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006134},
	doi = {10.1371/journal.pcbi.1006134},
	abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on “delta densities”, and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC’s 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
	language = {en},
	number = {6},
	urldate = {2020-03-29},
	journal = {PLOS Computational Biology},
	author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
	editor = {Viboud, Cecile},
	month = jun,
	year = {2018},
	pages = {e1006134},
	file = {Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:/mnt/data/Google Drive/Zotero/storage/KWJ8KSUS/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:application/pdf}
}

@article{sadhanala_additive_2019,
	title = {Additive models with trend filtering},
	volume = {47},
	issn = {0090-5364},
	url = {https://projecteuclid.org/euclid.aos/1572487382},
	doi = {10.1214/19-AOS1833},
	abstract = {We study additive models built with trend ﬁltering, i.e., additive models whose components are each regularized by the (discrete) total variation of their kth (discrete) derivative, for a chosen integer k ≥ 0. This results in kth degree piecewise polynomial components, (e.g., k = 0 gives piecewise constant components, k = 1 gives piecewise linear, k = 2 gives piecewise quadratic, etc.). Analogous to its advantages in the univariate case, additive trend ﬁltering has favorable theoretical and computational properties, thanks in large part to the localized nature of the (discrete) total variation regularizer that it uses. On the theory side, we derive fast error rates for additive trend ﬁltering estimates, and show these rates are minimax optimal when the underlying function is additive and has component functions whose derivatives are of bounded variation. We also show that these rates are unattainable by additive smoothing splines (and by additive models built from linear smoothers, in general). On the computational side, we use backﬁtting, to leverage fast univariate trend ﬁltering solvers; we also describe a new backﬁtting algorithm whose iterations can be run in parallel, which (as far as we can tell) is the ﬁrst of its kind. Lastly, we present a number of experiments to examine the empirical performance of trend ﬁltering.},
	language = {en},
	number = {6},
	urldate = {2020-03-29},
	journal = {The Annals of Statistics},
	author = {Sadhanala, Veeranjaneyulu and Tibshirani, Ryan J.},
	month = dec,
	year = {2019},
	pages = {3032--3068},
	file = {Sadhanala and Tibshirani - 2019 - Additive models with trend filtering.pdf:/mnt/data/Google Drive/Zotero/storage/2Y3KRKHQ/Sadhanala and Tibshirani - 2019 - Additive models with trend filtering.pdf:application/pdf}
}

@article{brooks_nonmechanistic_2018-1,
	title = {Nonmechanistic forecasts of seasonal influenza with iterative one-week-ahead distributions},
	volume = {14},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006134},
	doi = {10.1371/journal.pcbi.1006134},
	abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on “delta densities”, and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC’s 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
	language = {en},
	number = {6},
	urldate = {2020-03-29},
	journal = {PLOS Computational Biology},
	author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
	editor = {Viboud, Cecile},
	month = jun,
	year = {2018},
	pages = {e1006134},
	file = {Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:/mnt/data/Google Drive/Zotero/storage/88JUM5DJ/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:application/pdf}
}

@article{hyndman_automatic_2008,
	title = {Automatic {Time} {Series} {Forecasting}: {The} \textbf{forecast} {Package} for \textit{{R}}},
	volume = {27},
	issn = {1548-7660},
	shorttitle = {Automatic {Time} {Series} {Forecasting}},
	url = {http://www.jstatsoft.org/v27/i03/},
	doi = {10.18637/jss.v027.i03},
	language = {en},
	number = {3},
	urldate = {2020-03-30},
	journal = {Journal of Statistical Software},
	author = {Hyndman, Rob J. and Khandakar, Yeasmin},
	year = {2008},
	file = {Hyndman and Khandakar - 2008 - Automatic Time Series Forecasting The bforecast.pdf:/mnt/data/Google Drive/Zotero/storage/J8VLZS83/Hyndman and Khandakar - 2008 - Automatic Time Series Forecasting The bforecast.pdf:application/pdf}
}

@book{noauthor_102_nodate,
	title = {10.2 {Grouped} time series {\textbar} {Forecasting}: {Principles} and {Practice}},
	shorttitle = {10.2 {Grouped} time series {\textbar} {Forecasting}},
	url = {https://Otexts.com/fpp2/},
	abstract = {2nd edition},
	urldate = {2020-03-31},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/6QI93F5W/gts.html:text/html}
}

@misc{noauthor_estimation_nodate,
	title = {Estimation of the {Continuous} {Ranked} {Probability} {Score} with {Limited} {Information} and {Applications} to {Ensemble} {Weather} {Forecasts} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/s11004-017-9709-7},
	urldate = {2020-04-02},
	file = {Estimation of the Continuous Ranked Probability Score with Limited Information and Applications to Ensemble Weather Forecasts | SpringerLink:/mnt/data/Google Drive/Zotero/storage/4XESHFER/s11004-017-9709-7.html:text/html}
}

@misc{noauthor_optimising_nodate,
	title = {Optimising {Renewal} {Models} for {Real}-{Time} {Epidemic} {Prediction} and {Estimation} {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/835181v2.full},
	urldate = {2020-04-02},
	file = {Optimising Renewal Models for Real-Time Epidemic Prediction and Estimation | bioRxiv:/mnt/data/Google Drive/Zotero/storage/8ZTZEYVU/835181v2.html:text/html}
}

@article{machete_early_2013-1,
	title = {Early {Warning} with {Calibrated} and {Sharper} {Probabilistic} {Forecasts}},
	volume = {32},
	issn = {1099-131X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2242},
	doi = {10.1002/for.2242},
	abstract = {ABSTRACTGiven a nonlinear model, a probabilistic forecast may be obtained by Monte Carlo simulations. At a given forecast horizon, Monte Carlo simulations yield sets of discrete forecasts, which can be converted to density forecasts. The resulting density forecasts will inevitably be downgraded by model misspecification. In order to enhance the quality of the density forecasts, one can mix them with the unconditional density. This paper examines the value of combining conditional density forecasts with the unconditional density. The findings have positive implications for issuing early warnings in different disciplines including economics and meteorology, but UK inflation forecasts are considered as an example. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2020-04-03},
	journal = {Journal of Forecasting},
	author = {Machete, Reason L.},
	year = {2013},
	keywords = {calibration, combining forecasts, density forecasts, scoring rule},
	pages = {452--468},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/PKZT8IGY/Machete - 2013 - Early Warning with Calibrated and Sharper Probabil.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/7LJPT7DX/for.html:text/html}
}

@article{aminikhanghahi_survey_2017,
	title = {A {Survey} of {Methods} for {Time} {Series} {Change} {Point} {Detection}},
	volume = {51},
	issn = {0219-1377},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5464762/},
	doi = {10.1007/s10115-016-0987-z},
	abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
	number = {2},
	urldate = {2020-04-03},
	journal = {Knowledge and information systems},
	author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
	month = may,
	year = {2017},
	pmid = {28603327},
	pmcid = {PMC5464762},
	pages = {339--367},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/LWWTKKN3/Aminikhanghahi and Cook - 2017 - A Survey of Methods for Time Series Change Point D.pdf:application/pdf}
}

@article{killick_optimal_2012,
	title = {Optimal detection of changepoints with a linear computational cost},
	volume = {107},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/1101.1438},
	doi = {10.1080/01621459.2012.737745},
	abstract = {We consider the problem of detecting multiple changepoints in large data sets. Our focus is on applications where the number of changepoints will increase as we collect more data: for example in genetics as we analyse larger regions of the genome, or in ﬁnance as we observe time-series over longer periods. We consider the common approach of detecting changepoints through minimising a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalised likelihood and minimum description length. We introduce a new ∗R. Killick is Senior Research Associate, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: r.killick@lancs.ac.uk). P. Fearnhead is Professor, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: p.fearnhead@lancs.ac.uk). I.A. Eckley is Senior Lecturer, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: i.eckley@lancs.ac.uk). The authors are grateful to Richard Davis and Alice Cleynen for providing the Auto-PARM and PDPA software respectively. Part of this research was conducted whilst R. Killick was a jointly funded Engineering and Physical Sciences Research Council (EPSRC) / Shell Research Ltd graduate student at Lancaster University. Both I.A. Eckley and R. Killick also gratefully acknowledge the ﬁnancial support of the EPSRC grant number EP/I016368/1.},
	language = {en},
	number = {500},
	urldate = {2020-04-03},
	journal = {Journal of the American Statistical Association},
	author = {Killick, R. and Fearnhead, P. and Eckley, I. A.},
	month = dec,
	year = {2012},
	note = {arXiv: 1101.1438},
	keywords = {Statistics - Methodology, Quantitative Biology - Quantitative Methods, Quantitative Biology - Genomics},
	pages = {1590--1598},
	annote = {Comment: 25 pages, 4 figures, To appear in Journal of the American Statistical Association},
	file = {Killick et al. - 2012 - Optimal detection of changepoints with a linear co.pdf:/mnt/data/Google Drive/Zotero/storage/NUKTI622/Killick et al. - 2012 - Optimal detection of changepoints with a linear co.pdf:application/pdf}
}

@techreport{parag_optimising_2019,
	type = {preprint},
	title = {Optimising {Renewal} {Models} for {Real}-{Time} {Epidemic} {Prediction} and {Estimation}},
	url = {http://biorxiv.org/lookup/doi/10.1101/835181},
	abstract = {Abstract
          
            The effective reproduction number,
            R
            
              t
            
            , is an important prognostic for infectious disease epidemics. Significant changes in
            R
            
              t
            
            can forewarn about new transmissions or predict the efficacy of interventions. The renewal model infers
            R
            
              t
            
            from incidence data and has been applied to Ebola virus disease and pandemic influenza outbreaks, among others. This model estimates
            R
            
              t
            
            using a sliding window of length
            k
            . While this facilitates real-time detection of statistically significant
            R
            
              t
            
            fluctuations, inference is highly
            k
            -sensitive. Models with too large or small
            k
            might ignore meaningful changes or over-interpret noise-induced ones. No principled
            k
            -selection scheme exists. We develop a practical yet rigorous scheme using the accumulated prediction error (APE) metric from information theory. We derive exact incidence prediction distributions and integrate these within an APE framework to identify the
            k
            best supported by available data. We find that this
            k
            optimises short-term prediction accuracy and expose how common, heuristic
            k
            -choices, which seem sensible, could be misleading.},
	language = {en},
	urldate = {2020-04-03},
	institution = {Bioinformatics},
	author = {Parag, Kv and Donnelly, Ca},
	month = nov,
	year = {2019},
	doi = {10.1101/835181},
	file = {Parag and Donnelly - 2019 - Optimising Renewal Models for Real-Time Epidemic P.pdf:/mnt/data/Google Drive/Zotero/storage/3VCUX3E7/Parag and Donnelly - 2019 - Optimising Renewal Models for Real-Time Epidemic P.pdf:application/pdf}
}

@article{rissanen_order_1986,
	title = {Order {Estimation} by {Accumulated} {Prediction} {Errors}},
	volume = {23},
	issn = {0021-9002},
	url = {https://www.jstor.org/stable/3214342},
	doi = {10.2307/3214342},
	abstract = {This paper presents a new criterion based on prediction error which allows the estimation of the number of parameters as well as structures in statistical models. The criterion is valid for short and long samples alike. Unlike Akaike's earlier criterion, also based on prediction error, the criterion proposed here appears to produce consistent error estimates in ARMA processes.},
	urldate = {2020-04-03},
	journal = {Journal of Applied Probability},
	author = {Rissanen, Jorma},
	year = {1986},
	pages = {55--61}
}

@article{noauthor_order_nodate,
	title = {Order {Estimation} by {Accumulated} {Prediction} {Errors}},
	abstract = {This paper presents a new criterion based on prediction error which allows the estimation of the number of parameters as well as structures in statistical models. The criterion is valid for short and long samples alike. Unlike Akaike's earlier criterion, also based on prediction error, the criterion proposed here appears to produce consistent error estimates in ARMA processes.},
	language = {en},
	pages = {8},
	file = {Order Estimation by Accumulated Prediction Errors.pdf:/mnt/data/Google Drive/Zotero/storage/JTIXZW74/Order Estimation by Accumulated Prediction Errors.pdf:application/pdf}
}

@article{holmes_analysis_nodate,
	title = {Analysis of multivariate time- series using the {MARSS} package},
	language = {en},
	author = {Holmes, E E and Ward, E J and Scheuerell, M D},
	pages = {284},
	file = {Holmes et al. - Analysis of multivariate time- series using the MA.pdf:/mnt/data/Google Drive/Zotero/storage/DLEKJXGW/Holmes et al. - Analysis of multivariate time- series using the MA.pdf:application/pdf}
}

@article{montero-manso_fforma_2020,
	title = {{FFORMA}: {Feature}-based forecast model averaging},
	volume = {36},
	issn = {01692070},
	shorttitle = {{FFORMA}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019300895},
	doi = {10.1016/j.ijforecast.2019.02.011},
	abstract = {We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First, we use a collection of time series to train a meta-model to assign weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features extracted from each series. In the second phase, we forecast new series using a weighted forecast combination where the weights are obtained from our previously trained meta-model. Our method outperforms a simple forecast combination, and outperforms all of the most popular individual methods in the time series forecasting literature. The approach achieved second position in the M4 competition.},
	language = {en},
	number = {1},
	urldate = {2020-04-06},
	journal = {International Journal of Forecasting},
	author = {Montero-Manso, Pablo and Athanasopoulos, George and Hyndman, Rob J. and Talagala, Thiyanga S.},
	month = jan,
	year = {2020},
	pages = {86--92},
	file = {Montero-Manso et al. - 2020 - FFORMA Feature-based forecast model averaging.pdf:/mnt/data/Google Drive/Zotero/storage/MNTZFESR/Montero-Manso et al. - 2020 - FFORMA Feature-based forecast model averaging.pdf:application/pdf}
}

@article{wilks_enforcing_2018,
	title = {Enforcing calibration in ensemble postprocessing: {Enforcing} {Calibration} in {Ensemble} {Postprocessing}},
	volume = {144},
	issn = {00359009},
	shorttitle = {Enforcing calibration in ensemble postprocessing},
	url = {http://doi.wiley.com/10.1002/qj.3185},
	doi = {10.1002/qj.3185},
	language = {en},
	number = {710},
	urldate = {2020-04-06},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Wilks, Daniel S.},
	month = jan,
	year = {2018},
	pages = {76--84},
	file = {Wilks - 2018 - Enforcing calibration in ensemble postprocessing .pdf:/mnt/data/Google Drive/Zotero/storage/KDGXTZRJ/Wilks - 2018 - Enforcing calibration in ensemble postprocessing .pdf:application/pdf}
}

@techreport{gibson_improving_2019,
	type = {preprint},
	title = {Improving {Probabilistic} {Infectious} {Disease} {Forecasting} {Through} {Coherence}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2019.12.27.889212},
	abstract = {With an estimated \$10.4 billion in medical costs and 31.4 million outpatient visits each year, inﬂuenza poses a serious burden of disease in the United States. To provide insights and advance warning into the spread of inﬂuenza, the U.S. Centers for Disease Control and Prevention (CDC) runs a challenge for forecasting weighted inﬂuenza-like illness (wILI) at the national and regional level. Many models produce independent forecasts for each geographical unit, ignoring the constraint that the national wILI is a weighted sum of regional wILI, where the weights correspond to the population size of the region. We propose a novel algorithm that transforms a set of independent forecast distributions to obey this constraint, which we refer to as probabilistically coherent. Enforcing probabilistic coherence led to an increase in forecast skill for 90\% of the models we tested over multiple ﬂu seasons, highlighting the importance of respecting the forecasting system’s geographical hierarchy.},
	language = {en},
	urldate = {2020-04-06},
	institution = {Bioinformatics},
	author = {Gibson, Graham Casey and Moran, Kelly R. and Reich, Nicholas G. and Osthus, Dave},
	month = dec,
	year = {2019},
	doi = {10.1101/2019.12.27.889212},
	file = {Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:/mnt/data/Google Drive/Zotero/storage/J22WMETR/Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:application/pdf}
}

@article{schaeybroeck_ensemble_2015,
	title = {Ensemble post-processing using member-by-member approaches: theoretical aspects},
	volume = {141},
	copyright = {© 2014 Royal Meteorological Society},
	issn = {1477-870X},
	shorttitle = {Ensemble post-processing using member-by-member approaches},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.2397},
	doi = {10.1002/qj.2397},
	abstract = {Linear post-processing approaches are proposed and fundamental mechanisms are analyzed by which the probabilistic skill of an ensemble forecast can be improved. The ensemble mean of the corrected forecast is a linear function of the ensemble mean(s) of the predictor(s). Likewise, the ensemble spread of the corrected forecast depends linearly on that of the uncorrected forecast. The regression coefficients are obtained by maximizing the likelihood function for the error distribution. Comparing different calibration approaches on simple systems that exhibit chaotic features (the Kuramoto–Sivashinsky equation, the spatially extended Lorenz system), four correction mechanisms are identified: the ensemble-mean scaling and nudging using the predictor(s), and the ensemble-spread scaling and nudging. Ensemble-spread corrections turn out to yield improvement only when ‘reliability’ constraints are imposed on the corrected forecast. First of all climatological reliability is enforced and is satisfied when the total variability of the forecast is equal to the variability of the observations. Second, ensemble reliability or calibration of the ensembles is enforced such that the squared error of the ensemble mean coincides with the ensemble variance. In terms of continuous ranked probability skill score, spread calibration provides much more gain in skill than the traditional ensemble-mean calibration and extends for lead times far beyond the error-doubling time. The skill performance is better than or as good as the benchmark calibration method which derives from statistical assumptions –non-homogeneous Gaussian regression. In addition to the member-by-member nature of the approach, benefits compared with the benchmark method can be pinpointed. In particular, although the post-processing methods are performed for each lead time, location and variable independently, they preserve the rank correlations and thus take dependencies across space, time, and different variables into account. In addition, higher-order ensemble moments like kurtosis and skewness correspond to those of the uncorrected forecasts.},
	language = {en},
	number = {688},
	urldate = {2020-04-06},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Schaeybroeck, Bert Van and Vannitsem, Stéphane},
	year = {2015},
	keywords = {member-by-member approach, model output statistics, statistical post-processing},
	pages = {807--818},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6YNK3RMJ/Schaeybroeck and Vannitsem - 2015 - Ensemble post-processing using member-by-member ap.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/C5IM6W5S/qj.html:text/html}
}

@article{fraley_ensemblebma_nodate,
	title = {{ensembleBMA}: {An} {R} {Package} for {Probabilistic} {Forecasting} using {Ensembles} and {Bayesian} {Model} {Averaging}},
	language = {en},
	author = {Fraley, Chris and Raftery, Adrian E and Gneiting, Tilmann and Sloughter, J McLean},
	pages = {19},
	file = {Fraley et al. - ensembleBMA An R Package for Probabilistic Foreca.pdf:/mnt/data/Google Drive/Zotero/storage/396E78V2/Fraley et al. - ensembleBMA An R Package for Probabilistic Foreca.pdf:application/pdf}
}

@article{monache_probabilistic_2006,
	title = {Probabilistic aspects of meteorological and ozone regional ensemble forecasts},
	volume = {111},
	copyright = {Copyright 2006 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005JD006917},
	doi = {10.1029/2005JD006917},
	abstract = {This study investigates whether probabilistic ozone forecasts from an ensemble can be made with skill: i.e., high verification resolution and reliability. Twenty-eight ozone forecasts were generated over the Lower Fraser Valley, British Columbia, Canada, for the 5-day period 11–15 August 2004 and compared with 1-hour averaged measurements of ozone concentrations at five stations. The forecasts were obtained by driving the Community Multiscale Air Quality Model (CMAQ) model with four meteorological forecasts and seven emission scenarios: a control run, ±50\% NOx, ±50\% volatile organic compounds (VOC), and ±50\% NOx combined with VOC. Probabilistic forecast quality is verified using relative operating characteristic curves, Talagrand diagrams, and a new reliability index. Results show that both meteorology and emission perturbations are needed to have a skillful probabilistic forecast system: the meteorology perturbation is important to capture the ozone temporal and spatial distribution and the emission perturbation is needed to span the range of ozone concentration magnitudes. Emission perturbations are more important than meteorology perturbations for capturing the likelihood of high ozone concentrations. Perturbations involving NOx resulted in a more skillful probabilistic forecast for the episode analyzed, and therefore the 50\% perturbation values appear to span much of the emission uncertainty for this case. All of the ensembles analyzed show a high ozone concentration bias in the Talagrand diagrams, even when the biases from the unperturbed emissions forecasts are removed from all ensemble members. This result indicates nonlinearity in the ensemble, which arises from both ozone chemistry and its interaction with input from particular meteorological models.},
	language = {en},
	number = {D24},
	urldate = {2020-04-07},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Monache, Luca Delle and Hacker, Joshua P. and Zhou, Yongmei and Deng, Xingxiu and Stull, Roland B.},
	year = {2006},
	keywords = {ensemble, ozone, probabilistic forecasts},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/VJAUP5ZB/Monache et al. - 2006 - Probabilistic aspects of meteorological and ozone .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/GT337KX3/2005JD006917.html:text/html}
}

@article{li_review_2017,
	title = {A review on statistical postprocessing methods for hydrometeorological ensemble forecasting},
	volume = {4},
	issn = {2049-1948},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1246},
	doi = {10.1002/wat2.1246},
	abstract = {Computer simulation models have been widely used to generate hydrometeorological forecasts. As the raw forecasts contain uncertainties arising from various sources, including model inputs and outputs, model initial and boundary conditions, model structure, and model parameters, it is necessary to apply statistical postprocessing methods to quantify and reduce those uncertainties. Different postprocessing methods have been developed for meteorological forecasts (e.g., precipitation) and for hydrological forecasts (e.g., streamflow) due to their different statistical properties. In this paper, we conduct a comprehensive review of the commonly used statistical postprocessing methods for both meteorological and hydrological forecasts. Moreover, methods to generate ensemble members that maintain the observed spatiotemporal and intervariable dependency are reviewed. Finally, some perspectives on the further development of statistical postprocessing methods for hydrometeorological ensemble forecasting are provided. WIREs Water 2017, 4:e1246. doi: 10.1002/wat2.1246 This article is categorized under: Science of Water {\textgreater} Methods Science of Water {\textgreater} Water Extremes},
	language = {en},
	number = {6},
	urldate = {2020-04-07},
	journal = {WIREs Water},
	author = {Li, Wentao and Duan, Qingyun and Miao, Chiyuan and Ye, Aizhong and Gong, Wei and Di, Zhenhua},
	year = {2017},
	pages = {e1246},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6WQKZ5C5/Li et al. - 2017 - A review on statistical postprocessing methods for.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/DE2Z8TLS/wat2.html:text/html}
}

@misc{noauthor_why_2016,
	title = {Why optim() is out of date},
	url = {https://www.r-bloggers.com/why-optim-is-out-of-date/},
	abstract = {Why optim() is out of date And perhaps you should be careful using it Once upon a time Once upon a time, there was a young Oxford D.Phil. graduate with a thesis on quantum mechanics who — by virtue of a mixup in identities — got hired as an Agricultural Economist. He was actually better…},
	language = {en-US},
	urldate = {2020-04-07},
	journal = {R-bloggers},
	month = nov,
	year = {2016},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/ANNBD4ZN/why-optim-is-out-of-date.html:text/html}
}

@article{nousu_statistical_2019,
	title = {Statistical post-processing of ensemble forecasts of the height of new snow},
	volume = {26},
	issn = {1023-5809},
	url = {https://www.nonlin-processes-geophys.net/26/339/2019/},
	doi = {https://doi.org/10.5194/npg-26-339-2019},
	abstract = {{\textless}p{\textgreater}{\textless}strong{\textgreater}Abstract.{\textless}/strong{\textgreater} Forecasting the height of new snow (HN) is crucial for avalanche hazard forecasting, road viability, ski resort management and tourism attractiveness. Météo-France operates the PEARP-S2M probabilistic forecasting system, including 35 members of the PEARP Numerical Weather Prediction system, where the SAFRAN downscaling tool refines the elevation resolution and the Crocus snowpack model represents the main physical processes in the snowpack. It provides better HN forecasts than direct NWP diagnostics but exhibits significant biases and underdispersion. We applied a statistical post-processing to these ensemble forecasts, based on non-homogeneous regression with a censored shifted Gamma distribution. Observations come from manual measurements of 24\&thinsp;h HN in the French Alps and Pyrenees. The calibration is tested at the station scale and the massif scale (i.e. aggregating different stations over areas of 1000\&thinsp;km{\textless}span class="inline-formula"{\textgreater}$^{\textrm{2}}${\textless}/span{\textgreater}). Compared to the raw forecasts, similar improvements are obtained for both spatial scales. Therefore, the post-processing can be applied at any point of the massifs. Two training datasets are tested: (1) a 22-year homogeneous reforecast for which the NWP model resolution and physical options are identical to the operational system but without the same initial perturbations; (2) 3-year real-time forecasts with a heterogeneous model configuration but the same perturbation methods. The impact of the training dataset depends on lead time and on the evaluation criteria. The long-term reforecast improves the reliability of severe snowfall but leads to overdispersion due to the discrepancy in real-time perturbations. Thus, the development of reliable automatic forecasting products of HN needs long reforecasts as homogeneous as possible with the operational systems.{\textless}/p{\textgreater}},
	language = {English},
	number = {3},
	urldate = {2020-04-08},
	journal = {Nonlinear Processes in Geophysics},
	author = {Nousu, Jari-Pekka and Lafaysse, Matthieu and Vernay, Matthieu and Bellier, Joseph and Evin, Guillaume and Joly, Bruno},
	month = sep,
	year = {2019},
	pages = {339--357},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/PCPFWPVD/Nousu et al. - 2019 - Statistical post-processing of ensemble forecasts .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/X26LRXTJ/2019.html:text/html}
}

@article{diaz_statistical_2020,
	title = {Statistical post-processing of ensemble forecasts of temperature in {Santiago} de {Chile}},
	volume = {27},
	copyright = {© 2019 Royal Meteorological Society},
	issn = {1469-8080},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/met.1818},
	doi = {10.1002/met.1818},
	abstract = {Modelling forecast uncertainty is a difficult task in any forecasting problem. In weather forecasting a possible solution is the use of forecast ensembles, which are obtained from multiple runs of numerical weather prediction models with various initial conditions and model parametrizations to provide information about the expected uncertainty. Currently all major meteorological centres issue forecasts using their operational ensemble prediction systems. However, it is a general problem that the spread of the ensemble is too small compared to observations at specific sites resulting in under-dispersive forecasts, leading to a lack of calibration. In order to correct this problem, various statistical calibration techniques have been developed in the last two decades. In the present work different post-processing techniques were tested for calibrating nine member ensemble forecasts of temperature for Santiago de Chile, obtained by the Weather Research and Forecasting model using different planetary boundary layer and land surface model parametrizations. In particular, the ensemble model output statistics and Bayesian model averaging techniques were implemented and, since the observations are characterized by large altitude differences, the estimation of model parameters was adapted to the actual conditions at hand. Compared to the raw ensemble, all tested post-processing approaches significantly improve the calibration of probabilistic forecasts and the accuracy of point forecasts. The ensemble model output statistics method using parameter estimation based on expert clustering of stations (according to their altitudes) shows the best forecast skill.},
	language = {en},
	number = {1},
	urldate = {2020-04-08},
	journal = {Meteorological Applications},
	author = {Díaz, Mailiu and Nicolis, Orietta and Marín, Julio César and Baran, Sándor},
	year = {2020},
	keywords = {Bayesian model averaging, ensemble model output statistics, ensemble post-processing, probabilistic forecasting, temperature forecast},
	pages = {e1818},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/U8R842AZ/Díaz et al. - 2020 - Statistical post-processing of ensemble forecasts .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/W8BARAGL/met.html:text/html}
}

@article{mcandrew_adaptively_2019,
	title = {Adaptively stacking ensembles for influenza forecasting with incomplete data},
	url = {http://arxiv.org/abs/1908.01675},
	abstract = {Seasonal influenza infects between 10 and 50 million people in the United States every year, overburdening hospitals during weeks of peak incidence. Named by the CDC as an important tool to fight the damaging effects of these epidemics, accurate forecasts of influenza and influenza-like illness (ILI) forewarn public health officials about when, and where, seasonal influenza outbreaks will hit hardest. Multi-model ensemble forecasts---weighted combinations of component models---have shown positive results in forecasting. Ensemble forecasts of influenza outbreaks have been static, training on all past ILI data at the beginning of a season, generating a set of optimal weights for each model in the ensemble, and keeping the weights constant. We propose an adaptive ensemble forecast that (i) changes model weights week-by-week throughout the influenza season, (ii) only needs the current influenza season's data to make predictions, and (iii) by introducing a prior distribution, shrinks weights toward the reference equal weighting approach and adjusts for observed ILI percentages that are subject to future revisions. We investigate the prior's ability to impact adaptive ensemble performance and, after finding an optimal prior via a cross-validation approach, compare our adaptive ensemble's performance to equal-weighted and static ensembles. Applied to forecasts of short-term ILI incidence at the regional and national level in the US, our adaptive model outperforms a naive equal-weighted ensemble, and has similar or better performance to the static ensemble, which requires multiple years of training data. Adaptive ensembles are able to quickly train and forecast during epidemics, and provide a practical tool to public health officials looking for forecasts that can conform to unique features of a specific season.},
	urldate = {2020-04-12},
	journal = {arXiv:1908.01675 [cs, stat]},
	author = {McAndrew, Thomas and Reich, Nicholas G.},
	month = jul,
	year = {2019},
	note = {arXiv: 1908.01675},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Applications},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/7SC4EU2N/McAndrew and Reich - 2019 - Adaptively stacking ensembles for influenza foreca.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/KKANAKWG/1908.html:text/html}
}

@misc{timothy_w_russell_joel_hellewell1_sam_abbott1_nick_golding_hamish_gibbs_christopher_i_jarvis_kevin_van_zandvoort_cmmid_ncov_working_group_stefan_flasche_rosalind_eggo_w_john_edmunds__adam_j_kucharski_using_2020,
	title = {Using a delay-adjusted case fatality ratio to estimate under-reporting},
	url = {https://cmmid.github.io/topics/covid19/severity/global_cfr_estimates.html},
	abstract = {Using a corrected case fatality ratio, we calculate estimates of the level of under-reporting for any country with greater than ten deaths},
	language = {en},
	urldate = {2020-04-12},
	journal = {CMMID Repository},
	author = {Timothy W Russell, Joel Hellewell1, Sam Abbott1, Nick Golding, Hamish Gibbs, Christopher I Jarvis, Kevin van Zandvoort, CMMID nCov working group, Stefan Flasche, Rosalind Eggo, W John Edmunds \& Adam J Kucharski.},
	month = mar,
	year = {2020},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/BK7WAVI3/global_cfr_estimates.html:text/html}
}

@techreport{jarvis_quantifying_2020,
	type = {preprint},
	title = {Quantifying the impact of physical distance measures on the transmission of {COVID}-19 in the {UK}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.03.31.20049023},
	abstract = {Background: 
To mitigate and slow the spread of COVID-19, many countries have adopted unprecedented physical distancing policies, including the UK. We evaluate whether these measures might be sufficient to control the epidemic by estimating their impact on the reproduction number (R0, the average number of secondary cases generated per case).

Methods: 
We asked a representative sample of UK adults about their contact patterns on the previous day. The questionnaire documents the age and location of contacts and as well as a measure of their intimacy (whether physical contact was made or not). In addition, we asked about adherence to different physical distancing measures. The first surveys were sent on Tuesday 24th March, one day after a “lockdown” was implemented across the UK. We compared measured contact patterns during the lockdown to patterns of social contact made during a non-epidemic period. By comparing these, we estimated the change in reproduction number as a consequence of the physical distancing measures imposed. We used a meta-analysis of published estimates to inform our estimates of the reproduction number before interventions were put in place.

Findings: 
We found a 73\% reduction in the average daily number of contacts observed per participant (from 10.2 to 2.9). This would be sufficient to reduce R0 from 2.6 prior to lockdown to 0.62 (95\% confidence interval [CI] 0.37 - 0.89) after the lockdown, based on all types of contact and 0.37 (95\% CI = 0.22 - 0.53) for physical contacts only.


Interpretation:

The physical distancing measures adopted by the UK public have substantially reduced contact levels and will likely lead to a substantial impact and a decline in cases in the coming weeks. However, this projected decline in incidence will not occur immediately as there are significant delays between infection, the onset of symptomatic disease and hospitalisation, as well as further delays to these events being reported. Tracking behavioural change can give a more rapid assessment of the impact of physical distancing measures than routine epidemiological surveillance.},
	language = {en},
	urldate = {2020-04-12},
	institution = {Epidemiology},
	author = {Jarvis, Christopher I and Van Zandvoort, Kevin and Gimma, Amy and Prem, Kiesha and {CMMID COVID-19 working group} and Klepac, Petra and Rubin, G James and Edmunds, W John},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.03.31.20049023},
	file = {Jarvis et al. - 2020 - Quantifying the impact of physical distance measur.pdf:/mnt/data/Google Drive/Zotero/storage/LSE79PJL/Jarvis et al. - 2020 - Quantifying the impact of physical distance measur.pdf:application/pdf}
}

@techreport{davies_effect_2020,
	type = {preprint},
	title = {The effect of non-pharmaceutical interventions on {COVID}-19 cases, deaths and demand for hospital services in the {UK}: a modelling study},
	shorttitle = {The effect of non-pharmaceutical interventions on {COVID}-19 cases, deaths and demand for hospital services in the {UK}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.04.01.20049908},
	abstract = {Background 
Non-pharmaceutical interventions have been implemented to reduce transmission of SARS-CoV-2 in the UK. Projecting the size of an unmitigated epidemic and the potential effect of different control measures has been critical to support evidence-based policymaking during the early stages of the epidemic. 

Methods We used a stochastic age-structured transmission model to explore a range of intervention scenarios, including the introduction of school closures, social distancing, shielding of elderly groups, self-isolation of symptomatic cases, and extreme "lockdown"-type restrictions. We simulated different durations of interventions and triggers for introduction, as well as combinations of interventions. For each scenario, we projected estimated new cases over time, patients requiring inpatient and critical care (intensive care unit, ICU) treatment, and deaths.

Findings 
We found that mitigation measures aimed at reducing transmission would likely have decreased the reproduction number, but not sufficiently to prevent ICU demand from exceeding NHS availability. To keep ICU bed demand below capacity in the model, more extreme restrictions were necessary. In a scenario where "lockdown"-type interventions were put in place to reduce transmission, these interventions would need to be in place for a large proportion of the coming year in order to prevent healthcare demand exceeding availability.

Interpretation
The characteristics of SARS-CoV-2 mean that extreme measures are likely required to bring the epidemic under control and to prevent very large numbers of deaths and an excess of demand on hospital beds, especially those in ICUs.},
	language = {en},
	urldate = {2020-04-12},
	institution = {Infectious Diseases (except HIV/AIDS)},
	author = {Davies, Nicholas G and Kucharski, Adam J and Eggo, Rosalind M and Gimma, Amy and {CMMID COVID-19 Working Group} and Edmunds, W. John},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.04.01.20049908},
	file = {Davies et al. - 2020 - The effect of non-pharmaceutical interventions on .pdf:/mnt/data/Google Drive/Zotero/storage/7SMLCJY4/Davies et al. - 2020 - The effect of non-pharmaceutical interventions on .pdf:application/pdf}
}

@article{hellewell_feasibility_2020,
	title = {Feasibility of controlling {COVID}-19 outbreaks by isolation of cases and contacts},
	volume = {8},
	issn = {2214-109X},
	url = {https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(20)30074-7/abstract},
	doi = {10.1016/S2214-109X(20)30074-7},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Isolation of cases and contact tracing is used to control outbreaks of infectious diseases, and has been used for coronavirus disease 2019 (COVID-19). Whether this strategy will achieve control depends on characteristics of both the pathogen and the response. Here we use a mathematical model to assess if isolation and contact tracing are able to control onwards transmission from imported cases of COVID-19.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}We developed a stochastic transmission model, parameterised to the COVID-19 outbreak. We used the model to quantify the potential effectiveness of contact tracing and isolation of cases at controlling a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-like pathogen. We considered scenarios that varied in the number of initial cases, the basic reproduction number (\textit{R}$_{\textrm{0}}$), the delay from symptom onset to isolation, the probability that contacts were traced, the proportion of transmission that occurred before symptom onset, and the proportion of subclinical infections. We assumed isolation prevented all further transmission in the model. Outbreaks were deemed controlled if transmission ended within 12 weeks or before 5000 cases in total. We measured the success of controlling outbreaks using isolation and contact tracing, and quantified the weekly maximum number of cases traced to measure feasibility of public health effort.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}Simulated outbreaks starting with five initial cases, an \textit{R}$_{\textrm{0}}$ of 1·5, and 0\% transmission before symptom onset could be controlled even with low contact tracing probability; however, the probability of controlling an outbreak decreased with the number of initial cases, when \textit{R}$_{\textrm{0}}$ was 2·5 or 3·5 and with more transmission before symptom onset. Across different initial numbers of cases, the majority of scenarios with an \textit{R}$_{\textrm{0}}$ of 1·5 were controllable with less than 50\% of contacts successfully traced. To control the majority of outbreaks, for \textit{R}$_{\textrm{0}}$ of 2·5 more than 70\% of contacts had to be traced, and for an \textit{R}$_{\textrm{0}}$ of 3·5 more than 90\% of contacts had to be traced. The delay between symptom onset and isolation had the largest role in determining whether an outbreak was controllable when \textit{R}$_{\textrm{0}}$ was 1·5. For \textit{R}$_{\textrm{0}}$ values of 2·5 or 3·5, if there were 40 initial cases, contact tracing and isolation were only potentially feasible when less than 1\% of transmission occurred before symptom onset.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}In most scenarios, highly effective contact tracing and case isolation is enough to control a new outbreak of COVID-19 within 3 months. The probability of control decreases with long delays from symptom onset to isolation, fewer cases ascertained by contact tracing, and increasing transmission before symptoms. This model can be modified to reflect updated transmission characteristics and more specific definitions of outbreak control to assess the potential success of local response efforts.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}Wellcome Trust, Global Challenges Research Fund, and Health Data Research UK.{\textless}/p{\textgreater}},
	language = {English},
	number = {4},
	urldate = {2020-04-12},
	journal = {The Lancet Global Health},
	author = {Hellewell, Joel and Abbott, Sam and Gimma, Amy and Bosse, Nikos I. and Jarvis, Christopher I. and Russell, Timothy W. and Munday, James D. and Kucharski, Adam J. and Edmunds, W. John and Sun, Fiona and Flasche, Stefan and Quilty, Billy J. and Davies, Nicholas and Liu, Yang and Clifford, Samuel and Klepac, Petra and Jit, Mark and Diamond, Charlie and Gibbs, Hamish and Zandvoort, Kevin van and Funk, Sebastian and Eggo, Rosalind M.},
	month = apr,
	year = {2020},
	pmid = {32119825},
	pages = {e488--e496},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/NNCP658M/Hellewell et al. - 2020 - Feasibility of controlling COVID-19 outbreaks by i.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/YDUTTSX3/fulltext.html:text/html}
}

@article{deasy_forecasting_2020,
	title = {Forecasting ultra-early intensive care strain from {COVID}-19 in {England}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2020.03.19.20039057v3},
	doi = {10.1101/2020.03.19.20039057},
	abstract = {{\textless}p{\textgreater}The COVID-19 pandemic has led to unprecedented strain on intensive care unit (ICU) admission in parts of the world. Strategies to create surge ICU capacity requires complex local and national service reconfiguration and reduction or cancellation of elective activity. Theses measures require time to implement and have an inevitable lag before additional capacity comes on-line. An accurate short-range forecast would be helpful in guiding such difficult, costly and ethically challenging decisions. At the time this work began, cases in England were starting to increase. Here we present an attempt at an agile short-range forecast based on published real-time COVID-19 case data from the seven National Health Service commissioning regions in England (East of England, London, Midlands, North East and Yorkshire, North West, South East and South West). We use a Monte Carlo approach to model the likely impact of current diagnoses on regional ICU capacity over a 14 day horizon. Our model is designed to be parsimonious and based on plausible epidemiological data from the literature available. On the basis of the modelling assumptions made, ICU occupancy is likely to increase dramatically in the the days following the time of modelling. If the current exponential growth continues, 5 out of 7 commissioning regions will have more critically ill COVID-19 patients than there are ICU beds within two weeks{\textbackslash}todo\{last thing to do\}. Despite variable growth in absolute patients, all commissioning regions are forecast to be heavily burdened under the assumptions used. Whilst, like any forecast model, there remain uncertainties both in terms of model specification and robust epidemiological data in this early prospective phase, it would seem that surge capacity will be required in the very near future. We hope that our model will help policy decision makers with their preparations. The uncertainties in the data highlight the urgent need for ongoing real-time surveillance to allow forecasts to be constantly updated using high quality local patient-facing data as it emerges.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-04-12},
	journal = {medRxiv},
	author = {Deasy, Jacob and Rocheteau, Emma and Kohler, Katharina and Stubbs, Daniel J. and Barbiero, Pietro and Liò, Pietro and Ercole, Ari},
	month = apr,
	year = {2020},
	pages = {2020.03.19.20039057},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/REVI8V8L/Deasy et al. - 2020 - Forecasting ultra-early intensive care strain from.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/R3EEKC86/2020.03.19.20039057v3.html:text/html}
}

@article{viboud_future_2019,
	title = {The future of influenza forecasts},
	volume = {116},
	copyright = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/8/2802},
	doi = {10.1073/pnas.1822167116},
	abstract = {Recent years have seen a growing interest in generating real-time epidemic forecasts to help control infectious diseases, prompted by a succession of global and regional outbreaks. Increased availability of epidemiological data and novel digital data streams such as search engine queries and social media (1, 2), together with the rise of machine learning and sophisticated statistical approaches, have injected new blood into the science of outbreak forecasts (3, 4). In parallel, mechanistic transmission models have benefited from computational advances and extensive data on the mobility and sociodemographic structure of human populations (5, 6). In this rapidly advancing research landscape, modeling consortiums have generated systematic model comparisons of the impact of new interventions and ensemble predictions of outbreak trajectory, for use by decision makers (7⇓⇓⇓⇓–12). Despite the rapid development of disease forecasting as a discipline, however, and the interest of public health policy makers in making better use of analytics tools to control outbreaks, forecasts are rarely operational in the same way that weather forecasts, extreme events, and climate predictions are. The influenza study by Reich et al. (13) in PNAS is a unique example of multiyear infectious disease forecasts featuring a variety of modeling approaches, with consistent model formulations and forecasting targets throughout the 7-y study period (13). This is a major improvement over previous model comparison studies that used different targets and time horizons and sometimes different epidemiological datasets.

While there is considerable interest among modelers in advancing the science of disease forecasts, the level of confidence of the public health community in exploiting these predictions in real-world situations remains unclear. The disconnect is in part due to poor understanding of modeling concepts by policy experts, which is compounded by a lack of a well-established operational framework for using and … 

[↵][1]1To whom correspondence should be addressed. Email: viboudc\{at\}mail.nih.gov.

 [1]: \#xref-corresp-1-1},
	language = {en},
	number = {8},
	urldate = {2020-04-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Viboud, Cécile and Vespignani, Alessandro},
	month = feb,
	year = {2019},
	pmid = {30737293},
	pages = {2802--2804},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/JFE2J84D/Viboud and Vespignani - 2019 - The future of influenza forecasts.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/67WHC8X4/2802.html:text/html}
}

@article{ferguson_impact_2020,
	title = {Impact of non-pharmaceutical interventions ({NPIs}) to reduce {COVID}- 19 mortality and healthcare demand},
	abstract = {The global impact of COVID-19 has been profound, and the public health threat it represents is the most serious seen in a respiratory virus since the 1918 H1N1 influenza pandemic. Here we present the results of epidemiological modelling which has informed policymaking in the UK and other countries in recent weeks. In the absence of a COVID-19 vaccine, we assess the potential role of a number of public health measures – so-called non-pharmaceutical interventions (NPIs) – aimed at reducing contact rates in the population and thereby reducing transmission of the virus. In the results presented here, we apply a previously published microsimulation model to two countries: the UK (Great Britain specifically) and the US. We conclude that the effectiveness of any one intervention in isolation is likely to be limited, requiring multiple interventions to be combined to have a substantial impact on transmission.},
	language = {en},
	author = {Ferguson, Neil M and Laydon, Daniel and Nedjati-Gilani, Gemma and Imai, Natsuko and Ainslie, Kylie and Baguelin, Marc and Bhatia, Sangeeta and Boonyasiri, Adhiratha and Cucunubá, Zulma and Cuomo-Dannenburg, Gina and Dighe, Amy and Fu, Han and Gaythorpe, Katy and Thompson, Hayley and Verity, Robert and Volz, Erik and Wang, Haowei and Wang, Yuanrong and Walker, Patrick GT and Walters, Caroline and Winskill, Peter and Whittaker, Charles and Donnelly, Christl A and Riley, Steven and Ghani, Azra C},
	year = {2020},
	pages = {20},
	file = {Ferguson et al. - 2020 - Impact of non-pharmaceutical interventions (NPIs) .pdf:/mnt/data/Google Drive/Zotero/storage/794A9ZNP/Ferguson et al. - 2020 - Impact of non-pharmaceutical interventions (NPIs) .pdf:application/pdf}
}

@misc{thibaut_jombart_emily_s_nightingale_mark_jit_olivier_le_polain_de_waroux_gwen_knight_stefan_flasche_rosalind_eggo_adam_j_kucharski_carl_ab_pearson_simon_r_procter_cmmid_ncov_working_group__w_john_edmunds_forecasting_2020,
	title = {Forecasting critical care bed requirements for {COVID}-19 patients in {England}},
	url = {https://cmmid.github.io/topics/covid19/current-patterns-transmission/ICU-projections.html},
	abstract = {We estimate critical care bed demand for COVID-19 cases in England for the next two weeks. Results suggest that current capacity might be reached or exceeded by the end of March 2020.},
	language = {en},
	urldate = {2020-04-12},
	journal = {CMMID Repository},
	author = {Thibaut Jombart, Emily S Nightingale, Mark Jit, Olivier le Polain de Waroux, Gwen Knight, Stefan Flasche, Rosalind Eggo, Adam J Kucharski, Carl A.B. Pearson, Simon R Procter, CMMID nCov working group \& W John Edmunds.},
	month = mar,
	year = {2020},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/R38ZM99F/ICU-projections.html:text/html}
}

@misc{noauthor_collaborative_nodate,
	title = {A collaborative multiyear, multimodel assessment of seasonal influenza forecasting in the {United} {States} {\textbar} {PNAS}},
	url = {https://www.pnas.org/content/116/8/3146},
	urldate = {2020-04-12},
	file = {A collaborative multiyear, multimodel assessment of seasonal influenza forecasting in the United States | PNAS:/mnt/data/Google Drive/Zotero/storage/Z66CQLI6/3146.html:text/html}
}

@misc{sam_abbott_cran_nodate,
	title = {{CRAN} - {Package} idmodelr},
	url = {https://cran.r-project.org/web/packages/idmodelr/index.html},
	urldate = {2020-04-12},
	author = {{Sam Abbott}},
	file = {CRAN - Package idmodelr:/mnt/data/Google Drive/Zotero/storage/PCKAL6ZT/index.html:text/html}
}

@article{finger_real-time_2019,
	title = {Real-time analysis of the diphtheria outbreak in forcibly displaced {Myanmar} nationals in {Bangladesh}},
	volume = {17},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-019-1288-7},
	doi = {10.1186/s12916-019-1288-7},
	abstract = {Between August and December 2017, more than 625,000 Rohingya from Myanmar fled into Bangladesh, settling in informal makeshift camps in Cox’s Bazar district and joining 212,000 Rohingya already present. In early November, a diphtheria outbreak hit the camps, with 440 reported cases during the first month. A rise in cases during early December led to a collaboration between teams from Médecins sans Frontières—who were running a provisional diphtheria treatment centre—and the London School of Hygiene and Tropical Medicine with the goal to use transmission dynamic models to forecast the potential scale of the outbreak and the resulting resource needs.},
	number = {1},
	urldate = {2020-04-12},
	journal = {BMC Medicine},
	author = {Finger, Flavio and Funk, Sebastian and White, Kate and Siddiqui, M. Ruby and Edmunds, W. John and Kucharski, Adam J.},
	month = mar,
	year = {2019},
	pages = {58},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/RZM5I6TE/Finger et al. - 2019 - Real-time analysis of the diphtheria outbreak in f.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/UPFFPNMC/s12916-019-1288-7.html:text/html}
}

@article{camacho_temporal_2015,
	title = {Temporal {Changes} in {Ebola} {Transmission} in {Sierra} {Leone} and {Implications} for {Control} {Requirements}: a {Real}-time {Modelling} {Study}},
	issn = {2157-3999},
	shorttitle = {Temporal {Changes} in {Ebola} {Transmission} in {Sierra} {Leone} and {Implications} for {Control} {Requirements}},
	url = {index.html%3Fp=55052.html},
	doi = {10.1371/currents.outbreaks.406ae55e83ec0b5193e30856b9235ed2},
	language = {English},
	urldate = {2020-04-12},
	journal = {PLOS Currents Outbreaks},
	author = {Camacho, Anton and Kucharski, Adam and Aki-Sawyerr, Yvonne and White, Mark A. and Flasche, Stefan and Baguelin, Marc and Pollington, Timothy and Carney, Julia R. and Glover, Rebecca and Smout, Elizabeth and Tiffany, Amanda and Edmunds, W. John and Funk, Sebastian},
	month = feb,
	year = {2015},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/Q2PBGAHY/Camacho et al. - 2015 - Temporal Changes in Ebola Transmission in Sierra L.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/3HPFHCTN/index.htmlp=55052.html:text/html}
}

@article{burkner_approximate_2019,
	title = {Approximate leave-future-out cross-validation for {Bayesian} time series models},
	url = {http://arxiv.org/abs/1902.06281},
	abstract = {One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model's future predictive accuracy, for instance, for the purpose of model comparison or selection. Exact cross-validation for Bayesian models is often computationally expensive, but approximate cross-validation methods have been developed, most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate because the information from future observations is available to influence predictions of the past. To properly account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation.},
	urldate = {2020-04-13},
	journal = {arXiv:1902.06281 [stat]},
	author = {Bürkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki},
	month = oct,
	year = {2019},
	note = {arXiv: 1902.06281},
	keywords = {Statistics - Methodology},
	annote = {Comment: 26 pages, 15 figures, 2 tables},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/VYSN2ZSQ/Bürkner et al. - 2019 - Approximate leave-future-out cross-validation for .pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/62CNNBDG/1902.html:text/html}
}

@article{zamo_estimation_2018,
	title = {Estimation of the {Continuous} {Ranked} {Probability} {Score} with {Limited} {Information} and {Applications} to {Ensemble} {Weather} {Forecasts}},
	volume = {50},
	issn = {1874-8953},
	url = {https://doi.org/10.1007/s11004-017-9709-7},
	doi = {10.1007/s11004-017-9709-7},
	abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
	language = {en},
	number = {2},
	urldate = {2020-04-13},
	journal = {Mathematical Geosciences},
	author = {Zamo, Michaël and Naveau, Philippe},
	month = feb,
	year = {2018},
	pages = {209--234},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/IRN58QLH/Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf:application/pdf}
}

@misc{noauthor_fk83bvarsv_nodate,
	title = {{FK83}/bvarsv},
	url = {https://github.com/FK83/bvarsv},
	abstract = {Analysis of the Primiceri (REStud, 2005) model. Contribute to FK83/bvarsv development by creating an account on GitHub.},
	language = {en},
	urldate = {2020-04-13},
	journal = {GitHub},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/34QH4CE5/bvarsv_Nov2015_website.html:text/html}
}

@article{haider_prediction_nodate,
	title = {Prediction with {Mixture} {Models}},
	language = {de},
	author = {Haider, Peter},
	pages = {58},
	file = {Haider - Prediction with Mixture Models.pdf:/mnt/data/Google Drive/Zotero/storage/UXIZ5K8I/Haider - Prediction with Mixture Models.pdf:application/pdf}
}

@article{qi_sample-expand_2014,
	title = {Sample-expand method for predicting the specified structure of microporous aluminophosphate},
	volume = {185},
	issn = {1387-1811},
	url = {http://www.sciencedirect.com/science/article/pii/S1387181113005167},
	doi = {10.1016/j.micromeso.2013.10.009},
	abstract = {Imbalanced data sets often exist in many real-world fields and this problem has got more and more attention in recent years. In this paper, a sample-expand method is proposed as data pre-processing procedure to improve the predictive performance of the zeolite synthesis on imbalance data set. First, the data pre-processing is implemented for expanding samples by exploring the marginal structure of the given data set using k-nearest neighbor algorithm (KNN). Then, the expanded data set is input to support vector machines (SVM) for classification. Finally, Q times n-fold cross-validations procedure (CVs) is adopted to assess the prediction performance. The advantage of the data pre-processing is that it can obtain stable data set for establishing the training model and abide by the classification criteria of SVM, such that the improved predictive performance is achievable. Moreover, other classical machine learning methods are also presented to accomplish the prediction task. Compared experimental results demonstrate that SVM method can reach very satisfactory predictive accuracy on the pre-processing data set. Specially, the phase diagram of gel composition is provided as a guiding role for subsequent rational synthesis experiments.},
	language = {en},
	urldate = {2020-04-15},
	journal = {Microporous and Mesoporous Materials},
	author = {Qi, Miao and Qin, Zhanmin and Gao, Na and Kong, Jun and Guo, Yuting and Lu, Yinghua},
	month = feb,
	year = {2014},
	keywords = {Prediction, -nearest neighbor, Sample-expand, Support vector machines, Zeolite synthesis},
	pages = {1--6},
	file = {ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/XIGBX538/S1387181113005167.html:text/html}
}

@article{zamo_estimation_2018-1,
	title = {Estimation of the {Continuous} {Ranked} {Probability} {Score} with {Limited} {Information} and {Applications} to {Ensemble} {Weather} {Forecasts}},
	volume = {50},
	issn = {1874-8953},
	url = {https://doi.org/10.1007/s11004-017-9709-7},
	doi = {10.1007/s11004-017-9709-7},
	abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
	language = {en},
	number = {2},
	urldate = {2020-05-12},
	journal = {Mathematical Geosciences},
	author = {Zamo, Michaël and Naveau, Philippe},
	month = feb,
	year = {2018},
	pages = {209--234},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/JKHD2EQY/Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf:application/pdf}
}

@article{hersbach_decomposition_2000,
	title = {Decomposition of the {Continuous} {Ranked} {Probability} {Score} for {Ensemble} {Prediction} {Systems}},
	volume = {15},
	issn = {0882-8156},
	url = {https://journals.ametsoc.org/doi/10.1175/1520-0434%282000%29015%3C0559%3ADOTCRP%3E2.0.CO%3B2},
	doi = {10.1175/1520-0434(2000)015<0559:DOTCRP>2.0.CO;2},
	abstract = {Some time ago, the continuous ranked probability score (CRPS) was proposed as a new verification tool for (probabilistic) forecast systems. Its focus is on the entire permissible range of a certain (weather) parameter. The CRPS can be seen as a ranked probability score with an infinite number of classes, each of zero width. Alternatively, it can be interpreted as the integral of the Brier score over all possible threshold values for the parameter under consideration. For a deterministic forecast system the CRPS reduces to the mean absolute error. In this paper it is shown that for an ensemble prediction system the CRPS can be decomposed into a reliability part and a resolution/uncertainty part, in a way that is similar to the decomposition of the Brier score. The reliability part of the CRPS is closely connected to the rank histogram of the ensemble, while the resolution/uncertainty part can be related to the average spread within the ensemble and the behavior of its outliers. The usefulness of such a decomposition is illustrated for the ensemble prediction system running at the European Centre for Medium-Range Weather Forecasts. The evaluation of the CRPS and its decomposition proposed in this paper can be extended to systems issuing continuous probability forecasts, by realizing that these can be interpreted as the limit of ensemble forecasts with an infinite number of members.},
	number = {5},
	urldate = {2020-05-12},
	journal = {Weather and Forecasting},
	author = {Hersbach, Hans},
	month = oct,
	year = {2000},
	note = {Publisher: American Meteorological Society},
	pages = {559--570},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/HB3ZPJY3/Hersbach - 2000 - Decomposition of the Continuous Ranked Probability.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/R4LMBJDM/1520-0434(2000)0150559DOTCRP2.0.html:text/html}
}

@techreport{lemaitre_assessing_2020,
	type = {preprint},
	title = {Assessing the impact of non-pharmaceutical interventions on {SARS}-{CoV}-2 transmission in {Switzerland}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.05.04.20090639},
	abstract = {Following the rapid dissemination of COVID-19 cases in Switzerland, large-scale non-pharmaceutical interventions (NPIs) were implemented by the cantons and the federal government between February 28 and March 20. Estimates of the impact of these interventions on SARS-CoV-2 transmission are critical for decision making in this and future outbreaks. We here aim to assess the impact of these NPIs on disease transmission by estimating changes in the basic reproduction number (R0) at national and cantonal levels in relation to the timing of these NPIs. We estimate the time-varying R0 nationally and in twelve cantons by fitting a stochastic transmission model explicitly simulating within hospital dynamics. We use individual-level data of {\textgreater}1,000 hospitalized patients in Switzerland and public daily reports of hospitalizations and deaths. We estimate the national R0 was 3.15 (95\% CI: 2.13-3.76) at the start of the epidemic. Starting from around March 6, we find a strong reduction in R0 with a 85\% median decrease (95\% quantile range, QR: 83\%-90\%) to a value of 0.44 (95\% QR: 0.27-0.65) in the period of March 29-April 5. At the cantonal-level R0 decreased over the course of the epidemic between 71\% and 94\%. We found that reductions in R0 were synchronous with changes in mobility patterns as estimated through smartphone activity, which started before the official implementation of NPIs. We found that most of the reduction of transmission is due to behavioural changes as opposed to natural immunity, the latter accounting for only about 3\% of the total reduction in effective transmission. As Switzerland considers relaxing some of the restrictions of social mixing, current estimates of R0 well below one are promising. However most of inferred transmission reduction was due to behaviour change ({\textless}3\% due to natural immunity buildup), with an estimated 97\% (95\% QR: 96.6\%-97.2\%) of the Swiss population still susceptible to SARS-CoV-2 as of April 24. These results warrant a cautious relaxation of social distance practices and close monitoring of changes in both the basic and effective reproduction numbers.},
	language = {en},
	urldate = {2020-05-13},
	institution = {Epidemiology},
	author = {Lemaitre, Joseph Chadi and Perez-Saez, Javier and Azman, Andrew and Rinaldo, Andrea and Fellay, Jacques},
	month = may,
	year = {2020},
	doi = {10.1101/2020.05.04.20090639},
	file = {Lemaitre et al. - 2020 - Assessing the impact of non-pharmaceutical interve.pdf:/mnt/data/Google Drive/Zotero/storage/63CW59X2/Lemaitre et al. - 2020 - Assessing the impact of non-pharmaceutical interve.pdf:application/pdf}
}

@incollection{wilks_univariate_2018,
	title = {Univariate {Ensemble} {Postprocessing}},
	isbn = {978-0-12-812372-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128123720000030},
	language = {en},
	urldate = {2020-05-14},
	booktitle = {Statistical {Postprocessing} of {Ensemble} {Forecasts}},
	publisher = {Elsevier},
	author = {Wilks, Daniel S.},
	year = {2018},
	doi = {10.1016/B978-0-12-812372-0.00003-0},
	pages = {49--89},
	file = {Wilks - 2018 - Univariate Ensemble Postprocessing.pdf:/mnt/data/Google Drive/Zotero/storage/T9LR4PUI/Wilks - 2018 - Univariate Ensemble Postprocessing.pdf:application/pdf}
}

@article{thorey_online_2017,
	title = {Online learning with the {Continuous} {Ranked} {Probability} {Score} for ensemble forecasting: {Ensemble} {Online} {Learning}},
	volume = {143},
	issn = {00359009},
	shorttitle = {Online learning with the {Continuous} {Ranked} {Probability} {Score} for ensemble forecasting},
	url = {http://doi.wiley.com/10.1002/qj.2940},
	doi = {10.1002/qj.2940},
	abstract = {Ensemble forecasting resorts to multiple individual forecasts to produce a discrete probability distribution which accurately represents the uncertainties. Before every forecast, a weighted empirical distribution function is derived from the ensemble, so as to minimize the Continuous Ranked Probability Score (CRPS). We apply online learning techniques, which have previously been used for deterministic forecasting, and we adapt them for the minimization of the CRPS. The proposed method theoretically guarantees that the aggregated forecast competes, in terms of CRPS, against the best weighted empirical distribution function with weights constant in time. This is illustrated on synthetic data. Besides, our study improves the knowledge of the CRPS expectation for model mixtures. We generalize results on the bias of the CRPS computed with ensemble forecasts, and propose a new scheme to achieve fair CRPS minimization, without any assumption on the distributions.},
	language = {en},
	number = {702},
	urldate = {2020-05-14},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Thorey, J. and Mallet, V. and Baudin, P.},
	month = jan,
	year = {2017},
	pages = {521--529},
	file = {Thorey et al. - 2017 - Online learning with the Continuous Ranked Probabi.pdf:/mnt/data/Google Drive/Zotero/storage/CR9EWRQ8/Thorey et al. - 2017 - Online learning with the Continuous Ranked Probabi.pdf:application/pdf}
}

@article{wickramasuriya_optimal_2019,
	title = {Optimal {Forecast} {Reconciliation} for {Hierarchical} and {Grouped} {Time} {Series} {Through} {Trace} {Minimization}},
	volume = {114},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1448825},
	doi = {10.1080/01621459.2018.1448825},
	abstract = {Large collections of time series often have aggregation constraints due to product or geographical groupings. The forecasts for the most disaggregated series are usually required to add-up exactly to the forecasts of the aggregated series, a constraint we refer to as “coherence”. Forecast reconciliation is the process of adjusting forecasts to make them coherent.},
	language = {en},
	number = {526},
	urldate = {2020-05-15},
	journal = {Journal of the American Statistical Association},
	author = {Wickramasuriya, Shanika L. and Athanasopoulos, George and Hyndman, Rob J.},
	month = apr,
	year = {2019},
	pages = {804--819},
	file = {Wickramasuriya et al. - 2019 - Optimal Forecast Reconciliation for Hierarchical a.pdf:/mnt/data/Google Drive/Zotero/storage/TIDP2UWF/Wickramasuriya et al. - 2019 - Optimal Forecast Reconciliation for Hierarchical a.pdf:application/pdf}
}

@article{makridakis_statistical_2018,
	title = {Statistical and {Machine} {Learning} forecasting methods: {Concerns} and ways forward},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Statistical and {Machine} {Learning} forecasting methods},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889},
	doi = {10.1371/journal.pone.0194889},
	abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
	language = {en},
	number = {3},
	urldate = {2020-05-15},
	journal = {PLOS ONE},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	month = mar,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Forecasting, Support vector machines, Computing methods, Neural networks, Optimization, Preprocessing, Statistical methods},
	pages = {e0194889},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/GV5QJGJ8/Makridakis et al. - 2018 - Statistical and Machine Learning forecasting metho.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/NSJH4N43/article.html:text/html}
}

@article{burkner_brms_2017,
	title = {\textbf{brms} : {An} \textit{{R}} {Package} for {Bayesian} {Multilevel} {Models} {Using} \textit{{Stan}}},
	volume = {80},
	issn = {1548-7660},
	shorttitle = {\textbf{brms}},
	url = {http://www.jstatsoft.org/v80/i01/},
	doi = {10.18637/jss.v080.i01},
	abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to ﬁt – among others – linear, robust linear, binomial, Poisson, survival, response times, ordinal, quantile, zero-inﬂated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user deﬁned covariance structures, censored data, as well as metaanalytic standard errors. Prior speciﬁcations are ﬂexible and explicitly encourage users to apply prior distributions that actually reﬂect their beliefs. In addition, model ﬁt can easily be assessed and compared using posterior-predictive checks and leave-one-out crossvalidation. If you use brms, please cite this article as published in the Journal of Statistical Software (Bu¨rkner 2017).},
	language = {en},
	number = {1},
	urldate = {2020-05-15},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	year = {2017},
	file = {Bürkner - 2017 - bbrmsb  An iRi Package for Bayesian Mul.pdf:/mnt/data/Google Drive/Zotero/storage/LX9P9SKT/Bürkner - 2017 - bbrmsb  An iRi Package for Bayesian Mul.pdf:application/pdf}
}

@techreport{gibson_improving_2019-1,
	type = {preprint},
	title = {Improving {Probabilistic} {Infectious} {Disease} {Forecasting} {Through} {Coherence}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2019.12.27.889212},
	abstract = {With an estimated \$10.4 billion in medical costs and 31.4 million outpatient visits each year, inﬂuenza poses a serious burden of disease in the United States. To provide insights and advance warning into the spread of inﬂuenza, the U.S. Centers for Disease Control and Prevention (CDC) runs a challenge for forecasting weighted inﬂuenza-like illness (wILI) at the national and regional level. Many models produce independent forecasts for each geographical unit, ignoring the constraint that the national wILI is a weighted sum of regional wILI, where the weights correspond to the population size of the region. We propose a novel algorithm that transforms a set of independent forecast distributions to obey this constraint, which we refer to as probabilistically coherent. Enforcing probabilistic coherence led to an increase in forecast skill for 90\% of the models we tested over multiple ﬂu seasons, highlighting the importance of respecting the forecasting system’s geographical hierarchy.},
	language = {en},
	urldate = {2020-05-16},
	institution = {Bioinformatics},
	author = {Gibson, Graham Casey and Moran, Kelly R. and Reich, Nicholas G. and Osthus, Dave},
	month = dec,
	year = {2019},
	doi = {10.1101/2019.12.27.889212},
	file = {Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:/mnt/data/Google Drive/Zotero/storage/LRMDHF65/Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:application/pdf}
}

@article{yao_using_2018-1,
	title = {Using {Stacking} to {Average} {Bayesian} {Predictive} {Distributions} (with {Discussion})},
	volume = {13},
	issn = {1936-0975},
	url = {https://projecteuclid.org/euclid.ba/1516093227},
	doi = {10.1214/17-BA1091},
	abstract = {Bayesian model averaging is ﬂawed in the M-open setting in which the true data-generating process is not one of the candidate models being ﬁt. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions. We extend the utility function to any proper scoring rule and use Pareto smoothed importance sampling to eﬃciently compute the required leave-one-out posterior distributions. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), Pseudo-BMA, and a variant of Pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with bootstrapped-Pseudo-BMA as an approximate alternative when computation cost is an issue.},
	language = {en},
	number = {3},
	urldate = {2020-05-16},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	month = sep,
	year = {2018},
	pages = {917--1007},
	file = {Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:/mnt/data/Google Drive/Zotero/storage/CAU9DYUN/Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:application/pdf}
}

@article{donnat_modeling_2020,
	title = {Modeling the {Heterogeneity} in {COVID}-19's {Reproductive} {Number} and its {Impact} on {Predictive} {Scenarios}},
	url = {http://arxiv.org/abs/2004.05272},
	abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
	language = {en},
	urldate = {2020-05-19},
	journal = {arXiv:2004.05272 [q-bio, stat]},
	author = {Donnat, Claire and Holmes, Susan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05272},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:/mnt/data/Google Drive/Zotero/storage/J5RVUD26/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:application/pdf}
}

@article{donnat_modeling_2020-1,
	title = {Modeling the {Heterogeneity} in {COVID}-19's {Reproductive} {Number} and its {Impact} on {Predictive} {Scenarios}},
	url = {http://arxiv.org/abs/2004.05272},
	abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
	language = {en},
	urldate = {2020-05-25},
	journal = {arXiv:2004.05272 [q-bio, stat]},
	author = {Donnat, Claire and Holmes, Susan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05272},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:/mnt/data/Google Drive/Zotero/storage/VWXSMSET/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:application/pdf}
}

@article{yao_bayesian_2019,
	title = {Bayesian {Aggregation}},
	url = {http://arxiv.org/abs/1912.11218},
	abstract = {A general challenge in statistics is prediction in the presence of multiple candidate models or learning algorithms. Model aggregation tries to combine all predictive distributions from individual models, which is more stable and flexible than single model selection. In this article we describe when and how to aggregate models under the lens of Bayesian decision theory. Among two widely used methods, Bayesian model averaging (BMA) and Bayesian stacking, we compare their predictive performance, and review their theoretical optimality, probabilistic interpretation, practical implementation, and extensions in complex models.},
	urldate = {2020-05-25},
	journal = {arXiv:1912.11218 [stat]},
	author = {Yao, Yuling},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.11218},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/U3WDTIZ5/Yao - 2019 - Bayesian Aggregation.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/Y2K797FW/1912.html:text/html}
}

@article{nowotarski_computing_2015,
	title = {Computing electricity spot price prediction intervals using quantile regression and forecast averaging},
	volume = {30},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-014-0523-0},
	doi = {10.1007/s00180-014-0523-0},
	abstract = {We examine possible accuracy gains from forecast averaging in the context of interval forecasts of electricity spot prices. First, we test whether constructing empirical prediction intervals (PI) from combined electricity spot price forecasts leads to better forecasts than those obtained from individual methods. Next, we propose a new method for constructing PI—Quantile Regression Averaging (QRA)—which utilizes the concept of quantile regression and a pool of point forecasts of individual (i.e. not combined) models. While the empirical PI from combined forecasts do not provide significant gains, the QRA-based PI are found to be more accurate than those of the best individual model—the smoothed nonparametric autoregressive model.},
	language = {en},
	number = {3},
	urldate = {2020-05-27},
	journal = {Computational Statistics},
	author = {Nowotarski, Jakub and Weron, Rafał},
	month = sep,
	year = {2015},
	pages = {791--803},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/73B9W3G7/Nowotarski and Weron - 2015 - Computing electricity spot price prediction interv.pdf:application/pdf}
}

@article{raftery_using_2005,
	title = {Using {Bayesian} {Model} {Averaging} to {Calibrate} {Forecast} {Ensembles}},
	volume = {133},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/full/10.1175/MWR2906.1},
	doi = {10.1175/MWR2906.1},
	abstract = {Ensembles used for probabilistic weather forecasting often exhibit a spread-error correlation, but they tend to be underdispersive. This paper proposes a statistical method for postprocessing ensembles based on Bayesian model averaging (BMA), which is a standard method for combining predictive distributions from different sources. The BMA predictive probability density function (PDF) of any quantity of interest is a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights are equal to posterior probabilities of the models generating the forecasts and reflect the models' relative contributions to predictive skill over the training period. The BMA weights can be used to assess the usefulness of ensemble members, and this can be used as a basis for selecting ensemble members; this can be useful given the cost of running large ensembles. The BMA PDF can be represented as an unweighted ensemble of any desired size, by simulating from the BMA predictive distribution. The BMA predictive variance can be decomposed into two components, one corresponding to the between-forecast variability, and the second to the within-forecast variability. Predictive PDFs or intervals based solely on the ensemble spread incorporate the first component but not the second. Thus BMA provides a theoretical explanation of the tendency of ensembles to exhibit a spread-error correlation but yet be underdispersive. The method was applied to 48-h forecasts of surface temperature in the Pacific Northwest in January–June 2000 using the University of Washington fifth-generation Pennsylvania State University–NCAR Mesoscale Model (MM5) ensemble. The predictive PDFs were much better calibrated than the raw ensemble, and the BMA forecasts were sharp in that 90\% BMA prediction intervals were 66\% shorter on average than those produced by sample climatology. As a by-product, BMA yields a deterministic point forecast, and this had root-mean-square errors 7\% lower than the best of the ensemble members and 8\% lower than the ensemble mean. Similar results were obtained for forecasts of sea level pressure. Simulation experiments show that BMA performs reasonably well when the underlying ensemble is calibrated, or even overdispersed.},
	number = {5},
	urldate = {2020-06-02},
	journal = {Monthly Weather Review},
	author = {Raftery, Adrian E. and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
	month = may,
	year = {2005},
	note = {Publisher: American Meteorological Society},
	pages = {1155--1174},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BVZ3BIIP/Raftery et al. - 2005 - Using Bayesian Model Averaging to Calibrate Foreca.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/HZJ3R7W2/MWR2906.html:text/html}
}

@article{rubin_bayesian_1981,
	title = {The {Bayesian} {Bootstrap}},
	volume = {9},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176345338},
	doi = {10.1214/aos/1176345338},
	abstract = {The Bayesian bootstrap is the Bayesian analogue of the bootstrap. Instead of simulating the sampling distribution of a statistic estimating a parameter, the Bayesian bootstrap simulates the posterior distribution of the parameter; operationally and inferentially the methods are quite similar. Because both methods of drawing inferences are based on somewhat peculiar model assumptions and the resulting inferences are generally sensitive to these assumptions, neither method should be applied without some consideration of the reasonableness of these model assumptions. In this sense, neither method is a true bootstrap procedure yielding inferences unaided by external assumptions.},
	language = {EN},
	number = {1},
	urldate = {2020-06-02},
	journal = {Annals of Statistics},
	author = {Rubin, Donald B.},
	month = jan,
	year = {1981},
	mrnumber = {MR600538},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Dirichlet, jackknife, Model-free inference},
	pages = {130--134},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/EIYG5S5U/Rubin - 1981 - The Bayesian Bootstrap.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/3HHY4A4F/1176345338.html:text/html}
}

@article{clarke_comparing_nodate,
	title = {Comparing {Bayes} {Model} {Averaging} and {Stacking} {When} {Model} {Approximation} {Error} {Cannot} be {Ignored}},
	abstract = {We compare Bayes Model Averaging, BMA, to a non-Bayes form of model averaging called stacking. In stacking, the weights are no longer posterior probabilities of models; they are obtained by a technique based on cross-validation. When the correct data generating model (DGM) is on the list of models under consideration BMA is never worse than stacking and often is demonstrably better, provided that the noise level is of order commensurate with the coefﬁcients and explanatory variables. Here, however, we focus on the case that the correct DGM is not on the model list and may not be well approximated by the elements on the model list.},
	language = {en},
	author = {Clarke, Bertrand},
	pages = {30},
	file = {Clarke - Comparing Bayes Model Averaging and Stacking When .pdf:/mnt/data/Google Drive/Zotero/storage/SFV2ENGU/Clarke - Comparing Bayes Model Averaging and Stacking When .pdf:application/pdf}
}

@article{fragoso_bayesian_2018,
	title = {Bayesian {Model} {Averaging}: {A} {Systematic} {Review} and {Conceptual} {Classification}},
	volume = {86},
	copyright = {© 2017 The Authors. International Statistical Review © 2017 International Statistical Institute},
	issn = {1751-5823},
	shorttitle = {Bayesian {Model} {Averaging}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12243},
	doi = {10.1111/insr.12243},
	abstract = {Bayesian model averaging (BMA) provides a coherent and systematic mechanism for accounting for model uncertainty. It can be regarded as an direct application of Bayesian inference to the problem of model selection, combined estimation and prediction. BMA produces a straightforward model choice criterion and less risky predictions. However, the application of BMA is not always straightforward, leading to diverse assumptions and situational choices on its different aspects. Despite the widespread application of BMA in the literature, there were not many accounts of these differences and trends besides a few landmark revisions in the late 1990s and early 2000s, therefore not accounting for advancements made in the last decades. In this work, we present an account of these developments through a careful content analysis of 820 articles in BMA published between 1996 and 2016. We also develop a conceptual classification scheme to better describe this vast literature, understand its trends and future directions and provide guidance for the researcher interested in both the application and development of the methodology. The results of the classification scheme and content review are then used to discuss the present and future of the BMA literature.},
	language = {en},
	number = {1},
	urldate = {2020-06-02},
	journal = {International Statistical Review},
	author = {Fragoso, Tiago M. and Bertoli, Wesley and Louzada, Francisco},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12243},
	keywords = {Bayesian model averaging, conceptual classification scheme, qualitative content analysis, systematic review},
	pages = {1--28},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6M4VGYYN/Fragoso et al. - 2018 - Bayesian Model Averaging A Systematic Review and .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/XG55P67G/insr.html:text/html}
}

@article{svensson_note_2007,
	title = {A note on generation times in epidemic models},
	volume = {208},
	issn = {0025-5564},
	url = {http://www.sciencedirect.com/science/article/pii/S0025556406002094},
	doi = {10.1016/j.mbs.2006.10.010},
	abstract = {The time between the infection of a primary case and one of its secondary cases is called a generation time. The distribution (and mean) of the generation times is derived for a rather general class of epidemic models. The relation to assumptions on distributions of latency times and infectious times or more generally on random time varying infectiousness, is investigated. Serial times, defined as the times between occurrence of observable events in the progress of an infectious disease (e.g., the onset of clinical symptoms), are also considered.},
	language = {en},
	number = {1},
	urldate = {2020-06-05},
	journal = {Mathematical Biosciences},
	author = {Svensson, Åke},
	month = jul,
	year = {2007},
	keywords = {Epidemic models, Generation times, Serial times, Transmission intervals},
	pages = {300--311},
	file = {ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/UDUF9FLS/S0025556406002094.html:text/html}
}

@article{britton_estimation_2019,
	title = {Estimation in emerging epidemics: biases and remedies},
	volume = {16},
	shorttitle = {Estimation in emerging epidemics},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2018.0670},
	doi = {10.1098/rsif.2018.0670},
	abstract = {When analysing new emerging infectious disease outbreaks, one typically has observational data over a limited period of time and several parameters to estimate, such as growth rate, the basic reproduction number R0, the case fatality rate and distributions of serial intervals, generation times, latency and incubation times and times between onset of symptoms, notification, death and recovery/discharge. These parameters form the basis for predicting a future outbreak, planning preventive measures and monitoring the progress of the disease outbreak. We study inference problems during the emerging phase of an outbreak, and point out potential sources of bias, with emphasis on: contact tracing backwards in time, replacing generation times by serial intervals, multiple potential infectors and censoring effects amplified by exponential growth. These biases directly affect the estimation of, for example, the generation time distribution and the case fatality rate, but can then propagate to other estimates such as R0 and growth rate. We propose methods to remove or at least reduce bias using statistical modelling. We illustrate the theory by numerical examples and simulations.},
	number = {150},
	urldate = {2020-06-05},
	journal = {Journal of The Royal Society Interface},
	author = {Britton, Tom and Scalia Tomba, Gianpaolo},
	month = jan,
	year = {2019},
	note = {Publisher: Royal Society},
	pages = {20180670},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/UE77Z7A2/Britton and Scalia Tomba - 2019 - Estimation in emerging epidemics biases and remed.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/DZF7TEZF/rsif.2018.html:text/html}
}

@article{bracher_evaluating_2020,
	title = {Evaluating epidemic forecasts in an interval format},
	url = {http://arxiv.org/abs/2005.12881},
	abstract = {For practical reasons, many forecasts of case, hospitalization and death counts in the context of the current COVID-19 pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub run by the UMass-Amherst Influenza Forecasting Center of Excellence. Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This note provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a simple decomposition into a measure of sharpness and penalties for over- and underprediction.},
	urldate = {2020-06-06},
	journal = {arXiv:2005.12881 [q-bio, stat]},
	author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12881},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/ICGZJDHT/Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/T3UZEKSU/2005.html:text/html}
}

@article{bracher_evaluating_2020-1,
	title = {Evaluating epidemic forecasts in an interval format},
	url = {http://arxiv.org/abs/2005.12881},
	abstract = {For practical reasons, many forecasts of case, hospitalization and death counts in the context of the current COVID-19 pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID19 Forecast Hub run by the UMass-Amherst Inﬂuenza Forecasting Center of Excellence. Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This note provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts. Speciﬁcally, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a simple decomposition into a measure of sharpness and penalties for over- and underprediction.},
	language = {en},
	urldate = {2020-06-06},
	journal = {arXiv:2005.12881 [q-bio, stat]},
	author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12881},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:/mnt/data/Google Drive/Zotero/storage/CUL2NHPC/Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:application/pdf}
}

@article{flaxman_estimating_2020,
	title = {Estimating the effects of non-pharmaceutical interventions on {COVID}-19 in {Europe}},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2405-7},
	doi = {10.1038/s41586-020-2405-7},
	abstract = {Following the emergence of a novel coronavirus1 (SARS-CoV-2) and its spread outside of China, Europe has experienced large epidemics. In response, many European countries have implemented unprecedented non-pharmaceutical interventions such as closure of schools and national lockdowns. We study the impact of major interventions across 11 European countries for the period from the start of COVID-19 until the 4th of May 2020 when lockdowns started to be lifted. Our model calculates backwards from observed deaths to estimate transmission that occurred several weeks prior, allowing for the time lag between infection and death. We use partial pooling of information between countries with both individual and shared effects on the reproduction number. Pooling allows more information to be used, helps overcome data idiosyncrasies, and enables more timely estimates. Our model relies on fixed estimates of some epidemiological parameters such as the infection fatality rate, does not include importation or subnational variation and assumes that changes in the reproduction number are an immediate response to interventions rather than gradual changes in behavior. Amidst the ongoing pandemic, we rely on death data that is incomplete, with systematic biases in reporting, and subject to future consolidation. We estimate that, for all the countries we consider, current interventions have been sufficient to drive the reproduction number \$\$\{R\}\_\{t\}\$\$Rt below 1 (probability \$\$\{R\}\_\{t\}{\textbackslash},\$\$Rt{\textless} 1.0 is 99.9\%) and achieve epidemic control. We estimate that, across all 11 countries, between 12 and 15 million individuals have been infected with SARS-CoV-2 up to 4th May, representing between 3.2\% and 4.0\% of the population. Our results show that major non-pharmaceutical interventions and lockdown in particular have had a large effect on reducing transmission. Continued intervention should be considered to keep transmission of SARS-CoV-2 under control.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Nature},
	author = {Flaxman, Seth and Mishra, Swapnil and Gandy, Axel and Unwin, H. Juliette T. and Mellan, Thomas A. and Coupland, Helen and Whittaker, Charles and Zhu, Harrison and Berah, Tresnia and Eaton, Jeffrey W. and Monod, Mélodie and Ghani, Azra C. and Donnelly, Christl A. and Riley, Steven M. and Vollmer, Michaela A. C. and Ferguson, Neil M. and Okell, Lucy C. and Bhatt, Samir},
	month = jun,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {1--8},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/7BXLI453/Flaxman et al. - 2020 - Estimating the effects of non-pharmaceutical inter.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/69J4UAFM/s41586-020-2405-7.html:text/html}
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	copyright = {© 2019 The Authors. Statistics in Medicine  Published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
	doi = {10.1002/sim.8086},
	abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
	language = {en},
	number = {11},
	urldate = {2020-06-18},
	journal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
	keywords = {graphics for simulation, Monte Carlo, simulation design, simulation reporting, simulation studies},
	pages = {2074--2102},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/SLAE8FU9/Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/P294K42G/sim.html:text/html}
}

@article{hoge_bayesian_2020,
	title = {Bayesian {Model} {Weighting}: {The} {Many} {Faces} of {Model} {Averaging}},
	volume = {12},
	issn = {2073-4441},
	shorttitle = {Bayesian {Model} {Weighting}},
	url = {https://www.mdpi.com/2073-4441/12/2/309},
	doi = {10.3390/w12020309},
	abstract = {Model averaging makes it possible to use multiple models for one modelling task, like predicting a certain quantity of interest. Several Bayesian approaches exist that all yield a weighted average of predictive distributions. However, often, they are not properly applied which can lead to false conclusions. In this study, we focus on Bayesian Model Selection (BMS) and Averaging (BMA), Pseudo-BMS/BMA and Bayesian Stacking. We want to foster their proper use by, ﬁrst, clarifying their theoretical background and, second, contrasting their behaviours in an applied groundwater modelling task. We show that only Bayesian Stacking has the goal of model averaging for improved predictions by model combination. The other approaches pursue the quest of ﬁnding a single best model as the ultimate goal, and use model averaging only as a preliminary stage to prevent rash model choice. Improved predictions are thereby not guaranteed. In accordance with so-called M-settings that clarify the alleged relations between models and truth, we elicit which method is most promising.},
	language = {en},
	number = {2},
	urldate = {2020-06-20},
	journal = {Water},
	author = {Höge, Marvin and Guthke, Anneli and Nowak, Wolfgang},
	month = jan,
	year = {2020},
	pages = {309},
	file = {Höge et al. - 2020 - Bayesian Model Weighting The Many Faces of Model .pdf:/mnt/data/Google Drive/Zotero/storage/4J7B4XSH/Höge et al. - 2020 - Bayesian Model Weighting The Many Faces of Model .pdf:application/pdf}
}

@article{wilkinson_modelling_1977,
	title = {A modelling approach to {ABC}},
	language = {en},
	author = {Wilkinson, Richard},
	year = {1977},
	pages = {88},
	file = {Wilkinson - 1977 - A modelling approach to ABC.pdf:/mnt/data/Google Drive/Zotero/storage/95L75H5X/Wilkinson - 1977 - A modelling approach to ABC.pdf:application/pdf}
}

@article{roberts_gaussian_2013,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge inﬂuences design of the Gaussian process models and provide case examples to highlight the approaches.},
	language = {en},
	number = {1984},
	urldate = {2020-07-10},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
	month = feb,
	year = {2013},
	pages = {20110550},
	file = {Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:/mnt/data/Google Drive/Zotero/storage/7G2A6VD8/Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:application/pdf}
}

@article{roberts_gaussian_2013-1,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge inﬂuences design of the Gaussian process models and provide case examples to highlight the approaches.},
	language = {en},
	number = {1984},
	urldate = {2020-07-12},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
	month = feb,
	year = {2013},
	pages = {20110550},
	file = {Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:/mnt/data/Google Drive/Zotero/storage/KDQXM7AN/Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:application/pdf}
}
