# Summary and discussion {#discussion}

#### Thesis summary {-}

In this thesis we have given an extensive overview of different theoretical aspects of model evaluation and model aggregation and have applied this theory to the case study of eight models from the US Forecast Hub. We have discussed the forecasting paradigm and the notions of calibration and sharpness, and have presented several well-studied evaluation metrics as well as some previously unpublished. Based on these theoretical considerations we proposed a structured evaluation approach that was subsequently applied to data from the Forecast Hub. We also presented the `scoringutils` package that facilitates the evaluation process. Building on the discussion of proper scoring rules, we introduced two different model aggregation techniques. One of them constructs a quantile average ensemble to minimise the weighted interval score, while the other forms a mixture distribution based on the Continuous Ranked Probability Score. In order to implement the CRPS ensemble approach, we have introduced the `stackr` package. After the theoretical discussion of model evaluation and model aggregation, we provided background on the Forecast Hub and presented the data and models analysed in the ensuing case study. Applying the tools presented previously, we evaluated the performance of the eight Forecast Hub models and the three ensembles in detail. 

This thesis had three main objectives: To obtain a deeper understanding of model evaluation, 
to explore ways to aggregate models to ensembles, and to facilitate model evaluation and model
aggregation by creating appropriate tools. The following paragraphs address these objectives in the order in which they were discussed in this thesis, followed by a discussion of limitations and suggestions for future research. 

#### Results summary {-}

In order to facilitate model evaluation and model aggregation, we have successfully developed the `scoringutils` and `stackr` packages and demonstrated their value throughout this thesis. Both packages are still under active development and we hope to use them in future applications beyond this thesis.

By conducting a detailed evaluation of the Forecast Hub models and the ensembles, we obtained a deeper understanding of the models analysed as well of the evaluation process itself and the metrics involved. The structured evaluation approach proposed in Chapter \@ref(evaluation) provided a useful guideline and framework for this evaluation process. We found that the metrics discussed measured different aspects of performance, but all had a significant influence on the Weighted Interval Score. This suggests that they were all able to contribute something useful in terms of model evaluation and therefore merit their own attention. 

The examination of relationships between the metrics should be interpreted in view of trends in the actual data. While we would have expected a negative correlation between sharpness and penalties for over- and underprediction, we saw the opposite. This suggests that even though we should see a trade-off, better performing models tended to do better on both accounts. We also observed that in the data positive coverage deviation was associated with smaller WIS. While unnecessarily high interval coverage should increase Interval Scores, the affected models still performed better on average than models with lower interval coverage. 

For better performing models, the sharpness component of the WIS played a larger relative role for overall scores than for models that struggled with calibration. For those, WIS performance was dominated by the penalties for over- and underprediction. This finding is plausible in light of our goal to maximisie sharpness subject to calibration. Overall Interval Scores were dominated by predictions around the median of the predictive distributions, while tails mattered less. This can be attributed to the construction of the WIS that gives less weights to outer prediction intervals, as well as to the fact that outer prediction intervals incur fewer misprediction penalties. Ranges around the 50% prediction interval contributed most to sharpness. 
We explored two slightly different ways of assessing systematic over- or underprediction. 

The bias metric presented in Chapter \@ref(evaluation) proved to be more reflective of the general tendency of a model to over- or underpredict. The misprediction penalty component of the WIS was more heavily influenced by extreme values. Unfortunately, it remained unclear which of these two measures should take precedence in evaluation or model improvement. We observed that better performing models were also noticeably better calibrated. We were mostly able to diagnose and locate calibration issues very precisely using coverage plots and PIT histograms. However, we also saw that the aggregate picture may be misleading or at least incomplete. This became most apparent with the epiforecasts-ensemble1 model that showed good calibration on the aggregate level, but sometimes exhibited miscalibration in individual locations. The models analysed showed a lot of variation in terms of sharpness. Comparing sharpness between models therefore only made sense after splitting the models in two groups of about equal performance. Within these groups, we could observe a tendency for better models to also be sharper. We could, however, not see models consistently increase or decrease their sharpness in response to past performance. 

We explored the QRA and CRPS ensembles alongside the mean-ensemble and the COVIDhub-ensemble that served as control and benchmark. We found that all ensemble models tended to perform very well, but could not clearly identify one ensemble that was superior to the others. For the qra-ensemble, taking four weeks of past observations into account was optimal consistently. We also observed that the qra-ensemble with only one week of past data performed almost equally well, while the ones with two and three weeks did not. This finding should therefore be treated with caution. For the crps-ensemble, we found that optimising for two-weeks-ahead forecasts consistently led to the best performance. This should again be regarded as limited evidence, given the small numbers of models, time points and locations analysed here. The crps-ensemble was able perform similarly well as the qra-ensemble, even though the model aggregation process involved fitting a distribution to quantiles. For most crps-ensemble variants we used a gamma distribution, but also fitted a metalog-distribution to examine the sensitivity of the results. We found that this did not improve performance significantly. This suggests that the imprecision introduced by fitting a gamma distribution did not have a large effect. Inaccuracies mostly affected the tails of the predictive distributions, which only had a small weight in terms of overall WIS. As we did not thoroughly test the metalog-distribution ensemble, research is needed to come to more definitive conclusions. We observed a tendency for the qra-ensemble to overpredict, while the crps-ensemble exhibited a slight downwards bias. We were, however, not able to provide an explanation for this result. When examining ensemble weights we observed that even models not among the top performers contributed to both ensembles. This suggests that including a greater number of diverse models in the ensemble may help improve performance. 


#### Limitations {-}

Limited knowledge of the model details made it hard to point to specific model characteristics that could explain better or worse performance and as well to make suggestions for improvement. We found that two out of three SEIR models were among the top performers, but this should not be considered as strong evidence in favour of SEIR models given the small number of models and observations. Models tended to perform consistently well or badly and we therefore could not unambiguously identify relative model advantages in specific situations. Relative advantages between models were dwarfed by general performance differences. 
As we were not able to adequately handle missing predictions, the choice to include the epiforecasts-ensemble1 model made it necessary to restrict the set of locations and forecast dates to a very limited set. This makes it hard to generalise patterns observed throughout the model evaluation process. All results should therefore be treated with caution. On the other hand, including even more locations and models would have made it even harder to devote appropriate attention to individual models. Evaluating eleven models at the same time already meant that the majority of the analysis was conducted on an aggregate level. 
The analysis was also limited by the the need to develop appropriate software. We were, for example, not able incorporate more than one forecast horizon in the CRPS ensemble or could not create PIT histograms without fitting samples to the quantiles first. Not having predictive samples available also limited comparability between the CRPS and QRA ensemble models. 


#### Outlook and future research {-}

A variety of interesting research questions warrant further investigation. Most notably, the evaluation process demonstrated here could be expanded to all models and observations from the Forecast Hub. While the evaluation results obtained here are only of limited value due to the small number of models and observations included, a full evaluation would be of great interest to researchers and policy makers. Results could then be compared with other analyses that are currently being conducted. A full evaluation would also allow us to determine whether the CRPS and QRA ensembles could have outperformed the COVIDhub-ensemble, had they had access to the identical candidate models to inform their ensemble distributions. Ideally, we would also like to see an equally weighted mixture distribution alongside the COVIDhub-ensemble (an equally weighted quantile average ensemble). This would make it possible to determine whether one of the two strategies to combine predictive distributions is superior to the other, irrespective of the optimisation strategy. 

Further research could also be done with respect to improving the two model aggregation techniques. The CRPS ensembles could potentially be improved by allowing for more than one forecast horizon to inform weights. As we observed a tendency of the QRA to ensemble to overpredict, we suggest to investigate whether this is a consistent property of the QRA that could be leveraged to further improve the model aggregation approach. In addition, weights for both models could be varied by state instead of having one weight per location. This would make it possible for the ensemble to leverage relative advantages that models may enjoy in specific locations or circumstances. Ideally, one would probably like to approach this in a hierarchical framework, where weights would be shared across locations unless there is strong evidence that weights should be different in a specific location. Analogously, different ranges can be weighted differently for QRA ensembles, but we do not currently have good knowledge of situations in which this might be advantageous. To improve how models adapt their sharpness in response to past performance, we would also like to investigate post-processing of forecasts based on accordance with past observations. 

With regards to the tools presented in this thesis, we plan to add the plots presented throughout this thesis to the `scoringutils` package. Ideally, we would also like to add an automated report that users could generate to obtain feedback for their model. We also intend to increase the flexibility of the `stackr` package by allowing for multiple forecast horizons at a time. Overall, we hope this thesis has provided an important first step that motivates and enables future research. 






<!-- In terms of overall model development,  To make results from the evaluation process more useful, one could explore whether one should, in case they disagree, rather follow the bias metric or the misprediction penalties when trying to improve a model. --> 




<!-- LIMITATIONS -->
<!-- - hard to translate this into concrete improvements -->
<!-- - not everything can be seen on an aggregate level -->
<!-- - need much more for enesmebles: e.g. weights by state -->




<!-- - One could argue that the comparison of the ensembles is not entirely fair as the COVIDhub-ensemble model is a much larger ensemble with more components. On the other hand we can require that other ensembles at least not perform worse.  -->



<!-- - connect bias / coverage deviation more with actually looking at the plots -> when are states hard and easy to forecast?  -->
<!-- - More connection between results and model types: which models perform well / badly? -->







<!-- Procedure:  -->

<!-- 1) get a feeling for the data -> visualisation -->
<!-- 2) get a ranking, e.g. if we wanted to make a quick decision -->
<!--     -> summarised score -->
<!--     -> regression -->

<!-- 3) understand the metrics and what really drives them -->
<!--     -> look at correlation between scores -->
<!--     -> look at contributions from missing / too little sharpness -->
<!--     -> look at contribution from individual ranges -->
<!--     -> correlation between WIS and case numbers / WIS and horizons? -->
<!--     -> contributions from states /  -->

<!-- 4) understand what drives differences in performance  -->
<!--     -> different states: which were the ones that models did badly in?  -->
<!--     -> different horizons? when? interaction with states?  -->

<!-- 5) understand individual models and how to improve them -->
<!--     -> calibration -->
<!--     -> sharpness -->

<!-- summary: do I find characteristics of well / badly performing models?  -->




<!-- Interesting questions -->
<!-- - how do different ranges contribute to scoes -->
<!-- -> if a score is not good, what do we look for?  -->
<!-- which states were hard to forecast and why?  -->
<!-- do models do consistently well or badly?  -->
<!-- What do models get right?  -->
<!-- 	- trend changes -->
<!-- 	- changes in uncertainty?  -->
<!-- Drivers of the WIS -->
<!-- 	correlation metrics -->
<!-- 	scores at different ranges -->
<!-- 	scores at different locations -->

