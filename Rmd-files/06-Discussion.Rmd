# Summary and discussion {#discussion}

This thesis has given an extensive overview of different theoretical aspects of model evaluation and model aggregation and have applied this theory to the case study of eight models from the US Forecast Hub. Chapter \@ref(evaluation) has given an overview of different approaches to model evaluation and has presented several well-studied evaluation metrics as well as some previously unpublished. Based on these theoretical considerations we proposed a structured evaluation approach that was later applied in Chapter \@ref(results). We also presented the `scoringutils` package that facilitates the evaluation process. 
In Chapter \@ref(model-aggregation) we introduced two different model aggregation techniques that build on the proper scoring rules presented in Chapter \@ref(evaluation). One of them created a quantile average ensemble to minimise the weighted interval score, while the other created a mixture distribution based on the Continuous Ranked Probability Score. We presented the `stackr` package that implements the CRPS ensemble. Chapter \@ref(background-data) provided background on the Forecast Hub and on the data and models analysed in the case study. Chapter \@ref(results) finally used the tools previously discussed to evaluate the performance of the eight Forecast Hub models and the three ensembles. We finally conducted a small sensitivity analysis to examine the robustness of our results. 

We found that all ensemble models tended to perform very well, but could not clearly identify one ensemble that was superior to the others. For the qra-ensemble, taking four weeks of past observations into account, was consistently the best choice. Given that the qra-ensemble with only one week of past data performed almost equally well, this finding should be taken with caution. For the crps-ensemble, we found that optimising for two-weeks-ahead forecasts consistently led to the best performance. This should again be regarded as limited evidence given the small numbers of models, time points and locations analysed here. We found that the crps-ensemble was able perform similarly well as the qra-ensemble, even though the model aggregation process involved fitting a distribution to quantiles. We, however, also found that fitting a metalog-distribution instead of a gamma distribution did not improve performance significantly. We can therefore hypothesise that the inaccuracies introduced by fitting a gamma distribution did not matter too much because they mostly affected the outer tails that only had a small weight in terms of overall WIS. We did, however, not thoroughly test the metalog-distribution ensemble and more research is needed to come to more definitive conclusions. It is interesting to note that the qra-ensemble on average had a much larger tendency to overpredict than the crps-ensemble. It would be interesting to investigate whether this is a consistent property of the QRA that could be leveraged to further improve the model aggregation approach. None of the ensemble models performed significantly worse than the COVIDhub-ensemble, but also none of them performed better. It would be interesting to examine whether an ensemble that had access to the same number of models that informed the COVIDhub-ensemble would be able to beat it. When examining ensemble weights we observed that even models not among the top performers contributed to both ensembles. We can therefore hypothesise that including a greater number of diverse models in the ensemble may help improve performance. Another extension to the analysis presented here would be to allow ensemble weights to vary by state. 

Apart from the ensembles, we also found two out of three SEIR models among the top performers. This suggests that compartmental models may in principle be suited well for the prediction of Covid-19 death numbers. Given the small number of models and observations, this should, however, be considered very limited evidence. The fact that we did not have full knowledge of how the models worked made it very hard to point to specific characteristics that led to better or worse performance. While calibration and sharpness were reliably able to distinguish better from worse performing models, we could not really link these differences to underlying properties. This made it also difficult to translate insights from the evaluation process into specific suggestions on how to improve the models. 

Lastly, the specific setting imposed limitations on the evaluation. The choice to include the epiforecasts-ensemble1 model made it necessary to restrict the set of locations and forecast dates to a very limited sense. This makes it hard to generalise patterns observed throughout the model evaluation process. On the other hand, including even more locations and models would have made it even harder to devote appropriate attention to individual models. Evaluating eleven models at the same time already meant that the majority of the analysis was conducted on an aggregate level. The insufficiency of looking only at predictions aggregated over all locations and time points became most apparent for the epiforecasts-ensemble1 model. We noted that this model looked much better in terms of calibration and most summarised scores than its performance would have suggested. Apart from the Weighted Interval Score and its components, the model looked roughly comparable in terms of its summarised bias, coverage deviation and sharpness values to the UMass-MechBayes model in Figure \@ref(fig:coloured-summarised-scores). It also looked relatively well calibrated in the bias plot in Figure @ref(fig:bias-all) and aggregate coverage plots in Figure \@ref(fig:coverage-all). Only when looking at a plot of observations versus predictions could we observe that the model exhibited erratic behaviour at times. We therefore highlight again the importance of visualising the actual forecasts along with the evaluation metrics discussed in this thesis. 






<!-- LIMITATIONS -->
<!-- - hard to translate this into concrete improvements -->
<!-- - not everything can be seen on an aggregate level -->
<!-- - need much more for enesmebles: e.g. weights by state -->







<!-- - One could argue that the comparison of the ensembles is not entirely fair as the COVIDhub-ensemble model is a much larger ensemble with more components. On the other hand we can require that other ensembles at least not perform worse.  -->

<!-- - some riddles can't be answered with the scores: epiforecasts-ensemble1 -->
<!-- - this weird perfect correlation -->

<!-- - restriction due to inclusion of the epiforecasts-ensemble1 -> locations and dates -->
<!-- - break up interval score in width part and miss penalty part -->



<!-- - connect bias / coverage deviation more with actually looking at the plots -> when are states hard and easy to forecast?  -->
<!-- - More connection between results and model types: which models perform well / badly? -->







<!-- Procedure:  -->

<!-- 1) get a feeling for the data -> visualisation -->
<!-- 2) get a ranking, e.g. if we wanted to make a quick decision -->
<!--     -> summarised score -->
<!--     -> regression -->

<!-- 3) understand the metrics and what really drives them -->
<!--     -> look at correlation between scores -->
<!--     -> look at contributions from missing / too little sharpness -->
<!--     -> look at contribution from individual ranges -->
<!--     -> correlation between WIS and case numbers / WIS and horizons? -->
<!--     -> contributions from states /  -->

<!-- 4) understand what drives differences in performance  -->
<!--     -> different states: which were the ones that models did badly in?  -->
<!--     -> different horizons? when? interaction with states?  -->

<!-- 5) understand individual models and how to improve them -->
<!--     -> calibration -->
<!--     -> sharpness -->

<!-- summary: do I find characteristics of well / badly performing models?  -->




<!-- Interesting questions -->
<!-- - how do different ranges contribute to scoes -->
<!-- -> if a score is not good, what do we look for?  -->
<!-- which states were hard to forecast and why?  -->
<!-- do models do consistently well or badly?  -->
<!-- What do models get right?  -->
<!-- 	- trend changes -->
<!-- 	- changes in uncertainty?  -->
<!-- Drivers of the WIS -->
<!-- 	correlation metrics -->
<!-- 	scores at different ranges -->
<!-- 	scores at different locations -->

